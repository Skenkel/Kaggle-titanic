{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nerd/Data/Conda/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn import svm, linear_model\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler,PolynomialFeatures, LabelEncoder,FunctionTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "%matplotlib inline\n",
    "titanic = pd.read_csv('train.csv')\n",
    "testdf = pd.read_csv('test.csv')\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "titanic['FamilyCount']= (titanic['SibSp'])+(titanic['Parch'])\n",
    "titanic['IsFemale'] = (titanic.Sex=='female').astype(int)\n",
    "titanic['IsMale'] = (titanic.Sex=='male').astype(int)\n",
    "testdf['FamilyCount']= (testdf['SibSp'])+(testdf['Parch'])\n",
    "testdf['IsFemale'] = (testdf.Sex=='female').astype(int)\n",
    "testdf['IsMale'] = (testdf.Sex=='male').astype(int)\n",
    "titanic['Cabin'] =titanic['Cabin'].fillna(value='?')\n",
    "titanic['Embarked'] =titanic['Embarked'].fillna(value='S')\n",
    "titanic['Age'] =titanic['Age'].fillna(value='?')\n",
    "testdf['Cabin'] =testdf['Cabin'].fillna(value='?')\n",
    "testdf['Embarked'] =testdf['Embarked'].fillna(value='S')\n",
    "testdf['Age'] =testdf['Age'].fillna(value='?')\n",
    "testdf.loc[152,'Fare']=13.6755"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic['IsAlone']= (titanic['FamilyCount']==0).astype(int)\n",
    "testdf['IsAlone']= (testdf['FamilyCount']==0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def titlepull(title_string):\n",
    "    title_list = title_string.split()\n",
    "    for possible_title in title_list:\n",
    "        if possible_title[-1] == '.':\n",
    "            return possible_title\n",
    "def cabinpull(cabin_string):\n",
    "    if cabin_string[0]=='?':\n",
    "        return '?'\n",
    "    else:\n",
    "        return cabin_string[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ticketpull(ticketstring):\n",
    "    if ticketstring == 'LINE':\n",
    "        return 9\n",
    "    string = ticketstring.split(' ')[-1]\n",
    "    return string[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic['Deck']=titanic.Cabin.apply(cabinpull)\n",
    "titanic['Title']= titanic.Name.apply(titlepull)\n",
    "testdf['Deck']=testdf.Cabin.apply(cabinpull)\n",
    "testdf['Title']= testdf.Name.apply(titlepull)\n",
    "titanic['Ticketnum']= titanic.Ticket.apply(ticketpull)\n",
    "testdf['Ticketnum']=testdf.Ticket.apply(ticketpull)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decks_list = ['?','C','B','D','E','F','A','G','T']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name_mappings = {\n",
    "    'Dona.':'Royalty',\n",
    "    'Countess.':'Royalty',\n",
    "    'Sir.':'Royalty',\n",
    "    'Jonkheer.':'Royalty',\n",
    "    'Don.':'Royalty',\n",
    "    'Lady.':'Royalty',\n",
    "    'Ms.':'Miss.',\n",
    "    'Col.':'Officer',\n",
    "    'Capt.':'Officer',\n",
    "    'Major.':'Officer',\n",
    "    'Col.':'Officer',\n",
    "    'Mlle.':'Miss.',\n",
    "    'Mme.':'Mrs.',\n",
    "    'Mrs.':'Mrs.',\n",
    "    'Miss.':'Miss',\n",
    "    'Mr.':'Mr.',\n",
    "    'Master.':'Master.',\n",
    "    'Dr.':'Dr.',\n",
    "    'Rev.':'Rev.'\n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic['Title']= titanic['Title'].map(name_mappings)\n",
    "testdf['Title']= testdf['Title'].map(name_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validtitles = list(set(name_mappings.values()))\n",
    "titanic.FamilyCount = titanic['FamilyCount'].astype(\"category\", categories=range(11), ordered=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testdf.FamilyCount = testdf['FamilyCount'].astype(\"category\", categories=range(11), ordered=False)\n",
    "titanic.Title = titanic['Title'].astype(\"category\", categories=validtitles, ordered=False)\n",
    "testdf.Title = testdf['Title'].astype(\"category\", categories=validtitles, ordered=False)\n",
    "titanic.Deck = titanic['Deck'].astype(\"category\", categories=decks_list, ordered=False)\n",
    "testdf.Deck = testdf['Deck'].astype(\"category\", categories=decks_list, ordered=False)\n",
    "def numeric_features(df):\n",
    "    return df.select_dtypes(exclude= ['object','category'])\n",
    "numeric_features_tf = FunctionTransformer(numeric_features, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmg = KMeans()\n",
    "kmg.fit(numeric_features(titanic.drop('Survived', axis=1)))\n",
    "titanic['cluster'] =  kmg.predict(numeric_features(titanic.drop('Survived', axis=1)))\n",
    "testdf['cluster'] = kmg.predict(numeric_features(testdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "age_transformers = [('numfeat' ,numeric_features_tf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fu_age_impute = FeatureUnion(age_transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knr = KNeighborsRegressor(n_jobs=-1)\n",
    "lr = LinearRegression()\n",
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "age_impute_pipe_lr =  Pipeline(\n",
    "    [\n",
    "        ('fu',fu_age_impute),\n",
    "        ('ss',ss),\n",
    "        ('lr',lr)\n",
    "    ]\n",
    "\n",
    ")\n",
    "age_impute_pipe_kn =  Pipeline(\n",
    "    [\n",
    "        ('fu_age',fu_age_impute),\n",
    "        ('ss',ss),\n",
    "        ('knr',knr)\n",
    "    ]\n",
    "\n",
    ")\n",
    "age_params = {\n",
    "    \n",
    "}\n",
    "age_params2 = {'knr__n_neighbors':range(4,15)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs_age_kn = GridSearchCV(age_impute_pipe_kn, param_grid=age_params2, cv=5)\n",
    "gs_age_lr = GridSearchCV(age_impute_pipe_lr, param_grid=age_params, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic_ages =titanic[(titanic['Age']!='?')]\n",
    "titanic_no_ages = titanic[(titanic['Age']=='?')]\n",
    "\n",
    "test_ages =testdf[(testdf['Age']!='?')]\n",
    "test_no_ages = testdf[(testdf['Age']=='?')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = titanic_ages.drop(['Age','Survived'],axis=1)\n",
    "y_train= titanic_ages['Age']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test1 =titanic_no_ages.drop(['Age','Survived'], axis=1)\n",
    "X_test2 = test_no_ages.drop('Age', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.185117252161\n"
     ]
    }
   ],
   "source": [
    "gs_age_lr.fit(X_train,y_train)\n",
    "print(gs_age_lr.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.155501231036\n"
     ]
    }
   ],
   "source": [
    "gs_age_kn.fit(X_train,y_train)\n",
    "print(gs_age_kn.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('fu_age', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('numfeat', FunctionTransformer(accept_sparse=False,\n",
       "          func=<function numeric_features at 0x7f00001877b8>,\n",
       "          inv_kw_args=None, inverse_func=None, kw_args=None, pass_y=False,\n",
       "          validate=False))],\n",
       "       transfor...kowski',\n",
       "          metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
       "          weights='uniform'))])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_impute_pipe_kn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44565589418749163"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_impute_pipe_kn.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test1['Age'] = gs_age_kn.predict(X_test1)\n",
    "X_test2['Age']= gs_age_kn.predict(X_test2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic.loc[X_test1.index,'Age'] = X_test1['Age']\n",
    "testdf.loc[X_test2.index,'Age'] = X_test2['Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ageband(age):\n",
    "    ageband= (int(age)//13)+1\n",
    "    return ageband"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic['AgeBand']=titanic.Age.apply(ageband)\n",
    "testdf['AgeBand']=testdf.Age.apply(ageband)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findsurname(wife):\n",
    "    if wife[-1]==')':\n",
    "        return wife.split()[0]\n",
    "    \n",
    "def findhusband(wife, husband):\n",
    "    for num in range(len(wife.split())):\n",
    "        if wife.split()[num][-1]=='.':\n",
    "            try:\n",
    "                return (wife.split()[num-1]==husband.split()[num-1])  & (wife.split()[num+1]==husband.split()[num+1])\n",
    "            except:\n",
    "                return False\n",
    "            \n",
    "        #Splitting by gender\n",
    "def dead_husband(wife,husbandlist):\n",
    "    #this takes a wife's name as an argument and a listas the second argument.  it returns a true or false\n",
    "    result= False # assume that the wife did not die\n",
    "    for num in range(len(husbandlist)):\n",
    "        if findhusband(wife, husbandlist[num]):\n",
    "            result = True\n",
    "    return result\n",
    "def dead_wife(husband,wifelist):\n",
    "    result= False # assume that the wife did not die\n",
    "    for num in range(len(wifelist)):\n",
    "        if findhusband(wifelist[num], husband):\n",
    "            result = True\n",
    "    return result\n",
    "            \n",
    "titanicmale=  titanic[titanic['Sex']=='male'] \n",
    "titanicfemale= titanic[titanic['Sex']=='female']\n",
    "testmale=  testdf[testdf['Sex']=='male'] \n",
    "testfemale= testdf[testdf['Sex']=='female']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "livemen= titanicmale[titanicmale['Survived']==1]\n",
    "deadmen= titanicmale[titanicmale['Survived']==0]\n",
    "livewomen= titanicfemale[titanicfemale['Survived']==1]\n",
    "deadwomen= titanicfemale[titanicfemale['Survived']==0]\n",
    "deadwomennames= deadwomen.Name.values\n",
    "livewomennames= livewomen.Name.values\n",
    "livemennames = livemen.Name.values\n",
    "deadmennames = deadmen.Name.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nerd/Data/Conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/nerd/Data/Conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/nerd/Data/Conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/home/nerd/Data/Conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/home/nerd/Data/Conda/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/home/nerd/Data/Conda/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/nerd/Data/Conda/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/home/nerd/Data/Conda/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# lets do women with dead husbands. \n",
    "titanicfemale['Dead_Husband']= titanicfemale.Name.apply(lambda x:  dead_husband(x,deadmennames))\n",
    "titanicfemale['Live_Husband']= titanicfemale.Name.apply(lambda x:  dead_husband(x,livemennames))\n",
    "titanicmale['Dead_Wife']=titanicmale.Name.apply(lambda x:  dead_wife(x,deadwomennames))\n",
    "titanicmale['Live_Wife']=titanicmale.Name.apply(lambda x:  dead_wife(x,livewomennames))\n",
    "# lets do women with dead husbands. \n",
    "testfemale['Dead_Husband']= testfemale.Name.apply(lambda x:  dead_husband(x,deadmennames))\n",
    "testfemale['Live_Husband']= testfemale.Name.apply(lambda x:  dead_husband(x,livemennames))\n",
    "testmale['Dead_Wife']=testmale.Name.apply(lambda x:  dead_wife(x,deadwomennames))\n",
    "testmale['Live_Wife']=testmale.Name.apply(lambda x:  dead_wife(x,livewomennames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testdf.loc[testfemale.index,'Dead_Husband']= testfemale['Dead_Husband'].astype(int)\n",
    "\n",
    "testdf.loc[testfemale.index,'Live_Husband']= testfemale['Live_Husband'].astype(int)\n",
    "testdf.loc[testmale.index,'Dead_Wife']= testmale['Dead_Wife'].astype(int)\n",
    "testdf.loc[testmale.index,'Live_Wife']= testmale['Live_Wife'].astype(int)\n",
    "titanic.loc[titanicfemale.index,'Dead_Husband']= titanicfemale['Dead_Husband'].astype(int)\n",
    "titanic.loc[titanicfemale.index,'Live_Husband']= titanicfemale['Live_Husband'].astype(int)\n",
    "titanic.loc[titanicmale.index,'Dead_Wife']= titanicmale['Dead_Wife'].astype(int)\n",
    "titanic.loc[titanicmale.index,'Live_Wife']= titanicmale['Live_Wife'].astype(int)\n",
    "titanic['Dead_Husband'].fillna(value=0, inplace=True)\n",
    "titanic['Dead_Wife'].fillna(value=0, inplace=True)\n",
    "testdf['Dead_Husband'].fillna(value=0, inplace=True)\n",
    "testdf['Dead_Wife'].fillna(value=0, inplace=True)\n",
    "titanic['Live_Husband'].fillna(value=0, inplace=True)\n",
    "titanic['Live_Wife'].fillna(value=0, inplace=True)\n",
    "testdf['Live_Husband'].fillna(value=0, inplace=True)\n",
    "testdf['Live_Wife'].fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dummies2(df):\n",
    "    cols = [ 'Embarked','Deck', 'Title', 'Ticketnum']\n",
    "    return pd.get_dummies(df[cols], columns=cols)\n",
    "dummies_tf2 = FunctionTransformer(dummies2, validate=False)\n",
    "def numeric_features2(df):\n",
    "    return df[['Pclass', 'IsFemale',\n",
    "       'AgeBand',  'Ticketnum']]\n",
    "def kclass (df):\n",
    "    return kmc.fit_predict(numeric_features(df))\n",
    "kclass_tf = FunctionTransformer(kclass, validate=False)\n",
    "numeric_features_tf2 = FunctionTransformer(numeric_features2, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "survive_transformers = [('numfeat' ,numeric_features_tf),('dummies' ,dummies_tf2),]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prep_for_tensor=  FeatureUnion(survive_transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('numfeat', FunctionTransformer(accept_sparse=False,\n",
       "          func=<function numeric_features at 0x7f00001877b8>,\n",
       "          inv_kw_args=None, inverse_func=None, kw_args=None, pass_y=False,\n",
       "          validate=False)), ('dummies', FunctionTransformer(accept_sparse=False,\n",
       "          func=<function dummies2 at 0x7effdc7a8400>, inv_kw_args=None,\n",
       "          inverse_func=None, kw_args=None, pass_y=False, validate=False))],\n",
       "       transformer_weights=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_for_tensor.fit(titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1.,    0.,    3., ...,    0.,    0.,    0.],\n",
       "       [   2.,    1.,    1., ...,    0.,    0.,    0.],\n",
       "       [   3.,    1.,    3., ...,    0.,    0.,    0.],\n",
       "       ..., \n",
       "       [ 889.,    0.,    3., ...,    0.,    0.,    0.],\n",
       "       [ 890.,    1.,    1., ...,    0.,    0.,    0.],\n",
       "       [ 891.,    0.,    3., ...,    0.,    0.,    0.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_for_tensor.transform(titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "survive_pipe_kn =  Pipeline([('prep',prep_for_tensor),('ss',ss),('pca',pca)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1.,    0.,    3., ...,    0.,    0.,    0.],\n",
       "       [   2.,    1.,    1., ...,    0.,    0.,    0.],\n",
       "       [   3.,    1.,    3., ...,    0.,    0.,    0.],\n",
       "       ..., \n",
       "       [ 889.,    0.,    3., ...,    0.,    0.,    0.],\n",
       "       [ 890.,    1.,    1., ...,    0.,    0.,    0.],\n",
       "       [ 891.,    0.,    3., ...,    0.,    0.,    0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_for_tensor.fit_transform(titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmc = KMeans()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.66443754e+00,  -6.19425850e-01,   1.43959061e+00, ...,\n",
       "         -4.52376226e-16,  -2.90409480e-16,  -6.20605741e-17],\n",
       "       [  5.04488097e+00,   1.33510314e+00,   7.20550849e-02, ...,\n",
       "          5.59327020e-16,   1.60730533e-15,   3.46704999e-16],\n",
       "       [  5.81730634e-01,  -2.34880951e+00,  -2.18588900e+00, ...,\n",
       "          1.21684040e-15,  -1.83253738e-16,  -4.59598755e-16],\n",
       "       ..., \n",
       "       [  1.02112803e+00,  -2.63473730e+00,   5.68028058e-01, ...,\n",
       "         -1.18140156e-17,   8.82934824e-17,   1.39518056e-17],\n",
       "       [  1.51728641e+00,   3.95044995e+00,  -3.71304984e-01, ...,\n",
       "          4.33612752e-16,   9.39921308e-17,   5.56383039e-17],\n",
       "       [ -2.51072495e+00,  -4.75171768e-02,  -1.67245215e+00, ...,\n",
       "          1.35486539e-16,  -1.51429344e-16,   6.54011522e-17]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survive_pipe_kn.fit_transform(titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=survive_pipe_kn.fit_transform(titanic.drop(['Survived','PassengerId'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = survive_pipe_kn.fit_transform(testdf.drop('PassengerId', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y= titanic['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X = numeric_features(titanic.drop('Survived', axis=1))\n",
    "kf = StratifiedKFold(random_state=2003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for train,test in kf.split(X,y):\n",
    "    X_train=X[train,:]\n",
    "    X_test=X[test,:]\n",
    "    y_train=y[train]\n",
    "    y_test=y[test]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feats = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build neural network with tflearn via tflearn tutorial\n",
    "\n",
    "net = tflearn.input_data(shape=[None, feats])\n",
    "net = tflearn.fully_connected(net, feats)\n",
    "net = tflearn.dropout(net, 0.5)\n",
    "net = tflearn.fully_connected(net, feats)\n",
    "net = tflearn.fully_connected(net, feats)\n",
    "net = tflearn.fully_connected(net, feats)\n",
    "net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "net = tflearn.regression(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "\n",
    "model = tflearn.DNN(net)\n",
    "params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensorgs = GridSearchCV(model,param_grid=params,cv=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 44)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels2 = pd.get_dummies(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3899  | total loss: \u001b[1m\u001b[32m0.46240\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1300 | loss: 0.46240 - acc: 0.8144 -- iter: 400/594\n",
      "Training Step: 3900  | total loss: \u001b[1m\u001b[32m0.52176\u001b[0m\u001b[0m | time: 0.011s\n",
      "| Adam | epoch: 1300 | loss: 0.52176 - acc: 0.7783 -- iter: 594/594\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# Start training (apply gradient descent algorithm)\n",
    "model.fit(X_train,labels2.values,batch_size=200, n_epoch=1300, show_metric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finalpred =model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.87323499,  0.12676504],\n",
       "       [ 0.32296097,  0.67703903],\n",
       "       [ 0.1953344 ,  0.80466557],\n",
       "       [ 0.05460249,  0.94539756],\n",
       "       [ 0.83342522,  0.16657481],\n",
       "       [ 0.15562592,  0.84437406],\n",
       "       [ 0.29676452,  0.70323545],\n",
       "       [ 0.95475739,  0.04524265],\n",
       "       [ 0.28826299,  0.71173704],\n",
       "       [ 0.91113991,  0.08886008],\n",
       "       [ 0.67385703,  0.32614291],\n",
       "       [ 0.91660762,  0.08339235],\n",
       "       [ 0.88386583,  0.11613411],\n",
       "       [ 0.66622782,  0.33377221],\n",
       "       [ 0.24252246,  0.75747758],\n",
       "       [ 0.20156726,  0.79843271],\n",
       "       [ 0.60478592,  0.39521405],\n",
       "       [ 0.88386101,  0.11613906],\n",
       "       [ 0.34878385,  0.65121621],\n",
       "       [ 0.83974713,  0.16025287],\n",
       "       [ 0.88386679,  0.11613328],\n",
       "       [ 0.38102782,  0.61897218],\n",
       "       [ 0.95806354,  0.04193648],\n",
       "       [ 0.34519863,  0.65480137],\n",
       "       [ 0.21839528,  0.78160471],\n",
       "       [ 0.83895957,  0.16104043],\n",
       "       [ 0.85355866,  0.14644131],\n",
       "       [ 0.57868874,  0.42131126],\n",
       "       [ 0.89310551,  0.10689445],\n",
       "       [ 0.849603  ,  0.15039694],\n",
       "       [ 0.89645898,  0.10354099],\n",
       "       [ 0.69602662,  0.30397338],\n",
       "       [ 0.93925285,  0.06074711],\n",
       "       [ 0.04766669,  0.95233333],\n",
       "       [ 0.88386583,  0.11613411],\n",
       "       [ 0.83974701,  0.16025299],\n",
       "       [ 0.90765053,  0.09234942],\n",
       "       [ 0.91113544,  0.08886461],\n",
       "       [ 0.85152406,  0.14847589],\n",
       "       [ 0.728598  ,  0.27140197],\n",
       "       [ 0.64704418,  0.35295588],\n",
       "       [ 0.29652959,  0.70347047],\n",
       "       [ 0.88386601,  0.11613397],\n",
       "       [ 0.8411963 ,  0.15880369],\n",
       "       [ 0.50048894,  0.49951103],\n",
       "       [ 0.86903304,  0.13096699],\n",
       "       [ 0.849603  ,  0.15039694],\n",
       "       [ 0.02306923,  0.9769308 ],\n",
       "       [ 0.64704418,  0.35295588],\n",
       "       [ 0.76005262,  0.23994736],\n",
       "       [ 0.47470134,  0.52529866],\n",
       "       [ 0.46226367,  0.5377363 ],\n",
       "       [ 0.84960335,  0.15039665],\n",
       "       [ 0.65030032,  0.34969971],\n",
       "       [ 0.78278565,  0.21721435],\n",
       "       [ 0.31366241,  0.68633765],\n",
       "       [ 0.88386583,  0.11613411],\n",
       "       [ 0.2563808 ,  0.74361914],\n",
       "       [ 0.86934704,  0.13065295],\n",
       "       [ 0.2705262 ,  0.72947377],\n",
       "       [ 0.27051425,  0.72948581],\n",
       "       [ 0.8271876 ,  0.17281242],\n",
       "       [ 0.88386583,  0.11613411],\n",
       "       [ 0.29731822,  0.70268178],\n",
       "       [ 0.76643634,  0.23356368],\n",
       "       [ 0.7535435 ,  0.2464565 ],\n",
       "       [ 0.91964942,  0.08035058],\n",
       "       [ 0.90007275,  0.09992723],\n",
       "       [ 0.7209987 ,  0.2790013 ],\n",
       "       [ 0.88386351,  0.11613647],\n",
       "       [ 0.89076978,  0.10923021],\n",
       "       [ 0.86574978,  0.13425025],\n",
       "       [ 0.76643634,  0.23356368],\n",
       "       [ 0.88386518,  0.11613486],\n",
       "       [ 0.91113991,  0.08886008],\n",
       "       [ 0.19875547,  0.80124456],\n",
       "       [ 0.32497674,  0.6750232 ],\n",
       "       [ 0.71586633,  0.2841337 ],\n",
       "       [ 0.91532183,  0.08467814],\n",
       "       [ 0.81553167,  0.18446834],\n",
       "       [ 0.81542158,  0.18457842],\n",
       "       [ 0.84960252,  0.15039749],\n",
       "       [ 0.84960449,  0.15039553],\n",
       "       [ 0.72563678,  0.27436316],\n",
       "       [ 0.62918353,  0.37081653],\n",
       "       [ 0.69993764,  0.30006236],\n",
       "       [ 0.27052966,  0.72947031],\n",
       "       [ 0.42269668,  0.57730329],\n",
       "       [ 0.92324889,  0.07675109],\n",
       "       [ 0.98090577,  0.01909419],\n",
       "       [ 0.89087594,  0.10912403],\n",
       "       [ 0.8011899 ,  0.19881007],\n",
       "       [ 0.97140908,  0.02859092],\n",
       "       [ 0.84961969,  0.15038024],\n",
       "       [ 0.84960264,  0.15039738],\n",
       "       [ 0.1921645 ,  0.80783558],\n",
       "       [ 0.60339034,  0.39660969],\n",
       "       [ 0.31318855,  0.68681145],\n",
       "       [ 0.76005262,  0.23994736],\n",
       "       [ 0.83228409,  0.16771589],\n",
       "       [ 0.6457082 ,  0.35429177],\n",
       "       [ 0.88919276,  0.11080724],\n",
       "       [ 0.91113991,  0.08886008],\n",
       "       [ 0.27052516,  0.72947478],\n",
       "       [ 0.73532516,  0.26467484],\n",
       "       [ 0.82730818,  0.17269184],\n",
       "       [ 0.11841763,  0.88158238],\n",
       "       [ 0.41183215,  0.58816791],\n",
       "       [ 0.34267065,  0.65732938],\n",
       "       [ 0.79548031,  0.20451969],\n",
       "       [ 0.9165746 ,  0.08342546],\n",
       "       [ 0.8563329 ,  0.14366715],\n",
       "       [ 0.24239688,  0.75760317],\n",
       "       [ 0.48542139,  0.51457858],\n",
       "       [ 0.13974646,  0.86025357],\n",
       "       [ 0.41259828,  0.58740175],\n",
       "       [ 0.0257305 ,  0.97426945],\n",
       "       [ 0.72443503,  0.27556497],\n",
       "       [ 0.6964429 ,  0.3035571 ],\n",
       "       [ 0.78280419,  0.21719585],\n",
       "       [ 0.8891899 ,  0.11081005],\n",
       "       [ 0.72522664,  0.27477339],\n",
       "       [ 0.18201275,  0.81798726],\n",
       "       [ 0.14841765,  0.85158235],\n",
       "       [ 0.79555136,  0.20444864],\n",
       "       [ 0.88386518,  0.11613486],\n",
       "       [ 0.20385578,  0.79614425],\n",
       "       [ 0.89076495,  0.10923501],\n",
       "       [ 0.74448019,  0.25551984],\n",
       "       [ 0.85624278,  0.14375725],\n",
       "       [ 0.37931132,  0.62068874],\n",
       "       [ 0.84960884,  0.15039116],\n",
       "       [ 0.45730293,  0.54269701],\n",
       "       [ 0.27052519,  0.72947478],\n",
       "       [ 0.82580191,  0.17419808],\n",
       "       [ 0.43742663,  0.56257343],\n",
       "       [ 0.22540227,  0.7745977 ],\n",
       "       [ 0.78658926,  0.21341071],\n",
       "       [ 0.81542158,  0.18457842],\n",
       "       [ 0.76643634,  0.23356368],\n",
       "       [ 0.76643634,  0.23356368],\n",
       "       [ 0.92103916,  0.07896084],\n",
       "       [ 0.72869569,  0.27130434],\n",
       "       [ 0.67928433,  0.32071567],\n",
       "       [ 0.87686783,  0.12313211],\n",
       "       [ 0.87686783,  0.12313211],\n",
       "       [ 0.47066599,  0.52933401],\n",
       "       [ 0.72554183,  0.2744582 ],\n",
       "       [ 0.30701745,  0.69298255],\n",
       "       [ 0.88418043,  0.11581954],\n",
       "       [ 0.87686801,  0.12313192],\n",
       "       [ 0.90266031,  0.09733973],\n",
       "       [ 0.88307977,  0.11692021],\n",
       "       [ 0.28285584,  0.71714413],\n",
       "       [ 0.41455451,  0.58544546],\n",
       "       [ 0.8306002 ,  0.1693998 ],\n",
       "       [ 0.26787582,  0.73212415],\n",
       "       [ 0.11307986,  0.88692015],\n",
       "       [ 0.87687767,  0.1231224 ],\n",
       "       [ 0.84091246,  0.15908758],\n",
       "       [ 0.37067643,  0.6293236 ],\n",
       "       [ 0.24384131,  0.75615865],\n",
       "       [ 0.87686723,  0.12313274],\n",
       "       [ 0.75431347,  0.24568652],\n",
       "       [ 0.87686884,  0.12313118],\n",
       "       [ 0.07197797,  0.92802203],\n",
       "       [ 0.876908  ,  0.123092  ],\n",
       "       [ 0.90560544,  0.09439453],\n",
       "       [ 0.82280105,  0.17719892],\n",
       "       [ 0.17348692,  0.82651311],\n",
       "       [ 0.84091151,  0.15908846],\n",
       "       [ 0.06466542,  0.93533456],\n",
       "       [ 0.89227468,  0.10772526],\n",
       "       [ 0.31856942,  0.68143058],\n",
       "       [ 0.84022164,  0.15977837],\n",
       "       [ 0.89348072,  0.10651924],\n",
       "       [ 0.8409245 ,  0.1590755 ],\n",
       "       [ 0.90560895,  0.09439105],\n",
       "       [ 0.14292637,  0.85707361],\n",
       "       [ 0.86217904,  0.13782099],\n",
       "       [ 0.47994399,  0.52005607],\n",
       "       [ 0.84091139,  0.15908867],\n",
       "       [ 0.69613177,  0.3038682 ],\n",
       "       [ 0.27229372,  0.72770631],\n",
       "       [ 0.83060014,  0.16939989],\n",
       "       [ 0.19565521,  0.80434477],\n",
       "       [ 0.30686873,  0.69313127],\n",
       "       [ 0.1025302 ,  0.89746982],\n",
       "       [ 0.39758533,  0.60241473],\n",
       "       [ 0.9567067 ,  0.04329327],\n",
       "       [ 0.84090602,  0.15909392],\n",
       "       [ 0.84090757,  0.15909243],\n",
       "       [ 0.33509481,  0.66490519],\n",
       "       [ 0.57719409,  0.42280594],\n",
       "       [ 0.35460794,  0.645392  ],\n",
       "       [ 0.69525075,  0.30474919],\n",
       "       [ 0.73275107,  0.26724893],\n",
       "       [ 0.75446475,  0.24553525],\n",
       "       [ 0.92396599,  0.07603403],\n",
       "       [ 0.72262508,  0.27737489],\n",
       "       [ 0.84091246,  0.15908758],\n",
       "       [ 0.84786683,  0.1521332 ],\n",
       "       [ 0.02454123,  0.9754588 ],\n",
       "       [ 0.27669859,  0.72330141],\n",
       "       [ 0.86217904,  0.13782096],\n",
       "       [ 0.36519837,  0.63480163],\n",
       "       [ 0.80532306,  0.19467698],\n",
       "       [ 0.28252882,  0.71747112],\n",
       "       [ 0.08250356,  0.9174965 ],\n",
       "       [ 0.24193415,  0.75806582],\n",
       "       [ 0.87686223,  0.12313773],\n",
       "       [ 0.87686712,  0.12313288],\n",
       "       [ 0.71217221,  0.28782779],\n",
       "       [ 0.3350983 ,  0.66490167],\n",
       "       [ 0.84786683,  0.1521332 ],\n",
       "       [ 0.05822562,  0.94177431],\n",
       "       [ 0.87686783,  0.12313219],\n",
       "       [ 0.97923112,  0.02076885],\n",
       "       [ 0.80530095,  0.19469906],\n",
       "       [ 0.72387511,  0.27612486],\n",
       "       [ 0.87686884,  0.12313118],\n",
       "       [ 0.61952376,  0.3804763 ],\n",
       "       [ 0.3351002 ,  0.66489983],\n",
       "       [ 0.87830395,  0.12169606],\n",
       "       [ 0.81958187,  0.18041815],\n",
       "       [ 0.60199451,  0.39800555],\n",
       "       [ 0.21171202,  0.78828794],\n",
       "       [ 0.87687254,  0.12312748],\n",
       "       [ 0.90845579,  0.09154428],\n",
       "       [ 0.12881251,  0.87118751],\n",
       "       [ 0.66486776,  0.33513224],\n",
       "       [ 0.83059388,  0.16940613],\n",
       "       [ 0.79972434,  0.2002756 ],\n",
       "       [ 0.20320991,  0.79679012],\n",
       "       [ 0.8306002 ,  0.1693998 ],\n",
       "       [ 0.13753958,  0.86246049],\n",
       "       [ 0.24335283,  0.75664717],\n",
       "       [ 0.2438854 ,  0.75611454],\n",
       "       [ 0.86217904,  0.13782096],\n",
       "       [ 0.8409121 ,  0.15908788],\n",
       "       [ 0.81768936,  0.18231067],\n",
       "       [ 0.1282392 ,  0.87176079],\n",
       "       [ 0.84091818,  0.1590818 ],\n",
       "       [ 0.87686884,  0.12313118],\n",
       "       [ 0.79972434,  0.2002756 ],\n",
       "       [ 0.71809202,  0.28190801],\n",
       "       [ 0.84091264,  0.15908739],\n",
       "       [ 0.78346616,  0.21653391],\n",
       "       [ 0.1556747 ,  0.84432524],\n",
       "       [ 0.86217374,  0.13782628],\n",
       "       [ 0.84091818,  0.1590818 ],\n",
       "       [ 0.93629533,  0.06370467],\n",
       "       [ 0.99221498,  0.00778505],\n",
       "       [ 0.88056904,  0.11943093],\n",
       "       [ 0.92734849,  0.07265155],\n",
       "       [ 0.19407734,  0.80592269],\n",
       "       [ 0.69803059,  0.30196935],\n",
       "       [ 0.94569415,  0.05430579],\n",
       "       [ 0.35619152,  0.64380848],\n",
       "       [ 0.04894856,  0.95105141],\n",
       "       [ 0.27857643,  0.72142357],\n",
       "       [ 0.23822984,  0.76177013],\n",
       "       [ 0.29070657,  0.70929337],\n",
       "       [ 0.46884614,  0.53115386],\n",
       "       [ 0.27195838,  0.72804165],\n",
       "       [ 0.86217904,  0.13782096],\n",
       "       [ 0.9548952 ,  0.04510484],\n",
       "       [ 0.81591088,  0.18408918],\n",
       "       [ 0.04557541,  0.95442462],\n",
       "       [ 0.92396599,  0.07603403],\n",
       "       [ 0.75432909,  0.24567087],\n",
       "       [ 0.23039669,  0.76960325],\n",
       "       [ 0.37088403,  0.629116  ],\n",
       "       [ 0.64810836,  0.35189161],\n",
       "       [ 0.87687767,  0.1231224 ],\n",
       "       [ 0.35699305,  0.64300692],\n",
       "       [ 0.87686783,  0.12313211],\n",
       "       [ 0.08907794,  0.91092199],\n",
       "       [ 0.86114502,  0.13885498],\n",
       "       [ 0.90561444,  0.09438552],\n",
       "       [ 0.25909171,  0.74090827],\n",
       "       [ 0.3068687 ,  0.69313133],\n",
       "       [ 0.71454811,  0.28545192],\n",
       "       [ 0.84091246,  0.15908758],\n",
       "       [ 0.87686783,  0.12313211],\n",
       "       [ 0.1691826 ,  0.8308174 ],\n",
       "       [ 0.15381914,  0.84618086],\n",
       "       [ 0.87686783,  0.12313211],\n",
       "       [ 0.19268984,  0.80731016],\n",
       "       [ 0.82977682,  0.17022316],\n",
       "       [ 0.84090602,  0.15909392],\n",
       "       [ 0.41946349,  0.58053654],\n",
       "       [ 0.92048925,  0.07951081],\n",
       "       [ 0.10348475,  0.89651525],\n",
       "       [ 0.73949099,  0.2605091 ],\n",
       "       [ 0.65406781,  0.34593216],\n",
       "       [ 0.8306002 ,  0.1693998 ]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalpred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = finalpred[:,1].round()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = ['TN1','TN2','TN3']\n",
    "shape = (train.shape[0], len(cols))\n",
    "zeros= np.zeros(shape)\n",
    "stacked_train = pd.DataFrame(zeros,columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TN1</th>\n",
       "      <th>TN2</th>\n",
       "      <th>TN3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TN1  TN2  TN3\n",
       "0  0.0  0.0  0.0\n",
       "1  0.0  0.0  0.0\n",
       "2  0.0  0.0  0.0\n",
       "3  0.0  0.0  0.0\n",
       "4  0.0  0.0  0.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca2 = PCA(n_components=15)\n",
    "def numeric_features3(df):\n",
    "    return df[['Pclass', 'IsFemale',\n",
    "       'AgeBand',  'Ticketnum', 'Fare']]\n",
    "numeric_features_tf3 = FunctionTransformer(numeric_features3, validate=False)\n",
    "survive_transformers2 = [('numfeat' ,numeric_features_tf3),('dummies' ,dummies_tf2)]\n",
    "prep_for_tensor2=  FeatureUnion(survive_transformers2)\n",
    "survive_pipe2=  Pipeline([('prep',prep_for_tensor2),('ss',ss),('pca',pca2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net2 = tflearn.input_data(shape=[None, feats])\n",
    "net2 = tflearn.fully_connected(net2, feats)\n",
    "net2 = tflearn.dropout(net2, 0.5)\n",
    "net2 = tflearn.fully_connected(net2, feats)\n",
    "net2 = tflearn.fully_connected(net2, feats)\n",
    "net2 = tflearn.fully_connected(net2, feats)\n",
    "net2 = tflearn.fully_connected(net2, 2, activation='softmax')\n",
    "net2 = tflearn.regression(net2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model2 = tflearn.DNN(net2)\n",
    "params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Run id: O3QIX3\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 594\n",
      "Validation samples: 0\n",
      "--\n",
      "Training Step: 1  | time: 0.056s\n",
      "| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 594/594\n",
      "--\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m0.62383\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 002 | loss: 0.62383 - acc: 0.4455 -- iter: 594/594\n",
      "--\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m0.68027\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 003 | loss: 0.68027 - acc: 0.5851 -- iter: 594/594\n",
      "--\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m0.68939\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 004 | loss: 0.68939 - acc: 0.6084 -- iter: 594/594\n",
      "--\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m0.69121\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 005 | loss: 0.69121 - acc: 0.6138 -- iter: 594/594\n",
      "--\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m0.69144\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 006 | loss: 0.69144 - acc: 0.6153 -- iter: 594/594\n",
      "--\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m0.69121\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 007 | loss: 0.69121 - acc: 0.6158 -- iter: 594/594\n",
      "--\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m0.69092\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 008 | loss: 0.69092 - acc: 0.6160 -- iter: 594/594\n",
      "--\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m0.69034\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 009 | loss: 0.69034 - acc: 0.6161 -- iter: 594/594\n",
      "--\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m0.68971\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 010 | loss: 0.68971 - acc: 0.6161 -- iter: 594/594\n",
      "--\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m0.68900\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 011 | loss: 0.68900 - acc: 0.6161 -- iter: 594/594\n",
      "--\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m0.68873\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 012 | loss: 0.68873 - acc: 0.6162 -- iter: 594/594\n",
      "--\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m0.68751\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 013 | loss: 0.68751 - acc: 0.6162 -- iter: 594/594\n",
      "--\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m0.68679\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 014 | loss: 0.68679 - acc: 0.6162 -- iter: 594/594\n",
      "--\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m0.68512\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 015 | loss: 0.68512 - acc: 0.6162 -- iter: 594/594\n",
      "--\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m0.68414\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 016 | loss: 0.68414 - acc: 0.6162 -- iter: 594/594\n",
      "--\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m0.68176\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 017 | loss: 0.68176 - acc: 0.6162 -- iter: 594/594\n",
      "--\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m0.68072\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 018 | loss: 0.68072 - acc: 0.6162 -- iter: 594/594\n",
      "--\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m0.67688\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 019 | loss: 0.67688 - acc: 0.6162 -- iter: 594/594\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m0.67621\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 020 | loss: 0.67621 - acc: 0.6167 -- iter: 594/594\n",
      "--\n",
      "Training Step: 21  | total loss: \u001b[1m\u001b[32m0.67071\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 021 | loss: 0.67071 - acc: 0.6197 -- iter: 594/594\n",
      "--\n",
      "Training Step: 22  | total loss: \u001b[1m\u001b[32m0.67003\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 022 | loss: 0.67003 - acc: 0.6171 -- iter: 594/594\n",
      "--\n",
      "Training Step: 23  | total loss: \u001b[1m\u001b[32m0.66189\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 023 | loss: 0.66189 - acc: 0.6290 -- iter: 594/594\n",
      "--\n",
      "Training Step: 24  | total loss: \u001b[1m\u001b[32m0.65941\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 024 | loss: 0.65941 - acc: 0.6321 -- iter: 594/594\n",
      "--\n",
      "Training Step: 25  | total loss: \u001b[1m\u001b[32m0.64859\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 025 | loss: 0.64859 - acc: 0.6488 -- iter: 594/594\n",
      "--\n",
      "Training Step: 26  | total loss: \u001b[1m\u001b[32m0.64723\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 026 | loss: 0.64723 - acc: 0.6460 -- iter: 594/594\n",
      "--\n",
      "Training Step: 27  | total loss: \u001b[1m\u001b[32m0.63264\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 027 | loss: 0.63264 - acc: 0.6678 -- iter: 594/594\n",
      "--\n",
      "Training Step: 28  | total loss: \u001b[1m\u001b[32m0.63410\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 028 | loss: 0.63410 - acc: 0.6637 -- iter: 594/594\n",
      "--\n",
      "Training Step: 29  | total loss: \u001b[1m\u001b[32m0.61450\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 029 | loss: 0.61450 - acc: 0.6857 -- iter: 594/594\n",
      "--\n",
      "Training Step: 30  | total loss: \u001b[1m\u001b[32m0.61937\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 030 | loss: 0.61937 - acc: 0.6744 -- iter: 594/594\n",
      "--\n",
      "Training Step: 31  | total loss: \u001b[1m\u001b[32m0.59599\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 031 | loss: 0.59599 - acc: 0.7002 -- iter: 594/594\n",
      "--\n",
      "Training Step: 32  | total loss: \u001b[1m\u001b[32m0.60545\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 032 | loss: 0.60545 - acc: 0.6889 -- iter: 594/594\n",
      "--\n",
      "Training Step: 33  | total loss: \u001b[1m\u001b[32m0.57821\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 033 | loss: 0.57821 - acc: 0.7173 -- iter: 594/594\n",
      "--\n",
      "Training Step: 34  | total loss: \u001b[1m\u001b[32m0.58810\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 034 | loss: 0.58810 - acc: 0.7089 -- iter: 594/594\n",
      "--\n",
      "Training Step: 35  | total loss: \u001b[1m\u001b[32m0.56159\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 035 | loss: 0.56159 - acc: 0.7315 -- iter: 594/594\n",
      "--\n",
      "Training Step: 36  | total loss: \u001b[1m\u001b[32m0.61790\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 036 | loss: 0.61790 - acc: 0.6931 -- iter: 594/594\n",
      "--\n",
      "Training Step: 37  | total loss: \u001b[1m\u001b[32m0.58366\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 037 | loss: 0.58366 - acc: 0.7228 -- iter: 594/594\n",
      "--\n",
      "Training Step: 38  | total loss: \u001b[1m\u001b[32m0.55691\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 038 | loss: 0.55691 - acc: 0.7441 -- iter: 594/594\n",
      "--\n",
      "Training Step: 39  | total loss: \u001b[1m\u001b[32m0.53421\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 039 | loss: 0.53421 - acc: 0.7605 -- iter: 594/594\n",
      "--\n",
      "Training Step: 40  | total loss: \u001b[1m\u001b[32m0.60379\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 040 | loss: 0.60379 - acc: 0.7139 -- iter: 594/594\n",
      "--\n",
      "Training Step: 41  | total loss: \u001b[1m\u001b[32m0.57157\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 041 | loss: 0.57157 - acc: 0.7371 -- iter: 594/594\n",
      "--\n",
      "Training Step: 42  | total loss: \u001b[1m\u001b[32m0.63181\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 042 | loss: 0.63181 - acc: 0.6950 -- iter: 594/594\n",
      "--\n",
      "Training Step: 43  | total loss: \u001b[1m\u001b[32m0.59733\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 043 | loss: 0.59733 - acc: 0.7206 -- iter: 594/594\n",
      "--\n",
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m0.56996\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 044 | loss: 0.56996 - acc: 0.7416 -- iter: 594/594\n",
      "--\n",
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m0.54690\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 045 | loss: 0.54690 - acc: 0.7597 -- iter: 594/594\n",
      "--\n",
      "Training Step: 46  | total loss: \u001b[1m\u001b[32m0.52955\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 046 | loss: 0.52955 - acc: 0.7745 -- iter: 594/594\n",
      "--\n",
      "Training Step: 47  | total loss: \u001b[1m\u001b[32m0.51483\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 047 | loss: 0.51483 - acc: 0.7858 -- iter: 594/594\n",
      "--\n",
      "Training Step: 48  | total loss: \u001b[1m\u001b[32m0.50369\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 048 | loss: 0.50369 - acc: 0.7940 -- iter: 594/594\n",
      "--\n",
      "Training Step: 49  | total loss: \u001b[1m\u001b[32m0.49245\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 049 | loss: 0.49245 - acc: 0.8013 -- iter: 594/594\n",
      "--\n",
      "Training Step: 50  | total loss: \u001b[1m\u001b[32m0.54992\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 050 | loss: 0.54992 - acc: 0.7576 -- iter: 594/594\n",
      "--\n",
      "Training Step: 51  | total loss: \u001b[1m\u001b[32m0.53186\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 051 | loss: 0.53186 - acc: 0.7702 -- iter: 594/594\n",
      "--\n",
      "Training Step: 52  | total loss: \u001b[1m\u001b[32m0.51724\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 052 | loss: 0.51724 - acc: 0.7812 -- iter: 594/594\n",
      "--\n",
      "Training Step: 53  | total loss: \u001b[1m\u001b[32m0.50379\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 053 | loss: 0.50379 - acc: 0.7904 -- iter: 594/594\n",
      "--\n",
      "Training Step: 54  | total loss: \u001b[1m\u001b[32m0.49242\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 054 | loss: 0.49242 - acc: 0.7983 -- iter: 594/594\n",
      "--\n",
      "Training Step: 55  | total loss: \u001b[1m\u001b[32m0.48197\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 055 | loss: 0.48197 - acc: 0.8048 -- iter: 594/594\n",
      "--\n",
      "Training Step: 56  | total loss: \u001b[1m\u001b[32m0.47313\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 056 | loss: 0.47313 - acc: 0.8107 -- iter: 594/594\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 57  | total loss: \u001b[1m\u001b[32m0.46451\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 057 | loss: 0.46451 - acc: 0.8154 -- iter: 594/594\n",
      "--\n",
      "Training Step: 58  | total loss: \u001b[1m\u001b[32m0.52257\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 058 | loss: 0.52257 - acc: 0.7777 -- iter: 594/594\n",
      "--\n",
      "Training Step: 59  | total loss: \u001b[1m\u001b[32m0.50798\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 059 | loss: 0.50798 - acc: 0.7868 -- iter: 594/594\n",
      "--\n",
      "Training Step: 60  | total loss: \u001b[1m\u001b[32m0.56174\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 060 | loss: 0.56174 - acc: 0.7542 -- iter: 594/594\n",
      "--\n",
      "Training Step: 61  | total loss: \u001b[1m\u001b[32m0.54334\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 061 | loss: 0.54334 - acc: 0.7667 -- iter: 594/594\n",
      "--\n",
      "Training Step: 62  | total loss: \u001b[1m\u001b[32m0.52695\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 062 | loss: 0.52695 - acc: 0.7763 -- iter: 594/594\n",
      "--\n",
      "Training Step: 63  | total loss: \u001b[1m\u001b[32m0.51340\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 063 | loss: 0.51340 - acc: 0.7851 -- iter: 594/594\n",
      "--\n",
      "Training Step: 64  | total loss: \u001b[1m\u001b[32m0.55823\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 064 | loss: 0.55823 - acc: 0.7549 -- iter: 594/594\n",
      "--\n",
      "Training Step: 65  | total loss: \u001b[1m\u001b[32m0.54152\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 065 | loss: 0.54152 - acc: 0.7666 -- iter: 594/594\n",
      "--\n",
      "Training Step: 66  | total loss: \u001b[1m\u001b[32m0.52683\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 066 | loss: 0.52683 - acc: 0.7764 -- iter: 594/594\n",
      "--\n",
      "Training Step: 67  | total loss: \u001b[1m\u001b[32m0.51409\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 067 | loss: 0.51409 - acc: 0.7848 -- iter: 594/594\n",
      "--\n",
      "Training Step: 68  | total loss: \u001b[1m\u001b[32m0.50418\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 068 | loss: 0.50418 - acc: 0.7918 -- iter: 594/594\n",
      "--\n",
      "Training Step: 69  | total loss: \u001b[1m\u001b[32m0.49533\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 069 | loss: 0.49533 - acc: 0.7988 -- iter: 594/594\n",
      "--\n",
      "Training Step: 70  | total loss: \u001b[1m\u001b[32m0.48664\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 070 | loss: 0.48664 - acc: 0.8039 -- iter: 594/594\n",
      "--\n",
      "Training Step: 71  | total loss: \u001b[1m\u001b[32m0.47910\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 071 | loss: 0.47910 - acc: 0.8084 -- iter: 594/594\n",
      "--\n",
      "Training Step: 72  | total loss: \u001b[1m\u001b[32m0.52064\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 072 | loss: 0.52064 - acc: 0.7804 -- iter: 594/594\n",
      "--\n",
      "Training Step: 73  | total loss: \u001b[1m\u001b[32m0.50948\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 073 | loss: 0.50948 - acc: 0.7874 -- iter: 594/594\n",
      "--\n",
      "Training Step: 74  | total loss: \u001b[1m\u001b[32m0.50001\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 074 | loss: 0.50001 - acc: 0.7933 -- iter: 594/594\n",
      "--\n",
      "Training Step: 75  | total loss: \u001b[1m\u001b[32m0.49138\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 075 | loss: 0.49138 - acc: 0.7977 -- iter: 594/594\n",
      "--\n",
      "Training Step: 76  | total loss: \u001b[1m\u001b[32m0.48246\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 076 | loss: 0.48246 - acc: 0.8037 -- iter: 594/594\n",
      "--\n",
      "Training Step: 77  | total loss: \u001b[1m\u001b[32m0.47486\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 077 | loss: 0.47486 - acc: 0.8079 -- iter: 594/594\n",
      "--\n",
      "Training Step: 78  | total loss: \u001b[1m\u001b[32m0.46856\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 078 | loss: 0.46856 - acc: 0.8116 -- iter: 594/594\n",
      "--\n",
      "Training Step: 79  | total loss: \u001b[1m\u001b[32m0.46347\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 079 | loss: 0.46347 - acc: 0.8151 -- iter: 594/594\n",
      "--\n",
      "Training Step: 80  | total loss: \u001b[1m\u001b[32m0.51170\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 080 | loss: 0.51170 - acc: 0.7853 -- iter: 594/594\n",
      "--\n",
      "Training Step: 81  | total loss: \u001b[1m\u001b[32m0.50090\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 081 | loss: 0.50090 - acc: 0.7905 -- iter: 594/594\n",
      "--\n",
      "Training Step: 82  | total loss: \u001b[1m\u001b[32m0.49195\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 082 | loss: 0.49195 - acc: 0.7959 -- iter: 594/594\n",
      "--\n",
      "Training Step: 83  | total loss: \u001b[1m\u001b[32m0.48334\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 083 | loss: 0.48334 - acc: 0.8015 -- iter: 594/594\n",
      "--\n",
      "Training Step: 84  | total loss: \u001b[1m\u001b[32m0.52819\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 084 | loss: 0.52819 - acc: 0.7756 -- iter: 594/594\n",
      "--\n",
      "Training Step: 85  | total loss: \u001b[1m\u001b[32m0.51692\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 085 | loss: 0.51692 - acc: 0.7822 -- iter: 594/594\n",
      "--\n",
      "Training Step: 86  | total loss: \u001b[1m\u001b[32m0.55764\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 086 | loss: 0.55764 - acc: 0.7572 -- iter: 594/594\n",
      "--\n",
      "Training Step: 87  | total loss: \u001b[1m\u001b[32m0.54395\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 087 | loss: 0.54395 - acc: 0.7665 -- iter: 594/594\n",
      "--\n",
      "Training Step: 88  | total loss: \u001b[1m\u001b[32m0.57392\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 088 | loss: 0.57392 - acc: 0.7452 -- iter: 594/594\n",
      "--\n",
      "Training Step: 89  | total loss: \u001b[1m\u001b[32m0.55939\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 089 | loss: 0.55939 - acc: 0.7555 -- iter: 594/594\n",
      "--\n",
      "Training Step: 90  | total loss: \u001b[1m\u001b[32m0.54691\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 090 | loss: 0.54691 - acc: 0.7645 -- iter: 594/594\n",
      "--\n",
      "Training Step: 91  | total loss: \u001b[1m\u001b[32m0.53609\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 091 | loss: 0.53609 - acc: 0.7724 -- iter: 594/594\n",
      "--\n",
      "Training Step: 92  | total loss: \u001b[1m\u001b[32m0.56165\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 092 | loss: 0.56165 - acc: 0.7504 -- iter: 594/594\n",
      "--\n",
      "Training Step: 93  | total loss: \u001b[1m\u001b[32m0.55018\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 093 | loss: 0.55018 - acc: 0.7595 -- iter: 594/594\n",
      "--\n",
      "Training Step: 94  | total loss: \u001b[1m\u001b[32m0.54057\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 094 | loss: 0.54057 - acc: 0.7679 -- iter: 594/594\n",
      "--\n",
      "Training Step: 95  | total loss: \u001b[1m\u001b[32m0.53174\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 095 | loss: 0.53174 - acc: 0.7751 -- iter: 594/594\n",
      "--\n",
      "Training Step: 96  | total loss: \u001b[1m\u001b[32m0.52401\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 096 | loss: 0.52401 - acc: 0.7813 -- iter: 594/594\n",
      "--\n",
      "Training Step: 97  | total loss: \u001b[1m\u001b[32m0.51664\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 097 | loss: 0.51664 - acc: 0.7870 -- iter: 594/594\n",
      "--\n",
      "Training Step: 98  | total loss: \u001b[1m\u001b[32m0.50982\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 098 | loss: 0.50982 - acc: 0.7928 -- iter: 594/594\n",
      "--\n",
      "Training Step: 99  | total loss: \u001b[1m\u001b[32m0.50349\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 099 | loss: 0.50349 - acc: 0.7982 -- iter: 594/594\n",
      "--\n",
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m0.49685\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 100 | loss: 0.49685 - acc: 0.8034 -- iter: 594/594\n",
      "--\n",
      "Training Step: 101  | total loss: \u001b[1m\u001b[32m0.49078\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 101 | loss: 0.49078 - acc: 0.8074 -- iter: 594/594\n",
      "--\n",
      "Training Step: 102  | total loss: \u001b[1m\u001b[32m0.48463\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 102 | loss: 0.48463 - acc: 0.8108 -- iter: 594/594\n",
      "--\n",
      "Training Step: 103  | total loss: \u001b[1m\u001b[32m0.47867\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 103 | loss: 0.47867 - acc: 0.8136 -- iter: 594/594\n",
      "--\n",
      "Training Step: 104  | total loss: \u001b[1m\u001b[32m0.52270\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 104 | loss: 0.52270 - acc: 0.7844 -- iter: 594/594\n",
      "--\n",
      "Training Step: 105  | total loss: \u001b[1m\u001b[32m0.51222\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 105 | loss: 0.51222 - acc: 0.7896 -- iter: 594/594\n",
      "--\n",
      "Training Step: 106  | total loss: \u001b[1m\u001b[32m0.55030\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 106 | loss: 0.55030 - acc: 0.7620 -- iter: 594/594\n",
      "--\n",
      "Training Step: 107  | total loss: \u001b[1m\u001b[32m0.53715\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 107 | loss: 0.53715 - acc: 0.7707 -- iter: 594/594\n",
      "--\n",
      "Training Step: 108  | total loss: \u001b[1m\u001b[32m0.57249\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 108 | loss: 0.57249 - acc: 0.7453 -- iter: 594/594\n",
      "--\n",
      "Training Step: 109  | total loss: \u001b[1m\u001b[32m0.55768\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 109 | loss: 0.55768 - acc: 0.7556 -- iter: 594/594\n",
      "--\n",
      "Training Step: 110  | total loss: \u001b[1m\u001b[32m0.58479\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 110 | loss: 0.58479 - acc: 0.7346 -- iter: 594/594\n",
      "--\n",
      "Training Step: 111  | total loss: \u001b[1m\u001b[32m0.56982\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 111 | loss: 0.56982 - acc: 0.7458 -- iter: 594/594\n",
      "--\n",
      "Training Step: 112  | total loss: \u001b[1m\u001b[32m0.55622\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 112 | loss: 0.55622 - acc: 0.7556 -- iter: 594/594\n",
      "--\n",
      "Training Step: 113  | total loss: \u001b[1m\u001b[32m0.54488\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 113 | loss: 0.54488 - acc: 0.7640 -- iter: 594/594\n",
      "--\n",
      "Training Step: 114  | total loss: \u001b[1m\u001b[32m0.56770\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 114 | loss: 0.56770 - acc: 0.7425 -- iter: 594/594\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 115  | total loss: \u001b[1m\u001b[32m0.55522\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 115 | loss: 0.55522 - acc: 0.7514 -- iter: 594/594\n",
      "--\n",
      "Training Step: 116  | total loss: \u001b[1m\u001b[32m0.54437\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 116 | loss: 0.54437 - acc: 0.7605 -- iter: 594/594\n",
      "--\n",
      "Training Step: 117  | total loss: \u001b[1m\u001b[32m0.53444\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 117 | loss: 0.53444 - acc: 0.7682 -- iter: 594/594\n",
      "--\n",
      "Training Step: 118  | total loss: \u001b[1m\u001b[32m0.52589\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 118 | loss: 0.52589 - acc: 0.7749 -- iter: 594/594\n",
      "--\n",
      "Training Step: 119  | total loss: \u001b[1m\u001b[32m0.51775\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 119 | loss: 0.51775 - acc: 0.7814 -- iter: 594/594\n",
      "--\n",
      "Training Step: 120  | total loss: \u001b[1m\u001b[32m0.51036\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 120 | loss: 0.51036 - acc: 0.7875 -- iter: 594/594\n",
      "--\n",
      "Training Step: 121  | total loss: \u001b[1m\u001b[32m0.50293\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 121 | loss: 0.50293 - acc: 0.7936 -- iter: 594/594\n",
      "--\n",
      "Training Step: 122  | total loss: \u001b[1m\u001b[32m0.53498\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 122 | loss: 0.53498 - acc: 0.7683 -- iter: 594/594\n",
      "--\n",
      "Training Step: 123  | total loss: \u001b[1m\u001b[32m0.52482\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 123 | loss: 0.52482 - acc: 0.7758 -- iter: 594/594\n",
      "--\n",
      "Training Step: 124  | total loss: \u001b[1m\u001b[32m0.51576\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 124 | loss: 0.51576 - acc: 0.7822 -- iter: 594/594\n",
      "--\n",
      "Training Step: 125  | total loss: \u001b[1m\u001b[32m0.50741\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 125 | loss: 0.50741 - acc: 0.7882 -- iter: 594/594\n",
      "--\n",
      "Training Step: 126  | total loss: \u001b[1m\u001b[32m0.49925\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 126 | loss: 0.49925 - acc: 0.7937 -- iter: 594/594\n",
      "--\n",
      "Training Step: 127  | total loss: \u001b[1m\u001b[32m0.49128\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 127 | loss: 0.49128 - acc: 0.7990 -- iter: 594/594\n",
      "--\n",
      "Training Step: 128  | total loss: \u001b[1m\u001b[32m0.48407\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 128 | loss: 0.48407 - acc: 0.8029 -- iter: 594/594\n",
      "--\n",
      "Training Step: 129  | total loss: \u001b[1m\u001b[32m0.47756\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 129 | loss: 0.47756 - acc: 0.8063 -- iter: 594/594\n",
      "--\n",
      "Training Step: 130  | total loss: \u001b[1m\u001b[32m0.47129\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 130 | loss: 0.47129 - acc: 0.8102 -- iter: 594/594\n",
      "--\n",
      "Training Step: 131  | total loss: \u001b[1m\u001b[32m0.46407\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 131 | loss: 0.46407 - acc: 0.8144 -- iter: 594/594\n",
      "--\n",
      "Training Step: 132  | total loss: \u001b[1m\u001b[32m0.45876\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 132 | loss: 0.45876 - acc: 0.8176 -- iter: 594/594\n",
      "--\n",
      "Training Step: 133  | total loss: \u001b[1m\u001b[32m0.45282\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 133 | loss: 0.45282 - acc: 0.8198 -- iter: 594/594\n",
      "--\n",
      "Training Step: 134  | total loss: \u001b[1m\u001b[32m0.44787\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 134 | loss: 0.44787 - acc: 0.8222 -- iter: 594/594\n",
      "--\n",
      "Training Step: 135  | total loss: \u001b[1m\u001b[32m0.44313\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 135 | loss: 0.44313 - acc: 0.8242 -- iter: 594/594\n",
      "--\n",
      "Training Step: 136  | total loss: \u001b[1m\u001b[32m0.50363\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 136 | loss: 0.50363 - acc: 0.7938 -- iter: 594/594\n",
      "--\n",
      "Training Step: 137  | total loss: \u001b[1m\u001b[32m0.49320\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 137 | loss: 0.49320 - acc: 0.7991 -- iter: 594/594\n",
      "--\n",
      "Training Step: 138  | total loss: \u001b[1m\u001b[32m0.48406\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 138 | loss: 0.48406 - acc: 0.8040 -- iter: 594/594\n",
      "--\n",
      "Training Step: 139  | total loss: \u001b[1m\u001b[32m0.47539\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 139 | loss: 0.47539 - acc: 0.8081 -- iter: 594/594\n",
      "--\n",
      "Training Step: 140  | total loss: \u001b[1m\u001b[32m0.46776\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 140 | loss: 0.46776 - acc: 0.8118 -- iter: 594/594\n",
      "--\n",
      "Training Step: 141  | total loss: \u001b[1m\u001b[32m0.46128\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 141 | loss: 0.46128 - acc: 0.8155 -- iter: 594/594\n",
      "--\n",
      "Training Step: 142  | total loss: \u001b[1m\u001b[32m0.52170\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 142 | loss: 0.52170 - acc: 0.7848 -- iter: 594/594\n",
      "--\n",
      "Training Step: 143  | total loss: \u001b[1m\u001b[32m0.51028\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 143 | loss: 0.51028 - acc: 0.7910 -- iter: 594/594\n",
      "--\n",
      "Training Step: 144  | total loss: \u001b[1m\u001b[32m0.55524\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 144 | loss: 0.55524 - acc: 0.7653 -- iter: 594/594\n",
      "--\n",
      "Training Step: 145  | total loss: \u001b[1m\u001b[32m0.54047\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 145 | loss: 0.54047 - acc: 0.7729 -- iter: 594/594\n",
      "--\n",
      "Training Step: 146  | total loss: \u001b[1m\u001b[32m0.52800\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 146 | loss: 0.52800 - acc: 0.7796 -- iter: 594/594\n",
      "--\n",
      "Training Step: 147  | total loss: \u001b[1m\u001b[32m0.51734\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 147 | loss: 0.51734 - acc: 0.7855 -- iter: 594/594\n",
      "--\n",
      "Training Step: 148  | total loss: \u001b[1m\u001b[32m0.50776\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 148 | loss: 0.50776 - acc: 0.7904 -- iter: 594/594\n",
      "--\n",
      "Training Step: 149  | total loss: \u001b[1m\u001b[32m0.49962\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 149 | loss: 0.49962 - acc: 0.7952 -- iter: 594/594\n",
      "--\n",
      "Training Step: 150  | total loss: \u001b[1m\u001b[32m0.49212\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 150 | loss: 0.49212 - acc: 0.8001 -- iter: 594/594\n",
      "--\n",
      "Training Step: 151  | total loss: \u001b[1m\u001b[32m0.48572\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 151 | loss: 0.48572 - acc: 0.8041 -- iter: 594/594\n",
      "--\n",
      "Training Step: 152  | total loss: \u001b[1m\u001b[32m0.52144\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 152 | loss: 0.52144 - acc: 0.7770 -- iter: 594/594\n",
      "--\n",
      "Training Step: 153  | total loss: \u001b[1m\u001b[32m0.51227\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 153 | loss: 0.51227 - acc: 0.7835 -- iter: 594/594\n",
      "--\n",
      "Training Step: 154  | total loss: \u001b[1m\u001b[32m0.50496\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 154 | loss: 0.50496 - acc: 0.7888 -- iter: 594/594\n",
      "--\n",
      "Training Step: 155  | total loss: \u001b[1m\u001b[32m0.49729\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 155 | loss: 0.49729 - acc: 0.7943 -- iter: 594/594\n",
      "--\n",
      "Training Step: 156  | total loss: \u001b[1m\u001b[32m0.53422\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 156 | loss: 0.53422 - acc: 0.7667 -- iter: 594/594\n",
      "--\n",
      "Training Step: 157  | total loss: \u001b[1m\u001b[32m0.52406\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 157 | loss: 0.52406 - acc: 0.7749 -- iter: 594/594\n",
      "--\n",
      "Training Step: 158  | total loss: \u001b[1m\u001b[32m0.51487\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 158 | loss: 0.51487 - acc: 0.7816 -- iter: 594/594\n",
      "--\n",
      "Training Step: 159  | total loss: \u001b[1m\u001b[32m0.50680\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 159 | loss: 0.50680 - acc: 0.7878 -- iter: 594/594\n",
      "--\n",
      "Training Step: 160  | total loss: \u001b[1m\u001b[32m0.50008\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 160 | loss: 0.50008 - acc: 0.7933 -- iter: 594/594\n",
      "--\n",
      "Training Step: 161  | total loss: \u001b[1m\u001b[32m0.49296\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 161 | loss: 0.49296 - acc: 0.7983 -- iter: 594/594\n",
      "--\n",
      "Training Step: 162  | total loss: \u001b[1m\u001b[32m0.48675\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 162 | loss: 0.48675 - acc: 0.8032 -- iter: 594/594\n",
      "--\n",
      "Training Step: 163  | total loss: \u001b[1m\u001b[32m0.48088\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 163 | loss: 0.48088 - acc: 0.8069 -- iter: 594/594\n",
      "--\n",
      "Training Step: 164  | total loss: \u001b[1m\u001b[32m0.47509\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 164 | loss: 0.47509 - acc: 0.8105 -- iter: 594/594\n",
      "--\n",
      "Training Step: 165  | total loss: \u001b[1m\u001b[32m0.46981\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 165 | loss: 0.46981 - acc: 0.8143 -- iter: 594/594\n",
      "--\n",
      "Training Step: 166  | total loss: \u001b[1m\u001b[32m0.46433\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 166 | loss: 0.46433 - acc: 0.8172 -- iter: 594/594\n",
      "--\n",
      "Training Step: 167  | total loss: \u001b[1m\u001b[32m0.45888\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 167 | loss: 0.45888 - acc: 0.8204 -- iter: 594/594\n",
      "--\n",
      "Training Step: 168  | total loss: \u001b[1m\u001b[32m0.49467\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 168 | loss: 0.49467 - acc: 0.7984 -- iter: 594/594\n",
      "--\n",
      "Training Step: 169  | total loss: \u001b[1m\u001b[32m0.48605\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 169 | loss: 0.48605 - acc: 0.8024 -- iter: 594/594\n",
      "--\n",
      "Training Step: 170  | total loss: \u001b[1m\u001b[32m0.52921\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 170 | loss: 0.52921 - acc: 0.7757 -- iter: 594/594\n",
      "--\n",
      "Training Step: 171  | total loss: \u001b[1m\u001b[32m0.51664\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 171 | loss: 0.51664 - acc: 0.7827 -- iter: 594/594\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 172  | total loss: \u001b[1m\u001b[32m0.50557\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 172 | loss: 0.50557 - acc: 0.7896 -- iter: 594/594\n",
      "--\n",
      "Training Step: 173  | total loss: \u001b[1m\u001b[32m0.49576\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 173 | loss: 0.49576 - acc: 0.7950 -- iter: 594/594\n",
      "--\n",
      "Training Step: 174  | total loss: \u001b[1m\u001b[32m0.48660\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 174 | loss: 0.48660 - acc: 0.7996 -- iter: 594/594\n",
      "--\n",
      "Training Step: 175  | total loss: \u001b[1m\u001b[32m0.47890\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 175 | loss: 0.47890 - acc: 0.8045 -- iter: 594/594\n",
      "--\n",
      "Training Step: 176  | total loss: \u001b[1m\u001b[32m0.47098\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 176 | loss: 0.47098 - acc: 0.8091 -- iter: 594/594\n",
      "--\n",
      "Training Step: 177  | total loss: \u001b[1m\u001b[32m0.46414\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 177 | loss: 0.46414 - acc: 0.8134 -- iter: 594/594\n",
      "--\n",
      "Training Step: 178  | total loss: \u001b[1m\u001b[32m0.45807\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 178 | loss: 0.45807 - acc: 0.8167 -- iter: 594/594\n",
      "--\n",
      "Training Step: 179  | total loss: \u001b[1m\u001b[32m0.45308\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 179 | loss: 0.45308 - acc: 0.8199 -- iter: 594/594\n",
      "--\n",
      "Training Step: 180  | total loss: \u001b[1m\u001b[32m0.50439\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 180 | loss: 0.50439 - acc: 0.7908 -- iter: 594/594\n",
      "--\n",
      "Training Step: 181  | total loss: \u001b[1m\u001b[32m0.49403\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 181 | loss: 0.49403 - acc: 0.7960 -- iter: 594/594\n",
      "--\n",
      "Training Step: 182  | total loss: \u001b[1m\u001b[32m0.54362\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 182 | loss: 0.54362 - acc: 0.7679 -- iter: 594/594\n",
      "--\n",
      "Training Step: 183  | total loss: \u001b[1m\u001b[32m0.53051\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 183 | loss: 0.53051 - acc: 0.7752 -- iter: 594/594\n",
      "--\n",
      "Training Step: 184  | total loss: \u001b[1m\u001b[32m0.56764\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 184 | loss: 0.56764 - acc: 0.7507 -- iter: 594/594\n",
      "--\n",
      "Training Step: 185  | total loss: \u001b[1m\u001b[32m0.55277\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 185 | loss: 0.55277 - acc: 0.7598 -- iter: 594/594\n",
      "--\n",
      "Training Step: 186  | total loss: \u001b[1m\u001b[32m0.54033\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 186 | loss: 0.54033 - acc: 0.7681 -- iter: 594/594\n",
      "--\n",
      "Training Step: 187  | total loss: \u001b[1m\u001b[32m0.52945\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 187 | loss: 0.52945 - acc: 0.7762 -- iter: 594/594\n",
      "--\n",
      "Training Step: 188  | total loss: \u001b[1m\u001b[32m0.55960\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 188 | loss: 0.55960 - acc: 0.7529 -- iter: 594/594\n",
      "--\n",
      "Training Step: 189  | total loss: \u001b[1m\u001b[32m0.54748\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 189 | loss: 0.54748 - acc: 0.7620 -- iter: 594/594\n",
      "--\n",
      "Training Step: 190  | total loss: \u001b[1m\u001b[32m0.53638\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 190 | loss: 0.53638 - acc: 0.7701 -- iter: 594/594\n",
      "--\n",
      "Training Step: 191  | total loss: \u001b[1m\u001b[32m0.52712\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 191 | loss: 0.52712 - acc: 0.7771 -- iter: 594/594\n",
      "--\n",
      "Training Step: 192  | total loss: \u001b[1m\u001b[32m0.51916\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 192 | loss: 0.51916 - acc: 0.7838 -- iter: 594/594\n",
      "--\n",
      "Training Step: 193  | total loss: \u001b[1m\u001b[32m0.51164\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 193 | loss: 0.51164 - acc: 0.7902 -- iter: 594/594\n",
      "--\n",
      "Training Step: 194  | total loss: \u001b[1m\u001b[32m0.54013\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 194 | loss: 0.54013 - acc: 0.7641 -- iter: 594/594\n",
      "--\n",
      "Training Step: 195  | total loss: \u001b[1m\u001b[32m0.53060\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 195 | loss: 0.53060 - acc: 0.7717 -- iter: 594/594\n",
      "--\n",
      "Training Step: 196  | total loss: \u001b[1m\u001b[32m0.55758\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 196 | loss: 0.55758 - acc: 0.7480 -- iter: 594/594\n",
      "--\n",
      "Training Step: 197  | total loss: \u001b[1m\u001b[32m0.54683\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 197 | loss: 0.54683 - acc: 0.7576 -- iter: 594/594\n",
      "--\n",
      "Training Step: 198  | total loss: \u001b[1m\u001b[32m0.56835\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 198 | loss: 0.56835 - acc: 0.7372 -- iter: 594/594\n",
      "--\n",
      "Training Step: 199  | total loss: \u001b[1m\u001b[32m0.55711\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 199 | loss: 0.55711 - acc: 0.7477 -- iter: 594/594\n",
      "--\n",
      "Training Step: 200  | total loss: \u001b[1m\u001b[32m0.54728\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 200 | loss: 0.54728 - acc: 0.7574 -- iter: 594/594\n",
      "--\n",
      "Training Step: 201  | total loss: \u001b[1m\u001b[32m0.53811\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 201 | loss: 0.53811 - acc: 0.7662 -- iter: 594/594\n",
      "--\n",
      "Training Step: 202  | total loss: \u001b[1m\u001b[32m0.56042\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 202 | loss: 0.56042 - acc: 0.7443 -- iter: 594/594\n",
      "--\n",
      "Training Step: 203  | total loss: \u001b[1m\u001b[32m0.55018\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 203 | loss: 0.55018 - acc: 0.7540 -- iter: 594/594\n",
      "--\n",
      "Training Step: 204  | total loss: \u001b[1m\u001b[32m0.54024\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 204 | loss: 0.54024 - acc: 0.7631 -- iter: 594/594\n",
      "--\n",
      "Training Step: 205  | total loss: \u001b[1m\u001b[32m0.53200\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 205 | loss: 0.53200 - acc: 0.7720 -- iter: 594/594\n",
      "--\n",
      "Training Step: 206  | total loss: \u001b[1m\u001b[32m0.52394\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 206 | loss: 0.52394 - acc: 0.7797 -- iter: 594/594\n",
      "--\n",
      "Training Step: 207  | total loss: \u001b[1m\u001b[32m0.51671\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 207 | loss: 0.51671 - acc: 0.7845 -- iter: 594/594\n",
      "--\n",
      "Training Step: 208  | total loss: \u001b[1m\u001b[32m0.54804\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 208 | loss: 0.54804 - acc: 0.7564 -- iter: 594/594\n",
      "--\n",
      "Training Step: 209  | total loss: \u001b[1m\u001b[32m0.53750\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 209 | loss: 0.53750 - acc: 0.7648 -- iter: 594/594\n",
      "--\n",
      "Training Step: 210  | total loss: \u001b[1m\u001b[32m0.52754\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 210 | loss: 0.52754 - acc: 0.7721 -- iter: 594/594\n",
      "--\n",
      "Training Step: 211  | total loss: \u001b[1m\u001b[32m0.51871\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 211 | loss: 0.51871 - acc: 0.7793 -- iter: 594/594\n",
      "--\n",
      "Training Step: 212  | total loss: \u001b[1m\u001b[32m0.54903\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 212 | loss: 0.54903 - acc: 0.7547 -- iter: 594/594\n",
      "--\n",
      "Training Step: 213  | total loss: \u001b[1m\u001b[32m0.53805\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 213 | loss: 0.53805 - acc: 0.7632 -- iter: 594/594\n",
      "--\n",
      "Training Step: 214  | total loss: \u001b[1m\u001b[32m0.52785\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 214 | loss: 0.52785 - acc: 0.7714 -- iter: 594/594\n",
      "--\n",
      "Training Step: 215  | total loss: \u001b[1m\u001b[32m0.51826\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 215 | loss: 0.51826 - acc: 0.7786 -- iter: 594/594\n",
      "--\n",
      "Training Step: 216  | total loss: \u001b[1m\u001b[32m0.50952\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 216 | loss: 0.50952 - acc: 0.7846 -- iter: 594/594\n",
      "--\n",
      "Training Step: 217  | total loss: \u001b[1m\u001b[32m0.50120\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 217 | loss: 0.50120 - acc: 0.7905 -- iter: 594/594\n",
      "--\n",
      "Training Step: 218  | total loss: \u001b[1m\u001b[32m0.49306\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 218 | loss: 0.49306 - acc: 0.7959 -- iter: 594/594\n",
      "--\n",
      "Training Step: 219  | total loss: \u001b[1m\u001b[32m0.48542\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 219 | loss: 0.48542 - acc: 0.8007 -- iter: 594/594\n",
      "--\n",
      "Training Step: 220  | total loss: \u001b[1m\u001b[32m0.52361\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 220 | loss: 0.52361 - acc: 0.7767 -- iter: 594/594\n",
      "--\n",
      "Training Step: 221  | total loss: \u001b[1m\u001b[32m0.51258\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 221 | loss: 0.51258 - acc: 0.7835 -- iter: 594/594\n",
      "--\n",
      "Training Step: 222  | total loss: \u001b[1m\u001b[32m0.55548\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 222 | loss: 0.55548 - acc: 0.7550 -- iter: 594/594\n",
      "--\n",
      "Training Step: 223  | total loss: \u001b[1m\u001b[32m0.54130\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 223 | loss: 0.54130 - acc: 0.7633 -- iter: 594/594\n",
      "--\n",
      "Training Step: 224  | total loss: \u001b[1m\u001b[32m0.52870\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 224 | loss: 0.52870 - acc: 0.7715 -- iter: 594/594\n",
      "--\n",
      "Training Step: 225  | total loss: \u001b[1m\u001b[32m0.51747\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 225 | loss: 0.51747 - acc: 0.7789 -- iter: 594/594\n",
      "--\n",
      "Training Step: 226  | total loss: \u001b[1m\u001b[32m0.55375\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 226 | loss: 0.55375 - acc: 0.7544 -- iter: 594/594\n",
      "--\n",
      "Training Step: 227  | total loss: \u001b[1m\u001b[32m0.54045\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 227 | loss: 0.54045 - acc: 0.7636 -- iter: 594/594\n",
      "--\n",
      "Training Step: 228  | total loss: \u001b[1m\u001b[32m0.52825\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 228 | loss: 0.52825 - acc: 0.7716 -- iter: 594/594\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 229  | total loss: \u001b[1m\u001b[32m0.51733\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 229 | loss: 0.51733 - acc: 0.7789 -- iter: 594/594\n",
      "--\n",
      "Training Step: 230  | total loss: \u001b[1m\u001b[32m0.50787\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 230 | loss: 0.50787 - acc: 0.7852 -- iter: 594/594\n",
      "--\n",
      "Training Step: 231  | total loss: \u001b[1m\u001b[32m0.49935\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 231 | loss: 0.49935 - acc: 0.7912 -- iter: 594/594\n",
      "--\n",
      "Training Step: 232  | total loss: \u001b[1m\u001b[32m0.53318\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 232 | loss: 0.53318 - acc: 0.7683 -- iter: 594/594\n",
      "--\n",
      "Training Step: 233  | total loss: \u001b[1m\u001b[32m0.52226\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 233 | loss: 0.52226 - acc: 0.7755 -- iter: 594/594\n",
      "--\n",
      "Training Step: 234  | total loss: \u001b[1m\u001b[32m0.55605\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 234 | loss: 0.55605 - acc: 0.7508 -- iter: 594/594\n",
      "--\n",
      "Training Step: 235  | total loss: \u001b[1m\u001b[32m0.54325\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 235 | loss: 0.54325 - acc: 0.7596 -- iter: 594/594\n",
      "--\n",
      "Training Step: 236  | total loss: \u001b[1m\u001b[32m0.57497\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 236 | loss: 0.57497 - acc: 0.7370 -- iter: 594/594\n",
      "--\n",
      "Training Step: 237  | total loss: \u001b[1m\u001b[32m0.56079\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 237 | loss: 0.56079 - acc: 0.7480 -- iter: 594/594\n",
      "--\n",
      "Training Step: 238  | total loss: \u001b[1m\u001b[32m0.54836\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 238 | loss: 0.54836 - acc: 0.7575 -- iter: 594/594\n",
      "--\n",
      "Training Step: 239  | total loss: \u001b[1m\u001b[32m0.53736\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 239 | loss: 0.53736 - acc: 0.7671 -- iter: 594/594\n",
      "--\n",
      "Training Step: 240  | total loss: \u001b[1m\u001b[32m0.56303\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 240 | loss: 0.56303 - acc: 0.7444 -- iter: 594/594\n",
      "--\n",
      "Training Step: 241  | total loss: \u001b[1m\u001b[32m0.55141\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 241 | loss: 0.55141 - acc: 0.7542 -- iter: 594/594\n",
      "--\n",
      "Training Step: 242  | total loss: \u001b[1m\u001b[32m0.54070\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 242 | loss: 0.54070 - acc: 0.7631 -- iter: 594/594\n",
      "--\n",
      "Training Step: 243  | total loss: \u001b[1m\u001b[32m0.53128\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 243 | loss: 0.53128 - acc: 0.7715 -- iter: 594/594\n",
      "--\n",
      "Training Step: 244  | total loss: \u001b[1m\u001b[32m0.52306\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 244 | loss: 0.52306 - acc: 0.7792 -- iter: 594/594\n",
      "--\n",
      "Training Step: 245  | total loss: \u001b[1m\u001b[32m0.51556\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 245 | loss: 0.51556 - acc: 0.7853 -- iter: 594/594\n",
      "--\n",
      "Training Step: 246  | total loss: \u001b[1m\u001b[32m0.50865\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 246 | loss: 0.50865 - acc: 0.7909 -- iter: 594/594\n",
      "--\n",
      "Training Step: 247  | total loss: \u001b[1m\u001b[32m0.50179\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 247 | loss: 0.50179 - acc: 0.7962 -- iter: 594/594\n",
      "--\n",
      "Training Step: 248  | total loss: \u001b[1m\u001b[32m0.49506\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 248 | loss: 0.49506 - acc: 0.8006 -- iter: 594/594\n",
      "--\n",
      "Training Step: 249  | total loss: \u001b[1m\u001b[32m0.48848\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 249 | loss: 0.48848 - acc: 0.8042 -- iter: 594/594\n",
      "--\n",
      "Training Step: 250  | total loss: \u001b[1m\u001b[32m0.52508\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 250 | loss: 0.52508 - acc: 0.7766 -- iter: 594/594\n",
      "--\n",
      "Training Step: 251  | total loss: \u001b[1m\u001b[32m0.51482\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 251 | loss: 0.51482 - acc: 0.7836 -- iter: 594/594\n",
      "--\n",
      "Training Step: 252  | total loss: \u001b[1m\u001b[32m0.55188\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 252 | loss: 0.55188 - acc: 0.7558 -- iter: 594/594\n",
      "--\n",
      "Training Step: 253  | total loss: \u001b[1m\u001b[32m0.53936\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 253 | loss: 0.53936 - acc: 0.7647 -- iter: 594/594\n",
      "--\n",
      "Training Step: 254  | total loss: \u001b[1m\u001b[32m0.56706\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 254 | loss: 0.56706 - acc: 0.7438 -- iter: 594/594\n",
      "--\n",
      "Training Step: 255  | total loss: \u001b[1m\u001b[32m0.55312\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 255 | loss: 0.55312 - acc: 0.7536 -- iter: 594/594\n",
      "--\n",
      "Training Step: 256  | total loss: \u001b[1m\u001b[32m0.58147\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 256 | loss: 0.58147 - acc: 0.7329 -- iter: 594/594\n",
      "--\n",
      "Training Step: 257  | total loss: \u001b[1m\u001b[32m0.56639\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 257 | loss: 0.56639 - acc: 0.7438 -- iter: 594/594\n",
      "--\n",
      "Training Step: 258  | total loss: \u001b[1m\u001b[32m0.55289\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 258 | loss: 0.55289 - acc: 0.7543 -- iter: 594/594\n",
      "--\n",
      "Training Step: 259  | total loss: \u001b[1m\u001b[32m0.54135\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 259 | loss: 0.54135 - acc: 0.7630 -- iter: 594/594\n",
      "--\n",
      "Training Step: 260  | total loss: \u001b[1m\u001b[32m0.53115\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 260 | loss: 0.53115 - acc: 0.7712 -- iter: 594/594\n",
      "--\n",
      "Training Step: 261  | total loss: \u001b[1m\u001b[32m0.52190\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 261 | loss: 0.52190 - acc: 0.7785 -- iter: 594/594\n",
      "--\n",
      "Training Step: 262  | total loss: \u001b[1m\u001b[32m0.55141\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 262 | loss: 0.55141 - acc: 0.7536 -- iter: 594/594\n",
      "--\n",
      "Training Step: 263  | total loss: \u001b[1m\u001b[32m0.54008\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 263 | loss: 0.54008 - acc: 0.7626 -- iter: 594/594\n",
      "--\n",
      "Training Step: 264  | total loss: \u001b[1m\u001b[32m0.53011\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 264 | loss: 0.53011 - acc: 0.7704 -- iter: 594/594\n",
      "--\n",
      "Training Step: 265  | total loss: \u001b[1m\u001b[32m0.52131\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 265 | loss: 0.52131 - acc: 0.7772 -- iter: 594/594\n",
      "--\n",
      "Training Step: 266  | total loss: \u001b[1m\u001b[32m0.55572\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 266 | loss: 0.55572 - acc: 0.7484 -- iter: 594/594\n",
      "--\n",
      "Training Step: 267  | total loss: \u001b[1m\u001b[32m0.54381\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 267 | loss: 0.54381 - acc: 0.7576 -- iter: 594/594\n",
      "--\n",
      "Training Step: 268  | total loss: \u001b[1m\u001b[32m0.57127\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 268 | loss: 0.57127 - acc: 0.7340 -- iter: 594/594\n",
      "--\n",
      "Training Step: 269  | total loss: \u001b[1m\u001b[32m0.55800\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 269 | loss: 0.55800 - acc: 0.7448 -- iter: 594/594\n",
      "--\n",
      "Training Step: 270  | total loss: \u001b[1m\u001b[32m0.58378\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 270 | loss: 0.58378 - acc: 0.7225 -- iter: 594/594\n",
      "--\n",
      "Training Step: 271  | total loss: \u001b[1m\u001b[32m0.56960\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 271 | loss: 0.56960 - acc: 0.7348 -- iter: 594/594\n",
      "--\n",
      "Training Step: 272  | total loss: \u001b[1m\u001b[32m0.58769\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 272 | loss: 0.58769 - acc: 0.7187 -- iter: 594/594\n",
      "--\n",
      "Training Step: 273  | total loss: \u001b[1m\u001b[32m0.57404\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 273 | loss: 0.57404 - acc: 0.7302 -- iter: 594/594\n",
      "--\n",
      "Training Step: 274  | total loss: \u001b[1m\u001b[32m0.59436\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 274 | loss: 0.59436 - acc: 0.7114 -- iter: 594/594\n",
      "--\n",
      "Training Step: 275  | total loss: \u001b[1m\u001b[32m0.58095\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 275 | loss: 0.58095 - acc: 0.7239 -- iter: 594/594\n",
      "--\n",
      "Training Step: 276  | total loss: \u001b[1m\u001b[32m0.59963\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 276 | loss: 0.59963 - acc: 0.7055 -- iter: 594/594\n",
      "--\n",
      "Training Step: 277  | total loss: \u001b[1m\u001b[32m0.58627\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 277 | loss: 0.58627 - acc: 0.7187 -- iter: 594/594\n",
      "--\n",
      "Training Step: 278  | total loss: \u001b[1m\u001b[32m0.60557\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 278 | loss: 0.60557 - acc: 0.6986 -- iter: 594/594\n",
      "--\n",
      "Training Step: 279  | total loss: \u001b[1m\u001b[32m0.59257\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 279 | loss: 0.59257 - acc: 0.7108 -- iter: 594/594\n",
      "--\n",
      "Training Step: 280  | total loss: \u001b[1m\u001b[32m0.58096\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 280 | loss: 0.58096 - acc: 0.7229 -- iter: 594/594\n",
      "--\n",
      "Training Step: 281  | total loss: \u001b[1m\u001b[32m0.57089\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 281 | loss: 0.57089 - acc: 0.7329 -- iter: 594/594\n",
      "--\n",
      "Training Step: 282  | total loss: \u001b[1m\u001b[32m0.58881\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 282 | loss: 0.58881 - acc: 0.7136 -- iter: 594/594\n",
      "--\n",
      "Training Step: 283  | total loss: \u001b[1m\u001b[32m0.57770\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 283 | loss: 0.57770 - acc: 0.7256 -- iter: 594/594\n",
      "--\n",
      "Training Step: 284  | total loss: \u001b[1m\u001b[32m0.59352\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 284 | loss: 0.59352 - acc: 0.7088 -- iter: 594/594\n",
      "--\n",
      "Training Step: 285  | total loss: \u001b[1m\u001b[32m0.58194\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 285 | loss: 0.58194 - acc: 0.7217 -- iter: 594/594\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 286  | total loss: \u001b[1m\u001b[32m0.57192\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 286 | loss: 0.57192 - acc: 0.7327 -- iter: 594/594\n",
      "--\n",
      "Training Step: 287  | total loss: \u001b[1m\u001b[32m0.56249\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 287 | loss: 0.56249 - acc: 0.7423 -- iter: 594/594\n",
      "--\n",
      "Training Step: 288  | total loss: \u001b[1m\u001b[32m0.55419\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 288 | loss: 0.55419 - acc: 0.7504 -- iter: 594/594\n",
      "--\n",
      "Training Step: 289  | total loss: \u001b[1m\u001b[32m0.54551\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 289 | loss: 0.54551 - acc: 0.7583 -- iter: 594/594\n",
      "--\n",
      "Training Step: 290  | total loss: \u001b[1m\u001b[32m0.53776\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 290 | loss: 0.53776 - acc: 0.7658 -- iter: 594/594\n",
      "--\n",
      "Training Step: 291  | total loss: \u001b[1m\u001b[32m0.53033\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 291 | loss: 0.53033 - acc: 0.7733 -- iter: 594/594\n",
      "--\n",
      "Training Step: 292  | total loss: \u001b[1m\u001b[32m0.52293\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 292 | loss: 0.52293 - acc: 0.7801 -- iter: 594/594\n",
      "--\n",
      "Training Step: 293  | total loss: \u001b[1m\u001b[32m0.51564\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 293 | loss: 0.51564 - acc: 0.7859 -- iter: 594/594\n",
      "--\n",
      "Training Step: 294  | total loss: \u001b[1m\u001b[32m0.54657\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 294 | loss: 0.54657 - acc: 0.7599 -- iter: 594/594\n",
      "--\n",
      "Training Step: 295  | total loss: \u001b[1m\u001b[32m0.53581\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 295 | loss: 0.53581 - acc: 0.7679 -- iter: 594/594\n",
      "--\n",
      "Training Step: 296  | total loss: \u001b[1m\u001b[32m0.52530\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 296 | loss: 0.52530 - acc: 0.7751 -- iter: 594/594\n",
      "--\n",
      "Training Step: 297  | total loss: \u001b[1m\u001b[32m0.51550\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 297 | loss: 0.51550 - acc: 0.7811 -- iter: 594/594\n",
      "--\n",
      "Training Step: 298  | total loss: \u001b[1m\u001b[32m0.50629\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 298 | loss: 0.50629 - acc: 0.7873 -- iter: 594/594\n",
      "--\n",
      "Training Step: 299  | total loss: \u001b[1m\u001b[32m0.49745\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 299 | loss: 0.49745 - acc: 0.7926 -- iter: 594/594\n",
      "--\n",
      "Training Step: 300  | total loss: \u001b[1m\u001b[32m0.53808\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 300 | loss: 0.53808 - acc: 0.7652 -- iter: 594/594\n",
      "--\n",
      "Training Step: 301  | total loss: \u001b[1m\u001b[32m0.52536\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 301 | loss: 0.52536 - acc: 0.7734 -- iter: 594/594\n",
      "--\n",
      "Training Step: 302  | total loss: \u001b[1m\u001b[32m0.55874\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 302 | loss: 0.55874 - acc: 0.7531 -- iter: 594/594\n",
      "--\n",
      "Training Step: 303  | total loss: \u001b[1m\u001b[32m0.54418\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 303 | loss: 0.54418 - acc: 0.7620 -- iter: 594/594\n",
      "--\n",
      "Training Step: 304  | total loss: \u001b[1m\u001b[32m0.53181\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 304 | loss: 0.53181 - acc: 0.7699 -- iter: 594/594\n",
      "--\n",
      "Training Step: 305  | total loss: \u001b[1m\u001b[32m0.52026\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 305 | loss: 0.52026 - acc: 0.7778 -- iter: 594/594\n",
      "--\n",
      "Training Step: 306  | total loss: \u001b[1m\u001b[32m0.54988\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 306 | loss: 0.54988 - acc: 0.7569 -- iter: 594/594\n",
      "--\n",
      "Training Step: 307  | total loss: \u001b[1m\u001b[32m0.53654\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 307 | loss: 0.53654 - acc: 0.7656 -- iter: 594/594\n",
      "--\n",
      "Training Step: 308  | total loss: \u001b[1m\u001b[32m0.56843\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 308 | loss: 0.56843 - acc: 0.7446 -- iter: 594/594\n",
      "--\n",
      "Training Step: 309  | total loss: \u001b[1m\u001b[32m0.55413\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 309 | loss: 0.55413 - acc: 0.7541 -- iter: 594/594\n",
      "--\n",
      "Training Step: 310  | total loss: \u001b[1m\u001b[32m0.54113\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 310 | loss: 0.54113 - acc: 0.7629 -- iter: 594/594\n",
      "--\n",
      "Training Step: 311  | total loss: \u001b[1m\u001b[32m0.52923\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 311 | loss: 0.52923 - acc: 0.7718 -- iter: 594/594\n",
      "--\n",
      "Training Step: 312  | total loss: \u001b[1m\u001b[32m0.51825\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 312 | loss: 0.51825 - acc: 0.7794 -- iter: 594/594\n",
      "--\n",
      "Training Step: 313  | total loss: \u001b[1m\u001b[32m0.50918\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 313 | loss: 0.50918 - acc: 0.7860 -- iter: 594/594\n",
      "--\n",
      "Training Step: 314  | total loss: \u001b[1m\u001b[32m0.54302\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 314 | loss: 0.54302 - acc: 0.7613 -- iter: 594/594\n",
      "--\n",
      "Training Step: 315  | total loss: \u001b[1m\u001b[32m0.53115\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 315 | loss: 0.53115 - acc: 0.7698 -- iter: 594/594\n",
      "--\n",
      "Training Step: 316  | total loss: \u001b[1m\u001b[32m0.52105\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 316 | loss: 0.52105 - acc: 0.7779 -- iter: 594/594\n",
      "--\n",
      "Training Step: 317  | total loss: \u001b[1m\u001b[32m0.51195\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 317 | loss: 0.51195 - acc: 0.7843 -- iter: 594/594\n",
      "--\n",
      "Training Step: 318  | total loss: \u001b[1m\u001b[32m0.50380\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 318 | loss: 0.50380 - acc: 0.7907 -- iter: 594/594\n",
      "--\n",
      "Training Step: 319  | total loss: \u001b[1m\u001b[32m0.49559\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 319 | loss: 0.49559 - acc: 0.7966 -- iter: 594/594\n",
      "--\n",
      "Training Step: 320  | total loss: \u001b[1m\u001b[32m0.53172\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 320 | loss: 0.53172 - acc: 0.7715 -- iter: 594/594\n",
      "--\n",
      "Training Step: 321  | total loss: \u001b[1m\u001b[32m0.52091\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 321 | loss: 0.52091 - acc: 0.7794 -- iter: 594/594\n",
      "--\n",
      "Training Step: 322  | total loss: \u001b[1m\u001b[32m0.51071\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 322 | loss: 0.51071 - acc: 0.7865 -- iter: 594/594\n",
      "--\n",
      "Training Step: 323  | total loss: \u001b[1m\u001b[32m0.50199\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 323 | loss: 0.50199 - acc: 0.7922 -- iter: 594/594\n",
      "--\n",
      "Training Step: 324  | total loss: \u001b[1m\u001b[32m0.53933\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 324 | loss: 0.53933 - acc: 0.7645 -- iter: 594/594\n",
      "--\n",
      "Training Step: 325  | total loss: \u001b[1m\u001b[32m0.52804\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 325 | loss: 0.52804 - acc: 0.7722 -- iter: 594/594\n",
      "--\n",
      "Training Step: 326  | total loss: \u001b[1m\u001b[32m0.51762\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 326 | loss: 0.51762 - acc: 0.7785 -- iter: 594/594\n",
      "--\n",
      "Training Step: 327  | total loss: \u001b[1m\u001b[32m0.50811\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 327 | loss: 0.50811 - acc: 0.7863 -- iter: 594/594\n",
      "--\n",
      "Training Step: 328  | total loss: \u001b[1m\u001b[32m0.49968\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 328 | loss: 0.49968 - acc: 0.7929 -- iter: 594/594\n",
      "--\n",
      "Training Step: 329  | total loss: \u001b[1m\u001b[32m0.49205\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 329 | loss: 0.49205 - acc: 0.7978 -- iter: 594/594\n",
      "--\n",
      "Training Step: 330  | total loss: \u001b[1m\u001b[32m0.52970\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 330 | loss: 0.52970 - acc: 0.7697 -- iter: 594/594\n",
      "--\n",
      "Training Step: 331  | total loss: \u001b[1m\u001b[32m0.51912\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 331 | loss: 0.51912 - acc: 0.7774 -- iter: 594/594\n",
      "--\n",
      "Training Step: 332  | total loss: \u001b[1m\u001b[32m0.55083\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 332 | loss: 0.55083 - acc: 0.7560 -- iter: 594/594\n",
      "--\n",
      "Training Step: 333  | total loss: \u001b[1m\u001b[32m0.53797\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 333 | loss: 0.53797 - acc: 0.7649 -- iter: 594/594\n",
      "--\n",
      "Training Step: 334  | total loss: \u001b[1m\u001b[32m0.52655\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 334 | loss: 0.52655 - acc: 0.7725 -- iter: 594/594\n",
      "--\n",
      "Training Step: 335  | total loss: \u001b[1m\u001b[32m0.51692\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 335 | loss: 0.51692 - acc: 0.7796 -- iter: 594/594\n",
      "--\n",
      "Training Step: 336  | total loss: \u001b[1m\u001b[32m0.50787\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 336 | loss: 0.50787 - acc: 0.7864 -- iter: 594/594\n",
      "--\n",
      "Training Step: 337  | total loss: \u001b[1m\u001b[32m0.49976\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 337 | loss: 0.49976 - acc: 0.7920 -- iter: 594/594\n",
      "--\n",
      "Training Step: 338  | total loss: \u001b[1m\u001b[32m0.53475\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 338 | loss: 0.53475 - acc: 0.7672 -- iter: 594/594\n",
      "--\n",
      "Training Step: 339  | total loss: \u001b[1m\u001b[32m0.52352\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 339 | loss: 0.52352 - acc: 0.7755 -- iter: 594/594\n",
      "--\n",
      "Training Step: 340  | total loss: \u001b[1m\u001b[32m0.51372\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 340 | loss: 0.51372 - acc: 0.7828 -- iter: 594/594\n",
      "--\n",
      "Training Step: 341  | total loss: \u001b[1m\u001b[32m0.50448\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 341 | loss: 0.50448 - acc: 0.7897 -- iter: 594/594\n",
      "--\n",
      "Training Step: 342  | total loss: \u001b[1m\u001b[32m0.53827\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 342 | loss: 0.53827 - acc: 0.7656 -- iter: 594/594\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 343  | total loss: \u001b[1m\u001b[32m0.52660\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 343 | loss: 0.52660 - acc: 0.7737 -- iter: 594/594\n",
      "--\n",
      "Training Step: 344  | total loss: \u001b[1m\u001b[32m0.51642\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 344 | loss: 0.51642 - acc: 0.7810 -- iter: 594/594\n",
      "--\n",
      "Training Step: 345  | total loss: \u001b[1m\u001b[32m0.50727\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 345 | loss: 0.50727 - acc: 0.7878 -- iter: 594/594\n",
      "--\n",
      "Training Step: 346  | total loss: \u001b[1m\u001b[32m0.49940\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 346 | loss: 0.49940 - acc: 0.7935 -- iter: 594/594\n",
      "--\n",
      "Training Step: 347  | total loss: \u001b[1m\u001b[32m0.49182\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 347 | loss: 0.49182 - acc: 0.7988 -- iter: 594/594\n",
      "--\n",
      "Training Step: 348  | total loss: \u001b[1m\u001b[32m0.53035\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 348 | loss: 0.53035 - acc: 0.7728 -- iter: 594/594\n",
      "--\n",
      "Training Step: 349  | total loss: \u001b[1m\u001b[32m0.51925\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 349 | loss: 0.51925 - acc: 0.7800 -- iter: 594/594\n",
      "--\n",
      "Training Step: 350  | total loss: \u001b[1m\u001b[32m0.55771\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 350 | loss: 0.55771 - acc: 0.7517 -- iter: 594/594\n",
      "--\n",
      "Training Step: 351  | total loss: \u001b[1m\u001b[32m0.54429\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 351 | loss: 0.54429 - acc: 0.7609 -- iter: 594/594\n",
      "--\n",
      "Training Step: 352  | total loss: \u001b[1m\u001b[32m0.53200\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 352 | loss: 0.53200 - acc: 0.7693 -- iter: 594/594\n",
      "--\n",
      "Training Step: 353  | total loss: \u001b[1m\u001b[32m0.52166\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 353 | loss: 0.52166 - acc: 0.7769 -- iter: 594/594\n",
      "--\n",
      "Training Step: 354  | total loss: \u001b[1m\u001b[32m0.55123\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 354 | loss: 0.55123 - acc: 0.7549 -- iter: 594/594\n",
      "--\n",
      "Training Step: 355  | total loss: \u001b[1m\u001b[32m0.53922\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 355 | loss: 0.53922 - acc: 0.7639 -- iter: 594/594\n",
      "--\n",
      "Training Step: 356  | total loss: \u001b[1m\u001b[32m0.56728\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 356 | loss: 0.56728 - acc: 0.7411 -- iter: 594/594\n",
      "--\n",
      "Training Step: 357  | total loss: \u001b[1m\u001b[32m0.55429\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 357 | loss: 0.55429 - acc: 0.7518 -- iter: 594/594\n",
      "--\n",
      "Training Step: 358  | total loss: \u001b[1m\u001b[32m0.54292\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 358 | loss: 0.54292 - acc: 0.7613 -- iter: 594/594\n",
      "--\n",
      "Training Step: 359  | total loss: \u001b[1m\u001b[32m0.53259\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 359 | loss: 0.53259 - acc: 0.7702 -- iter: 594/594\n",
      "--\n",
      "Training Step: 360  | total loss: \u001b[1m\u001b[32m0.55972\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 360 | loss: 0.55972 - acc: 0.7472 -- iter: 594/594\n",
      "--\n",
      "Training Step: 361  | total loss: \u001b[1m\u001b[32m0.54804\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 361 | loss: 0.54804 - acc: 0.7568 -- iter: 594/594\n",
      "--\n",
      "Training Step: 362  | total loss: \u001b[1m\u001b[32m0.56907\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 362 | loss: 0.56907 - acc: 0.7372 -- iter: 594/594\n",
      "--\n",
      "Training Step: 363  | total loss: \u001b[1m\u001b[32m0.55688\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 363 | loss: 0.55688 - acc: 0.7475 -- iter: 594/594\n",
      "--\n",
      "Training Step: 364  | total loss: \u001b[1m\u001b[32m0.57820\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 364 | loss: 0.57820 - acc: 0.7280 -- iter: 594/594\n",
      "--\n",
      "Training Step: 365  | total loss: \u001b[1m\u001b[32m0.56604\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 365 | loss: 0.56604 - acc: 0.7390 -- iter: 594/594\n",
      "--\n",
      "Training Step: 366  | total loss: \u001b[1m\u001b[32m0.55479\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 366 | loss: 0.55479 - acc: 0.7495 -- iter: 594/594\n",
      "--\n",
      "Training Step: 367  | total loss: \u001b[1m\u001b[32m0.54479\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 367 | loss: 0.54479 - acc: 0.7580 -- iter: 594/594\n",
      "--\n",
      "Training Step: 368  | total loss: \u001b[1m\u001b[32m0.53613\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 368 | loss: 0.53613 - acc: 0.7652 -- iter: 594/594\n",
      "--\n",
      "Training Step: 369  | total loss: \u001b[1m\u001b[32m0.52779\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 369 | loss: 0.52779 - acc: 0.7734 -- iter: 594/594\n",
      "--\n",
      "Training Step: 370  | total loss: \u001b[1m\u001b[32m0.52015\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 370 | loss: 0.52015 - acc: 0.7807 -- iter: 594/594\n",
      "--\n",
      "Training Step: 371  | total loss: \u001b[1m\u001b[32m0.51305\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 371 | loss: 0.51305 - acc: 0.7880 -- iter: 594/594\n",
      "--\n",
      "Training Step: 372  | total loss: \u001b[1m\u001b[32m0.50584\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 372 | loss: 0.50584 - acc: 0.7939 -- iter: 594/594\n",
      "--\n",
      "Training Step: 373  | total loss: \u001b[1m\u001b[32m0.49891\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 373 | loss: 0.49891 - acc: 0.7990 -- iter: 594/594\n",
      "--\n",
      "Training Step: 374  | total loss: \u001b[1m\u001b[32m0.49262\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 374 | loss: 0.49262 - acc: 0.8034 -- iter: 594/594\n",
      "--\n",
      "Training Step: 375  | total loss: \u001b[1m\u001b[32m0.48634\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 375 | loss: 0.48634 - acc: 0.8073 -- iter: 594/594\n",
      "--\n",
      "Training Step: 376  | total loss: \u001b[1m\u001b[32m0.48000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 376 | loss: 0.48000 - acc: 0.8109 -- iter: 594/594\n",
      "--\n",
      "Training Step: 377  | total loss: \u001b[1m\u001b[32m0.47396\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 377 | loss: 0.47396 - acc: 0.8146 -- iter: 594/594\n",
      "--\n",
      "Training Step: 378  | total loss: \u001b[1m\u001b[32m0.46807\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 378 | loss: 0.46807 - acc: 0.8172 -- iter: 594/594\n",
      "--\n",
      "Training Step: 379  | total loss: \u001b[1m\u001b[32m0.46226\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 379 | loss: 0.46226 - acc: 0.8200 -- iter: 594/594\n",
      "--\n",
      "Training Step: 380  | total loss: \u001b[1m\u001b[32m0.45634\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 380 | loss: 0.45634 - acc: 0.8227 -- iter: 594/594\n",
      "--\n",
      "Training Step: 381  | total loss: \u001b[1m\u001b[32m0.45086\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 381 | loss: 0.45086 - acc: 0.8251 -- iter: 594/594\n",
      "--\n",
      "Training Step: 382  | total loss: \u001b[1m\u001b[32m0.44632\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 382 | loss: 0.44632 - acc: 0.8269 -- iter: 594/594\n",
      "--\n",
      "Training Step: 383  | total loss: \u001b[1m\u001b[32m0.44157\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 383 | loss: 0.44157 - acc: 0.8284 -- iter: 594/594\n",
      "--\n",
      "Training Step: 384  | total loss: \u001b[1m\u001b[32m0.43746\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 384 | loss: 0.43746 - acc: 0.8306 -- iter: 594/594\n",
      "--\n",
      "Training Step: 385  | total loss: \u001b[1m\u001b[32m0.43312\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 385 | loss: 0.43312 - acc: 0.8320 -- iter: 594/594\n",
      "--\n",
      "Training Step: 386  | total loss: \u001b[1m\u001b[32m0.49515\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 386 | loss: 0.49515 - acc: 0.8029 -- iter: 594/594\n",
      "--\n",
      "Training Step: 387  | total loss: \u001b[1m\u001b[32m0.48533\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 387 | loss: 0.48533 - acc: 0.8076 -- iter: 594/594\n",
      "--\n",
      "Training Step: 388  | total loss: \u001b[1m\u001b[32m0.47657\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 388 | loss: 0.47657 - acc: 0.8117 -- iter: 594/594\n",
      "--\n",
      "Training Step: 389  | total loss: \u001b[1m\u001b[32m0.46808\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 389 | loss: 0.46808 - acc: 0.8150 -- iter: 594/594\n",
      "--\n",
      "Training Step: 390  | total loss: \u001b[1m\u001b[32m0.46115\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 390 | loss: 0.46115 - acc: 0.8177 -- iter: 594/594\n",
      "--\n",
      "Training Step: 391  | total loss: \u001b[1m\u001b[32m0.45466\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 391 | loss: 0.45466 - acc: 0.8204 -- iter: 594/594\n",
      "--\n",
      "Training Step: 392  | total loss: \u001b[1m\u001b[32m0.50911\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 392 | loss: 0.50911 - acc: 0.7931 -- iter: 594/594\n",
      "--\n",
      "Training Step: 393  | total loss: \u001b[1m\u001b[32m0.49775\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 393 | loss: 0.49775 - acc: 0.7988 -- iter: 594/594\n",
      "--\n",
      "Training Step: 394  | total loss: \u001b[1m\u001b[32m0.54707\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 394 | loss: 0.54707 - acc: 0.7718 -- iter: 594/594\n",
      "--\n",
      "Training Step: 395  | total loss: \u001b[1m\u001b[32m0.53306\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 395 | loss: 0.53306 - acc: 0.7790 -- iter: 594/594\n",
      "--\n",
      "Training Step: 396  | total loss: \u001b[1m\u001b[32m0.56816\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 396 | loss: 0.56816 - acc: 0.7563 -- iter: 594/594\n",
      "--\n",
      "Training Step: 397  | total loss: \u001b[1m\u001b[32m0.55243\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 397 | loss: 0.55243 - acc: 0.7655 -- iter: 594/594\n",
      "--\n",
      "Training Step: 398  | total loss: \u001b[1m\u001b[32m0.58204\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 398 | loss: 0.58204 - acc: 0.7420 -- iter: 594/594\n",
      "--\n",
      "Training Step: 399  | total loss: \u001b[1m\u001b[32m0.56726\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 399 | loss: 0.56726 - acc: 0.7526 -- iter: 594/594\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 400  | total loss: \u001b[1m\u001b[32m0.55466\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 400 | loss: 0.55466 - acc: 0.7621 -- iter: 594/594\n",
      "--\n",
      "Training Step: 401  | total loss: \u001b[1m\u001b[32m0.54400\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 401 | loss: 0.54400 - acc: 0.7695 -- iter: 594/594\n",
      "--\n",
      "Training Step: 402  | total loss: \u001b[1m\u001b[32m0.56833\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 402 | loss: 0.56833 - acc: 0.7446 -- iter: 594/594\n",
      "--\n",
      "Training Step: 403  | total loss: \u001b[1m\u001b[32m0.55765\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 403 | loss: 0.55765 - acc: 0.7540 -- iter: 594/594\n",
      "--\n",
      "Training Step: 404  | total loss: \u001b[1m\u001b[32m0.54869\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 404 | loss: 0.54869 - acc: 0.7622 -- iter: 594/594\n",
      "--\n",
      "Training Step: 405  | total loss: \u001b[1m\u001b[32m0.54052\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 405 | loss: 0.54052 - acc: 0.7702 -- iter: 594/594\n",
      "--\n",
      "Training Step: 406  | total loss: \u001b[1m\u001b[32m0.56187\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 406 | loss: 0.56187 - acc: 0.7469 -- iter: 594/594\n",
      "--\n",
      "Training Step: 407  | total loss: \u001b[1m\u001b[32m0.55324\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 407 | loss: 0.55324 - acc: 0.7552 -- iter: 594/594\n",
      "--\n",
      "Training Step: 408  | total loss: \u001b[1m\u001b[32m0.56974\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 408 | loss: 0.56974 - acc: 0.7352 -- iter: 594/594\n",
      "--\n",
      "Training Step: 409  | total loss: \u001b[1m\u001b[32m0.56057\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 409 | loss: 0.56057 - acc: 0.7445 -- iter: 594/594\n",
      "--\n",
      "Training Step: 410  | total loss: \u001b[1m\u001b[32m0.55262\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 410 | loss: 0.55262 - acc: 0.7532 -- iter: 594/594\n",
      "--\n",
      "Training Step: 411  | total loss: \u001b[1m\u001b[32m0.54583\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 411 | loss: 0.54583 - acc: 0.7606 -- iter: 594/594\n",
      "--\n",
      "Training Step: 412  | total loss: \u001b[1m\u001b[32m0.53926\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 412 | loss: 0.53926 - acc: 0.7687 -- iter: 594/594\n",
      "--\n",
      "Training Step: 413  | total loss: \u001b[1m\u001b[32m0.53292\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 413 | loss: 0.53292 - acc: 0.7763 -- iter: 594/594\n",
      "--\n",
      "Training Step: 414  | total loss: \u001b[1m\u001b[32m0.55455\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 414 | loss: 0.55455 - acc: 0.7521 -- iter: 594/594\n",
      "--\n",
      "Training Step: 415  | total loss: \u001b[1m\u001b[32m0.54627\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 415 | loss: 0.54627 - acc: 0.7595 -- iter: 594/594\n",
      "--\n",
      "Training Step: 416  | total loss: \u001b[1m\u001b[32m0.56670\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 416 | loss: 0.56670 - acc: 0.7390 -- iter: 594/594\n",
      "--\n",
      "Training Step: 417  | total loss: \u001b[1m\u001b[32m0.55662\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 417 | loss: 0.55662 - acc: 0.7484 -- iter: 594/594\n",
      "--\n",
      "Training Step: 418  | total loss: \u001b[1m\u001b[32m0.57725\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 418 | loss: 0.57725 - acc: 0.7269 -- iter: 594/594\n",
      "--\n",
      "Training Step: 419  | total loss: \u001b[1m\u001b[32m0.56593\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 419 | loss: 0.56593 - acc: 0.7374 -- iter: 594/594\n",
      "--\n",
      "Training Step: 420  | total loss: \u001b[1m\u001b[32m0.58387\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 420 | loss: 0.58387 - acc: 0.7174 -- iter: 594/594\n",
      "--\n",
      "Training Step: 421  | total loss: \u001b[1m\u001b[32m0.57194\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 421 | loss: 0.57194 - acc: 0.7291 -- iter: 594/594\n",
      "--\n",
      "Training Step: 422  | total loss: \u001b[1m\u001b[32m0.56120\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 422 | loss: 0.56120 - acc: 0.7395 -- iter: 594/594\n",
      "--\n",
      "Training Step: 423  | total loss: \u001b[1m\u001b[32m0.55131\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 423 | loss: 0.55131 - acc: 0.7494 -- iter: 594/594\n",
      "--\n",
      "Training Step: 424  | total loss: \u001b[1m\u001b[32m0.54271\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 424 | loss: 0.54271 - acc: 0.7578 -- iter: 594/594\n",
      "--\n",
      "Training Step: 425  | total loss: \u001b[1m\u001b[32m0.53386\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 425 | loss: 0.53386 - acc: 0.7657 -- iter: 594/594\n",
      "--\n",
      "Training Step: 426  | total loss: \u001b[1m\u001b[32m0.52554\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 426 | loss: 0.52554 - acc: 0.7728 -- iter: 594/594\n",
      "--\n",
      "Training Step: 427  | total loss: \u001b[1m\u001b[32m0.51735\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 427 | loss: 0.51735 - acc: 0.7790 -- iter: 594/594\n",
      "--\n",
      "Training Step: 428  | total loss: \u001b[1m\u001b[32m0.54655\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 428 | loss: 0.54655 - acc: 0.7548 -- iter: 594/594\n",
      "--\n",
      "Training Step: 429  | total loss: \u001b[1m\u001b[32m0.53560\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 429 | loss: 0.53560 - acc: 0.7634 -- iter: 594/594\n",
      "--\n",
      "Training Step: 430  | total loss: \u001b[1m\u001b[32m0.56667\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 430 | loss: 0.56667 - acc: 0.7385 -- iter: 594/594\n",
      "--\n",
      "Training Step: 431  | total loss: \u001b[1m\u001b[32m0.55345\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 431 | loss: 0.55345 - acc: 0.7492 -- iter: 594/594\n",
      "--\n",
      "Training Step: 432  | total loss: \u001b[1m\u001b[32m0.54163\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 432 | loss: 0.54163 - acc: 0.7588 -- iter: 594/594\n",
      "--\n",
      "Training Step: 433  | total loss: \u001b[1m\u001b[32m0.53022\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 433 | loss: 0.53022 - acc: 0.7672 -- iter: 594/594\n",
      "--\n",
      "Training Step: 434  | total loss: \u001b[1m\u001b[32m0.56090\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 434 | loss: 0.56090 - acc: 0.7425 -- iter: 594/594\n",
      "--\n",
      "Training Step: 435  | total loss: \u001b[1m\u001b[32m0.54747\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 435 | loss: 0.54747 - acc: 0.7530 -- iter: 594/594\n",
      "--\n",
      "Training Step: 436  | total loss: \u001b[1m\u001b[32m0.57679\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 436 | loss: 0.57679 - acc: 0.7307 -- iter: 594/594\n",
      "--\n",
      "Training Step: 437  | total loss: \u001b[1m\u001b[32m0.56257\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 437 | loss: 0.56257 - acc: 0.7416 -- iter: 594/594\n",
      "--\n",
      "Training Step: 438  | total loss: \u001b[1m\u001b[32m0.58791\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 438 | loss: 0.58791 - acc: 0.7237 -- iter: 594/594\n",
      "--\n",
      "Training Step: 439  | total loss: \u001b[1m\u001b[32m0.57258\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 439 | loss: 0.57258 - acc: 0.7364 -- iter: 594/594\n",
      "--\n",
      "Training Step: 440  | total loss: \u001b[1m\u001b[32m0.59300\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 440 | loss: 0.59300 - acc: 0.7191 -- iter: 594/594\n",
      "--\n",
      "Training Step: 441  | total loss: \u001b[1m\u001b[32m0.57886\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 441 | loss: 0.57886 - acc: 0.7312 -- iter: 594/594\n",
      "--\n",
      "Training Step: 442  | total loss: \u001b[1m\u001b[32m0.56549\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 442 | loss: 0.56549 - acc: 0.7424 -- iter: 594/594\n",
      "--\n",
      "Training Step: 443  | total loss: \u001b[1m\u001b[32m0.55343\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 443 | loss: 0.55343 - acc: 0.7524 -- iter: 594/594\n",
      "--\n",
      "Training Step: 444  | total loss: \u001b[1m\u001b[32m0.54257\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 444 | loss: 0.54257 - acc: 0.7611 -- iter: 594/594\n",
      "--\n",
      "Training Step: 445  | total loss: \u001b[1m\u001b[32m0.53271\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 445 | loss: 0.53271 - acc: 0.7690 -- iter: 594/594\n",
      "--\n",
      "Training Step: 446  | total loss: \u001b[1m\u001b[32m0.55770\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 446 | loss: 0.55770 - acc: 0.7477 -- iter: 594/594\n",
      "--\n",
      "Training Step: 447  | total loss: \u001b[1m\u001b[32m0.54646\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 447 | loss: 0.54646 - acc: 0.7571 -- iter: 594/594\n",
      "--\n",
      "Training Step: 448  | total loss: \u001b[1m\u001b[32m0.53624\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 448 | loss: 0.53624 - acc: 0.7654 -- iter: 594/594\n",
      "--\n",
      "Training Step: 449  | total loss: \u001b[1m\u001b[32m0.52695\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 449 | loss: 0.52695 - acc: 0.7727 -- iter: 594/594\n",
      "--\n",
      "Training Step: 450  | total loss: \u001b[1m\u001b[32m0.51840\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 450 | loss: 0.51840 - acc: 0.7799 -- iter: 594/594\n",
      "--\n",
      "Training Step: 451  | total loss: \u001b[1m\u001b[32m0.51025\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 451 | loss: 0.51025 - acc: 0.7858 -- iter: 594/594\n",
      "--\n",
      "Training Step: 452  | total loss: \u001b[1m\u001b[32m0.50221\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 452 | loss: 0.50221 - acc: 0.7920 -- iter: 594/594\n",
      "--\n",
      "Training Step: 453  | total loss: \u001b[1m\u001b[32m0.49520\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 453 | loss: 0.49520 - acc: 0.7968 -- iter: 594/594\n",
      "--\n",
      "Training Step: 454  | total loss: \u001b[1m\u001b[32m0.52651\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 454 | loss: 0.52651 - acc: 0.7725 -- iter: 594/594\n",
      "--\n",
      "Training Step: 455  | total loss: \u001b[1m\u001b[32m0.51605\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 455 | loss: 0.51605 - acc: 0.7796 -- iter: 594/594\n",
      "--\n",
      "Training Step: 456  | total loss: \u001b[1m\u001b[32m0.55078\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 456 | loss: 0.55078 - acc: 0.7555 -- iter: 594/594\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 457  | total loss: \u001b[1m\u001b[32m0.53829\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 457 | loss: 0.53829 - acc: 0.7643 -- iter: 594/594\n",
      "--\n",
      "Training Step: 458  | total loss: \u001b[1m\u001b[32m0.57129\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 458 | loss: 0.57129 - acc: 0.7413 -- iter: 594/594\n",
      "--\n",
      "Training Step: 459  | total loss: \u001b[1m\u001b[32m0.55665\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 459 | loss: 0.55665 - acc: 0.7518 -- iter: 594/594\n",
      "--\n",
      "Training Step: 460  | total loss: \u001b[1m\u001b[32m0.54340\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 460 | loss: 0.54340 - acc: 0.7613 -- iter: 594/594\n",
      "--\n",
      "Training Step: 461  | total loss: \u001b[1m\u001b[32m0.53182\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 461 | loss: 0.53182 - acc: 0.7695 -- iter: 594/594\n",
      "--\n",
      "Training Step: 462  | total loss: \u001b[1m\u001b[32m0.55961\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 462 | loss: 0.55961 - acc: 0.7473 -- iter: 594/594\n",
      "--\n",
      "Training Step: 463  | total loss: \u001b[1m\u001b[32m0.54677\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 463 | loss: 0.54677 - acc: 0.7569 -- iter: 594/594\n",
      "--\n",
      "Training Step: 464  | total loss: \u001b[1m\u001b[32m0.57249\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 464 | loss: 0.57249 - acc: 0.7361 -- iter: 594/594\n",
      "--\n",
      "Training Step: 465  | total loss: \u001b[1m\u001b[32m0.55857\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 465 | loss: 0.55857 - acc: 0.7473 -- iter: 594/594\n",
      "--\n",
      "Training Step: 466  | total loss: \u001b[1m\u001b[32m0.58522\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 466 | loss: 0.58522 - acc: 0.7248 -- iter: 594/594\n",
      "--\n",
      "Training Step: 467  | total loss: \u001b[1m\u001b[32m0.57069\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 467 | loss: 0.57069 - acc: 0.7372 -- iter: 594/594\n",
      "--\n",
      "Training Step: 468  | total loss: \u001b[1m\u001b[32m0.59263\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 468 | loss: 0.59263 - acc: 0.7170 -- iter: 594/594\n",
      "--\n",
      "Training Step: 469  | total loss: \u001b[1m\u001b[32m0.57816\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 469 | loss: 0.57816 - acc: 0.7298 -- iter: 594/594\n",
      "--\n",
      "Training Step: 470  | total loss: \u001b[1m\u001b[32m0.56579\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 470 | loss: 0.56579 - acc: 0.7413 -- iter: 594/594\n",
      "--\n",
      "Training Step: 471  | total loss: \u001b[1m\u001b[32m0.55498\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 471 | loss: 0.55498 - acc: 0.7512 -- iter: 594/594\n",
      "--\n",
      "Training Step: 472  | total loss: \u001b[1m\u001b[32m0.57672\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 472 | loss: 0.57672 - acc: 0.7293 -- iter: 594/594\n",
      "--\n",
      "Training Step: 473  | total loss: \u001b[1m\u001b[32m0.56523\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 473 | loss: 0.56523 - acc: 0.7404 -- iter: 594/594\n",
      "--\n",
      "Training Step: 474  | total loss: \u001b[1m\u001b[32m0.58406\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 474 | loss: 0.58406 - acc: 0.7219 -- iter: 594/594\n",
      "--\n",
      "Training Step: 475  | total loss: \u001b[1m\u001b[32m0.57207\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 475 | loss: 0.57207 - acc: 0.7330 -- iter: 594/594\n",
      "--\n",
      "Training Step: 476  | total loss: \u001b[1m\u001b[32m0.59100\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 476 | loss: 0.59100 - acc: 0.7129 -- iter: 594/594\n",
      "--\n",
      "Training Step: 477  | total loss: \u001b[1m\u001b[32m0.57862\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 477 | loss: 0.57862 - acc: 0.7255 -- iter: 594/594\n",
      "--\n",
      "Training Step: 478  | total loss: \u001b[1m\u001b[32m0.59510\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 478 | loss: 0.59510 - acc: 0.7076 -- iter: 594/594\n",
      "--\n",
      "Training Step: 479  | total loss: \u001b[1m\u001b[32m0.58294\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 479 | loss: 0.58294 - acc: 0.7210 -- iter: 594/594\n",
      "--\n",
      "Training Step: 480  | total loss: \u001b[1m\u001b[32m0.59868\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 480 | loss: 0.59868 - acc: 0.7031 -- iter: 594/594\n",
      "--\n",
      "Training Step: 481  | total loss: \u001b[1m\u001b[32m0.58646\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 481 | loss: 0.58646 - acc: 0.7172 -- iter: 594/594\n",
      "--\n",
      "Training Step: 482  | total loss: \u001b[1m\u001b[32m0.60080\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 482 | loss: 0.60080 - acc: 0.7003 -- iter: 594/594\n",
      "--\n",
      "Training Step: 483  | total loss: \u001b[1m\u001b[32m0.58858\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 483 | loss: 0.58858 - acc: 0.7128 -- iter: 594/594\n",
      "--\n",
      "Training Step: 484  | total loss: \u001b[1m\u001b[32m0.57834\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 484 | loss: 0.57834 - acc: 0.7242 -- iter: 594/594\n",
      "--\n",
      "Training Step: 485  | total loss: \u001b[1m\u001b[32m0.56893\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 485 | loss: 0.56893 - acc: 0.7343 -- iter: 594/594\n",
      "--\n",
      "Training Step: 486  | total loss: \u001b[1m\u001b[32m0.58317\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 486 | loss: 0.58317 - acc: 0.7171 -- iter: 594/594\n",
      "--\n",
      "Training Step: 487  | total loss: \u001b[1m\u001b[32m0.57306\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 487 | loss: 0.57306 - acc: 0.7272 -- iter: 594/594\n",
      "--\n",
      "Training Step: 488  | total loss: \u001b[1m\u001b[32m0.59116\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 488 | loss: 0.59116 - acc: 0.7082 -- iter: 594/594\n",
      "--\n",
      "Training Step: 489  | total loss: \u001b[1m\u001b[32m0.57989\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 489 | loss: 0.57989 - acc: 0.7207 -- iter: 594/594\n",
      "--\n",
      "Training Step: 490  | total loss: \u001b[1m\u001b[32m0.56974\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 490 | loss: 0.56974 - acc: 0.7314 -- iter: 594/594\n",
      "--\n",
      "Training Step: 491  | total loss: \u001b[1m\u001b[32m0.56059\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 491 | loss: 0.56059 - acc: 0.7415 -- iter: 594/594\n",
      "--\n",
      "Training Step: 492  | total loss: \u001b[1m\u001b[32m0.55172\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 492 | loss: 0.55172 - acc: 0.7515 -- iter: 594/594\n",
      "--\n",
      "Training Step: 493  | total loss: \u001b[1m\u001b[32m0.54336\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 493 | loss: 0.54336 - acc: 0.7597 -- iter: 594/594\n",
      "--\n",
      "Training Step: 494  | total loss: \u001b[1m\u001b[32m0.56730\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 494 | loss: 0.56730 - acc: 0.7361 -- iter: 594/594\n",
      "--\n",
      "Training Step: 495  | total loss: \u001b[1m\u001b[32m0.55629\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 495 | loss: 0.55629 - acc: 0.7455 -- iter: 594/594\n",
      "--\n",
      "Training Step: 496  | total loss: \u001b[1m\u001b[32m0.54635\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 496 | loss: 0.54635 - acc: 0.7547 -- iter: 594/594\n",
      "--\n",
      "Training Step: 497  | total loss: \u001b[1m\u001b[32m0.53725\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 497 | loss: 0.53725 - acc: 0.7624 -- iter: 594/594\n",
      "--\n",
      "Training Step: 498  | total loss: \u001b[1m\u001b[32m0.56122\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 498 | loss: 0.56122 - acc: 0.7409 -- iter: 594/594\n",
      "--\n",
      "Training Step: 499  | total loss: \u001b[1m\u001b[32m0.54923\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 499 | loss: 0.54923 - acc: 0.7513 -- iter: 594/594\n",
      "--\n",
      "Training Step: 500  | total loss: \u001b[1m\u001b[32m0.53825\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 500 | loss: 0.53825 - acc: 0.7599 -- iter: 594/594\n",
      "--\n",
      "Training Step: 501  | total loss: \u001b[1m\u001b[32m0.52859\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 501 | loss: 0.52859 - acc: 0.7687 -- iter: 594/594\n",
      "--\n",
      "Training Step: 502  | total loss: \u001b[1m\u001b[32m0.51815\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 502 | loss: 0.51815 - acc: 0.7764 -- iter: 594/594\n",
      "--\n",
      "Training Step: 503  | total loss: \u001b[1m\u001b[32m0.50908\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 503 | loss: 0.50908 - acc: 0.7831 -- iter: 594/594\n",
      "--\n",
      "Training Step: 504  | total loss: \u001b[1m\u001b[32m0.50054\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 504 | loss: 0.50054 - acc: 0.7893 -- iter: 594/594\n",
      "--\n",
      "Training Step: 505  | total loss: \u001b[1m\u001b[32m0.49254\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 505 | loss: 0.49254 - acc: 0.7947 -- iter: 594/594\n",
      "--\n",
      "Training Step: 506  | total loss: \u001b[1m\u001b[32m0.48531\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 506 | loss: 0.48531 - acc: 0.7997 -- iter: 594/594\n",
      "--\n",
      "Training Step: 507  | total loss: \u001b[1m\u001b[32m0.47746\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 507 | loss: 0.47746 - acc: 0.8046 -- iter: 594/594\n",
      "--\n",
      "Training Step: 508  | total loss: \u001b[1m\u001b[32m0.52055\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 508 | loss: 0.52055 - acc: 0.7800 -- iter: 594/594\n",
      "--\n",
      "Training Step: 509  | total loss: \u001b[1m\u001b[32m0.50977\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 509 | loss: 0.50977 - acc: 0.7867 -- iter: 594/594\n",
      "--\n",
      "Training Step: 510  | total loss: \u001b[1m\u001b[32m0.54750\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 510 | loss: 0.54750 - acc: 0.7641 -- iter: 594/594\n",
      "--\n",
      "Training Step: 511  | total loss: \u001b[1m\u001b[32m0.53305\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 511 | loss: 0.53305 - acc: 0.7719 -- iter: 594/594\n",
      "--\n",
      "Training Step: 512  | total loss: \u001b[1m\u001b[32m0.57393\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 512 | loss: 0.57393 - acc: 0.7479 -- iter: 594/594\n",
      "--\n",
      "Training Step: 513  | total loss: \u001b[1m\u001b[32m0.55725\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 513 | loss: 0.55725 - acc: 0.7579 -- iter: 594/594\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 514  | total loss: \u001b[1m\u001b[32m0.58833\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 514 | loss: 0.58833 - acc: 0.7364 -- iter: 594/594\n",
      "--\n",
      "Training Step: 515  | total loss: \u001b[1m\u001b[32m0.57064\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 515 | loss: 0.57064 - acc: 0.7477 -- iter: 594/594\n",
      "--\n",
      "Training Step: 516  | total loss: \u001b[1m\u001b[32m0.59869\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 516 | loss: 0.59869 - acc: 0.7267 -- iter: 594/594\n",
      "--\n",
      "Training Step: 517  | total loss: \u001b[1m\u001b[32m0.58206\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 517 | loss: 0.58206 - acc: 0.7385 -- iter: 594/594\n",
      "--\n",
      "Training Step: 518  | total loss: \u001b[1m\u001b[32m0.56712\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 518 | loss: 0.56712 - acc: 0.7485 -- iter: 594/594\n",
      "--\n",
      "Training Step: 519  | total loss: \u001b[1m\u001b[32m0.55403\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 519 | loss: 0.55403 - acc: 0.7578 -- iter: 594/594\n",
      "--\n",
      "Training Step: 520  | total loss: \u001b[1m\u001b[32m0.57982\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 520 | loss: 0.57982 - acc: 0.7339 -- iter: 594/594\n",
      "--\n",
      "Training Step: 521  | total loss: \u001b[1m\u001b[32m0.56670\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 521 | loss: 0.56670 - acc: 0.7452 -- iter: 594/594\n",
      "--\n",
      "Training Step: 522  | total loss: \u001b[1m\u001b[32m0.55528\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 522 | loss: 0.55528 - acc: 0.7545 -- iter: 594/594\n",
      "--\n",
      "Training Step: 523  | total loss: \u001b[1m\u001b[32m0.54526\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 523 | loss: 0.54526 - acc: 0.7626 -- iter: 594/594\n",
      "--\n",
      "Training Step: 524  | total loss: \u001b[1m\u001b[32m0.53628\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 524 | loss: 0.53628 - acc: 0.7711 -- iter: 594/594\n",
      "--\n",
      "Training Step: 525  | total loss: \u001b[1m\u001b[32m0.52794\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 525 | loss: 0.52794 - acc: 0.7789 -- iter: 594/594\n",
      "--\n",
      "Training Step: 526  | total loss: \u001b[1m\u001b[32m0.55149\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 526 | loss: 0.55149 - acc: 0.7547 -- iter: 594/594\n",
      "--\n",
      "Training Step: 527  | total loss: \u001b[1m\u001b[32m0.54151\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 527 | loss: 0.54151 - acc: 0.7632 -- iter: 594/594\n",
      "--\n",
      "Training Step: 528  | total loss: \u001b[1m\u001b[32m0.56863\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 528 | loss: 0.56863 - acc: 0.7364 -- iter: 594/594\n",
      "--\n",
      "Training Step: 529  | total loss: \u001b[1m\u001b[32m0.55727\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 529 | loss: 0.55727 - acc: 0.7466 -- iter: 594/594\n",
      "--\n",
      "Training Step: 530  | total loss: \u001b[1m\u001b[32m0.57368\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 530 | loss: 0.57368 - acc: 0.7280 -- iter: 594/594\n",
      "--\n",
      "Training Step: 531  | total loss: \u001b[1m\u001b[32m0.56230\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 531 | loss: 0.56230 - acc: 0.7387 -- iter: 594/594\n",
      "--\n",
      "Training Step: 532  | total loss: \u001b[1m\u001b[32m0.55195\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 532 | loss: 0.55195 - acc: 0.7490 -- iter: 594/594\n",
      "--\n",
      "Training Step: 533  | total loss: \u001b[1m\u001b[32m0.54202\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 533 | loss: 0.54202 - acc: 0.7574 -- iter: 594/594\n",
      "--\n",
      "Training Step: 534  | total loss: \u001b[1m\u001b[32m0.56706\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 534 | loss: 0.56706 - acc: 0.7352 -- iter: 594/594\n",
      "--\n",
      "Training Step: 535  | total loss: \u001b[1m\u001b[32m0.55593\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 535 | loss: 0.55593 - acc: 0.7449 -- iter: 594/594\n",
      "--\n",
      "Training Step: 536  | total loss: \u001b[1m\u001b[32m0.54631\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 536 | loss: 0.54631 - acc: 0.7539 -- iter: 594/594\n",
      "--\n",
      "Training Step: 537  | total loss: \u001b[1m\u001b[32m0.53681\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 537 | loss: 0.53681 - acc: 0.7617 -- iter: 594/594\n",
      "--\n",
      "Training Step: 538  | total loss: \u001b[1m\u001b[32m0.52831\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 538 | loss: 0.52831 - acc: 0.7692 -- iter: 594/594\n",
      "--\n",
      "Training Step: 539  | total loss: \u001b[1m\u001b[32m0.52032\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 539 | loss: 0.52032 - acc: 0.7761 -- iter: 594/594\n",
      "--\n",
      "Training Step: 540  | total loss: \u001b[1m\u001b[32m0.51261\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 540 | loss: 0.51261 - acc: 0.7825 -- iter: 594/594\n",
      "--\n",
      "Training Step: 541  | total loss: \u001b[1m\u001b[32m0.50537\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 541 | loss: 0.50537 - acc: 0.7886 -- iter: 594/594\n",
      "--\n",
      "Training Step: 542  | total loss: \u001b[1m\u001b[32m0.53589\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 542 | loss: 0.53589 - acc: 0.7661 -- iter: 594/594\n",
      "--\n",
      "Training Step: 543  | total loss: \u001b[1m\u001b[32m0.52547\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 543 | loss: 0.52547 - acc: 0.7742 -- iter: 594/594\n",
      "--\n",
      "Training Step: 544  | total loss: \u001b[1m\u001b[32m0.51507\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 544 | loss: 0.51507 - acc: 0.7808 -- iter: 594/594\n",
      "--\n",
      "Training Step: 545  | total loss: \u001b[1m\u001b[32m0.50609\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 545 | loss: 0.50609 - acc: 0.7874 -- iter: 594/594\n",
      "--\n",
      "Training Step: 546  | total loss: \u001b[1m\u001b[32m0.54590\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 546 | loss: 0.54590 - acc: 0.7585 -- iter: 594/594\n",
      "--\n",
      "Training Step: 547  | total loss: \u001b[1m\u001b[32m0.53304\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 547 | loss: 0.53304 - acc: 0.7673 -- iter: 594/594\n",
      "--\n",
      "Training Step: 548  | total loss: \u001b[1m\u001b[32m0.52187\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 548 | loss: 0.52187 - acc: 0.7753 -- iter: 594/594\n",
      "--\n",
      "Training Step: 549  | total loss: \u001b[1m\u001b[32m0.51154\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 549 | loss: 0.51154 - acc: 0.7826 -- iter: 594/594\n",
      "--\n",
      "Training Step: 550  | total loss: \u001b[1m\u001b[32m0.50249\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 550 | loss: 0.50249 - acc: 0.7895 -- iter: 594/594\n",
      "--\n",
      "Training Step: 551  | total loss: \u001b[1m\u001b[32m0.49361\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 551 | loss: 0.49361 - acc: 0.7951 -- iter: 594/594\n",
      "--\n",
      "Training Step: 552  | total loss: \u001b[1m\u001b[32m0.48575\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 552 | loss: 0.48575 - acc: 0.7997 -- iter: 594/594\n",
      "--\n",
      "Training Step: 553  | total loss: \u001b[1m\u001b[32m0.47832\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 553 | loss: 0.47832 - acc: 0.8041 -- iter: 594/594\n",
      "--\n",
      "Training Step: 554  | total loss: \u001b[1m\u001b[32m0.52191\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 554 | loss: 0.52191 - acc: 0.7771 -- iter: 594/594\n",
      "--\n",
      "Training Step: 555  | total loss: \u001b[1m\u001b[32m0.51051\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 555 | loss: 0.51051 - acc: 0.7840 -- iter: 594/594\n",
      "--\n",
      "Training Step: 556  | total loss: \u001b[1m\u001b[32m0.55536\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 556 | loss: 0.55536 - acc: 0.7568 -- iter: 594/594\n",
      "--\n",
      "Training Step: 557  | total loss: \u001b[1m\u001b[32m0.54107\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 557 | loss: 0.54107 - acc: 0.7655 -- iter: 594/594\n",
      "--\n",
      "Training Step: 558  | total loss: \u001b[1m\u001b[32m0.57676\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 558 | loss: 0.57676 - acc: 0.7425 -- iter: 594/594\n",
      "--\n",
      "Training Step: 559  | total loss: \u001b[1m\u001b[32m0.56071\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 559 | loss: 0.56071 - acc: 0.7526 -- iter: 594/594\n",
      "--\n",
      "Training Step: 560  | total loss: \u001b[1m\u001b[32m0.59057\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 560 | loss: 0.59057 - acc: 0.7310 -- iter: 594/594\n",
      "--\n",
      "Training Step: 561  | total loss: \u001b[1m\u001b[32m0.57399\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 561 | loss: 0.57399 - acc: 0.7417 -- iter: 594/594\n",
      "--\n",
      "Training Step: 562  | total loss: \u001b[1m\u001b[32m0.56031\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 562 | loss: 0.56031 - acc: 0.7533 -- iter: 594/594\n",
      "--\n",
      "Training Step: 563  | total loss: \u001b[1m\u001b[32m0.54814\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 563 | loss: 0.54814 - acc: 0.7624 -- iter: 594/594\n",
      "--\n",
      "Training Step: 564  | total loss: \u001b[1m\u001b[32m0.53758\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 564 | loss: 0.53758 - acc: 0.7702 -- iter: 594/594\n",
      "--\n",
      "Training Step: 565  | total loss: \u001b[1m\u001b[32m0.52768\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 565 | loss: 0.52768 - acc: 0.7779 -- iter: 594/594\n",
      "--\n",
      "Training Step: 566  | total loss: \u001b[1m\u001b[32m0.51943\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 566 | loss: 0.51943 - acc: 0.7846 -- iter: 594/594\n",
      "--\n",
      "Training Step: 567  | total loss: \u001b[1m\u001b[32m0.51176\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 567 | loss: 0.51176 - acc: 0.7915 -- iter: 594/594\n",
      "--\n",
      "Training Step: 568  | total loss: \u001b[1m\u001b[32m0.54146\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 568 | loss: 0.54146 - acc: 0.7644 -- iter: 594/594\n",
      "--\n",
      "Training Step: 569  | total loss: \u001b[1m\u001b[32m0.53133\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 569 | loss: 0.53133 - acc: 0.7716 -- iter: 594/594\n",
      "--\n",
      "Training Step: 570  | total loss: \u001b[1m\u001b[32m0.52194\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 570 | loss: 0.52194 - acc: 0.7796 -- iter: 594/594\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 571  | total loss: \u001b[1m\u001b[32m0.51378\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 571 | loss: 0.51378 - acc: 0.7863 -- iter: 594/594\n",
      "--\n",
      "Training Step: 572  | total loss: \u001b[1m\u001b[32m0.54352\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 572 | loss: 0.54352 - acc: 0.7607 -- iter: 594/594\n",
      "--\n",
      "Training Step: 573  | total loss: \u001b[1m\u001b[32m0.53262\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 573 | loss: 0.53262 - acc: 0.7690 -- iter: 594/594\n",
      "--\n",
      "Training Step: 574  | total loss: \u001b[1m\u001b[32m0.52302\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 574 | loss: 0.52302 - acc: 0.7766 -- iter: 594/594\n",
      "--\n",
      "Training Step: 575  | total loss: \u001b[1m\u001b[32m0.51432\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 575 | loss: 0.51432 - acc: 0.7830 -- iter: 594/594\n",
      "--\n",
      "Training Step: 576  | total loss: \u001b[1m\u001b[32m0.54441\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 576 | loss: 0.54441 - acc: 0.7590 -- iter: 594/594\n",
      "--\n",
      "Training Step: 577  | total loss: \u001b[1m\u001b[32m0.53308\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 577 | loss: 0.53308 - acc: 0.7675 -- iter: 594/594\n",
      "--\n",
      "Training Step: 578  | total loss: \u001b[1m\u001b[32m0.52273\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 578 | loss: 0.52273 - acc: 0.7747 -- iter: 594/594\n",
      "--\n",
      "Training Step: 579  | total loss: \u001b[1m\u001b[32m0.51370\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 579 | loss: 0.51370 - acc: 0.7821 -- iter: 594/594\n",
      "--\n",
      "Training Step: 580  | total loss: \u001b[1m\u001b[32m0.50485\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 580 | loss: 0.50485 - acc: 0.7886 -- iter: 594/594\n",
      "--\n",
      "Training Step: 581  | total loss: \u001b[1m\u001b[32m0.49682\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 581 | loss: 0.49682 - acc: 0.7942 -- iter: 594/594\n",
      "--\n",
      "Training Step: 582  | total loss: \u001b[1m\u001b[32m0.53074\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 582 | loss: 0.53074 - acc: 0.7695 -- iter: 594/594\n",
      "--\n",
      "Training Step: 583  | total loss: \u001b[1m\u001b[32m0.51953\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 583 | loss: 0.51953 - acc: 0.7767 -- iter: 594/594\n",
      "--\n",
      "Training Step: 584  | total loss: \u001b[1m\u001b[32m0.51007\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 584 | loss: 0.51007 - acc: 0.7838 -- iter: 594/594\n",
      "--\n",
      "Training Step: 585  | total loss: \u001b[1m\u001b[32m0.50115\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 585 | loss: 0.50115 - acc: 0.7894 -- iter: 594/594\n",
      "--\n",
      "Training Step: 586  | total loss: \u001b[1m\u001b[32m0.53430\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 586 | loss: 0.53430 - acc: 0.7655 -- iter: 594/594\n",
      "--\n",
      "Training Step: 587  | total loss: \u001b[1m\u001b[32m0.52306\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 587 | loss: 0.52306 - acc: 0.7735 -- iter: 594/594\n",
      "--\n",
      "Training Step: 588  | total loss: \u001b[1m\u001b[32m0.51261\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 588 | loss: 0.51261 - acc: 0.7806 -- iter: 594/594\n",
      "--\n",
      "Training Step: 589  | total loss: \u001b[1m\u001b[32m0.50315\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 589 | loss: 0.50315 - acc: 0.7869 -- iter: 594/594\n",
      "--\n",
      "Training Step: 590  | total loss: \u001b[1m\u001b[32m0.49453\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 590 | loss: 0.49453 - acc: 0.7927 -- iter: 594/594\n",
      "--\n",
      "Training Step: 591  | total loss: \u001b[1m\u001b[32m0.48675\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 591 | loss: 0.48675 - acc: 0.7980 -- iter: 594/594\n",
      "--\n",
      "Training Step: 592  | total loss: \u001b[1m\u001b[32m0.52550\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 592 | loss: 0.52550 - acc: 0.7709 -- iter: 594/594\n",
      "--\n",
      "Training Step: 593  | total loss: \u001b[1m\u001b[32m0.51460\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 593 | loss: 0.51460 - acc: 0.7781 -- iter: 594/594\n",
      "--\n",
      "Training Step: 594  | total loss: \u001b[1m\u001b[32m0.54644\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 594 | loss: 0.54644 - acc: 0.7572 -- iter: 594/594\n",
      "--\n",
      "Training Step: 595  | total loss: \u001b[1m\u001b[32m0.53337\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 595 | loss: 0.53337 - acc: 0.7662 -- iter: 594/594\n",
      "--\n",
      "Training Step: 596  | total loss: \u001b[1m\u001b[32m0.52181\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 596 | loss: 0.52181 - acc: 0.7741 -- iter: 594/594\n",
      "--\n",
      "Training Step: 597  | total loss: \u001b[1m\u001b[32m0.51128\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 597 | loss: 0.51128 - acc: 0.7818 -- iter: 594/594\n",
      "--\n",
      "Training Step: 598  | total loss: \u001b[1m\u001b[32m0.50206\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 598 | loss: 0.50206 - acc: 0.7887 -- iter: 594/594\n",
      "--\n",
      "Training Step: 599  | total loss: \u001b[1m\u001b[32m0.49360\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 599 | loss: 0.49360 - acc: 0.7945 -- iter: 594/594\n",
      "--\n",
      "Training Step: 600  | total loss: \u001b[1m\u001b[32m0.52689\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 600 | loss: 0.52689 - acc: 0.7709 -- iter: 594/594\n",
      "--\n",
      "Training Step: 601  | total loss: \u001b[1m\u001b[32m0.51593\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 601 | loss: 0.51593 - acc: 0.7787 -- iter: 594/594\n",
      "--\n",
      "Training Step: 602  | total loss: \u001b[1m\u001b[32m0.50614\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 602 | loss: 0.50614 - acc: 0.7853 -- iter: 594/594\n",
      "--\n",
      "Training Step: 603  | total loss: \u001b[1m\u001b[32m0.49791\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 603 | loss: 0.49791 - acc: 0.7908 -- iter: 594/594\n",
      "--\n",
      "Training Step: 604  | total loss: \u001b[1m\u001b[32m0.54277\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 604 | loss: 0.54277 - acc: 0.7594 -- iter: 594/594\n",
      "--\n",
      "Training Step: 605  | total loss: \u001b[1m\u001b[32m0.53047\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 605 | loss: 0.53047 - acc: 0.7678 -- iter: 594/594\n",
      "--\n",
      "Training Step: 606  | total loss: \u001b[1m\u001b[32m0.56056\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 606 | loss: 0.56056 - acc: 0.7455 -- iter: 594/594\n",
      "--\n",
      "Training Step: 607  | total loss: \u001b[1m\u001b[32m0.54702\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 607 | loss: 0.54702 - acc: 0.7555 -- iter: 594/594\n",
      "--\n",
      "Training Step: 608  | total loss: \u001b[1m\u001b[32m0.53535\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 608 | loss: 0.53535 - acc: 0.7643 -- iter: 594/594\n",
      "--\n",
      "Training Step: 609  | total loss: \u001b[1m\u001b[32m0.52483\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 609 | loss: 0.52483 - acc: 0.7725 -- iter: 594/594\n",
      "--\n",
      "Training Step: 610  | total loss: \u001b[1m\u001b[32m0.51561\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 610 | loss: 0.51561 - acc: 0.7801 -- iter: 594/594\n",
      "--\n",
      "Training Step: 611  | total loss: \u001b[1m\u001b[32m0.50668\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 611 | loss: 0.50668 - acc: 0.7871 -- iter: 594/594\n",
      "--\n",
      "Training Step: 612  | total loss: \u001b[1m\u001b[32m0.49872\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 612 | loss: 0.49872 - acc: 0.7923 -- iter: 594/594\n",
      "--\n",
      "Training Step: 613  | total loss: \u001b[1m\u001b[32m0.49175\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 613 | loss: 0.49175 - acc: 0.7977 -- iter: 594/594\n",
      "--\n",
      "Training Step: 614  | total loss: \u001b[1m\u001b[32m0.52260\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 614 | loss: 0.52260 - acc: 0.7738 -- iter: 594/594\n",
      "--\n",
      "Training Step: 615  | total loss: \u001b[1m\u001b[32m0.51230\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 615 | loss: 0.51230 - acc: 0.7811 -- iter: 594/594\n",
      "--\n",
      "Training Step: 616  | total loss: \u001b[1m\u001b[32m0.50347\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 616 | loss: 0.50347 - acc: 0.7879 -- iter: 594/594\n",
      "--\n",
      "Training Step: 617  | total loss: \u001b[1m\u001b[32m0.49526\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 617 | loss: 0.49526 - acc: 0.7939 -- iter: 594/594\n",
      "--\n",
      "Training Step: 618  | total loss: \u001b[1m\u001b[32m0.52896\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 618 | loss: 0.52896 - acc: 0.7679 -- iter: 594/594\n",
      "--\n",
      "Training Step: 619  | total loss: \u001b[1m\u001b[32m0.51810\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 619 | loss: 0.51810 - acc: 0.7765 -- iter: 594/594\n",
      "--\n",
      "Training Step: 620  | total loss: \u001b[1m\u001b[32m0.50856\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 620 | loss: 0.50856 - acc: 0.7832 -- iter: 594/594\n",
      "--\n",
      "Training Step: 621  | total loss: \u001b[1m\u001b[32m0.49997\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 621 | loss: 0.49997 - acc: 0.7889 -- iter: 594/594\n",
      "--\n",
      "Training Step: 622  | total loss: \u001b[1m\u001b[32m0.53757\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 622 | loss: 0.53757 - acc: 0.7638 -- iter: 594/594\n",
      "--\n",
      "Training Step: 623  | total loss: \u001b[1m\u001b[32m0.52584\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 623 | loss: 0.52584 - acc: 0.7716 -- iter: 594/594\n",
      "--\n",
      "Training Step: 624  | total loss: \u001b[1m\u001b[32m0.51521\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 624 | loss: 0.51521 - acc: 0.7785 -- iter: 594/594\n",
      "--\n",
      "Training Step: 625  | total loss: \u001b[1m\u001b[32m0.50535\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 625 | loss: 0.50535 - acc: 0.7851 -- iter: 594/594\n",
      "--\n",
      "Training Step: 626  | total loss: \u001b[1m\u001b[32m0.49642\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 626 | loss: 0.49642 - acc: 0.7910 -- iter: 594/594\n",
      "--\n",
      "Training Step: 627  | total loss: \u001b[1m\u001b[32m0.48929\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 627 | loss: 0.48929 - acc: 0.7962 -- iter: 594/594\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 628  | total loss: \u001b[1m\u001b[32m0.48182\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 628 | loss: 0.48182 - acc: 0.8011 -- iter: 594/594\n",
      "--\n",
      "Training Step: 629  | total loss: \u001b[1m\u001b[32m0.47517\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 629 | loss: 0.47517 - acc: 0.8050 -- iter: 594/594\n",
      "--\n",
      "Training Step: 630  | total loss: \u001b[1m\u001b[32m0.51244\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 630 | loss: 0.51244 - acc: 0.7806 -- iter: 594/594\n",
      "--\n",
      "Training Step: 631  | total loss: \u001b[1m\u001b[32m0.50235\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 631 | loss: 0.50235 - acc: 0.7872 -- iter: 594/594\n",
      "--\n",
      "Training Step: 632  | total loss: \u001b[1m\u001b[32m0.49345\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 632 | loss: 0.49345 - acc: 0.7931 -- iter: 594/594\n",
      "--\n",
      "Training Step: 633  | total loss: \u001b[1m\u001b[32m0.48517\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 633 | loss: 0.48517 - acc: 0.7977 -- iter: 594/594\n",
      "--\n",
      "Training Step: 634  | total loss: \u001b[1m\u001b[32m0.47724\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 634 | loss: 0.47724 - acc: 0.8026 -- iter: 594/594\n",
      "--\n",
      "Training Step: 635  | total loss: \u001b[1m\u001b[32m0.47067\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 635 | loss: 0.47067 - acc: 0.8068 -- iter: 594/594\n",
      "--\n",
      "Training Step: 636  | total loss: \u001b[1m\u001b[32m0.46445\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 636 | loss: 0.46445 - acc: 0.8107 -- iter: 594/594\n",
      "--\n",
      "Training Step: 637  | total loss: \u001b[1m\u001b[32m0.45865\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 637 | loss: 0.45865 - acc: 0.8141 -- iter: 594/594\n",
      "--\n",
      "Training Step: 638  | total loss: \u001b[1m\u001b[32m0.45347\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 638 | loss: 0.45347 - acc: 0.8169 -- iter: 594/594\n",
      "--\n",
      "Training Step: 639  | total loss: \u001b[1m\u001b[32m0.44847\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 639 | loss: 0.44847 - acc: 0.8194 -- iter: 594/594\n",
      "--\n",
      "Training Step: 640  | total loss: \u001b[1m\u001b[32m0.50380\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 640 | loss: 0.50380 - acc: 0.7883 -- iter: 594/594\n",
      "--\n",
      "Training Step: 641  | total loss: \u001b[1m\u001b[32m0.49390\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 641 | loss: 0.49390 - acc: 0.7940 -- iter: 594/594\n",
      "--\n",
      "Training Step: 642  | total loss: \u001b[1m\u001b[32m0.48396\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 642 | loss: 0.48396 - acc: 0.7991 -- iter: 594/594\n",
      "--\n",
      "Training Step: 643  | total loss: \u001b[1m\u001b[32m0.47584\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 643 | loss: 0.47584 - acc: 0.8038 -- iter: 594/594\n",
      "--\n",
      "Training Step: 644  | total loss: \u001b[1m\u001b[32m0.46832\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 644 | loss: 0.46832 - acc: 0.8083 -- iter: 594/594\n",
      "--\n",
      "Training Step: 645  | total loss: \u001b[1m\u001b[32m0.46171\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 645 | loss: 0.46171 - acc: 0.8122 -- iter: 594/594\n",
      "--\n",
      "Training Step: 646  | total loss: \u001b[1m\u001b[32m0.45583\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 646 | loss: 0.45583 - acc: 0.8160 -- iter: 594/594\n",
      "--\n",
      "Training Step: 647  | total loss: \u001b[1m\u001b[32m0.45057\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 647 | loss: 0.45057 - acc: 0.8187 -- iter: 594/594\n",
      "--\n",
      "Training Step: 648  | total loss: \u001b[1m\u001b[32m0.44549\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 648 | loss: 0.44549 - acc: 0.8217 -- iter: 594/594\n",
      "--\n",
      "Training Step: 649  | total loss: \u001b[1m\u001b[32m0.44092\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 649 | loss: 0.44092 - acc: 0.8245 -- iter: 594/594\n",
      "--\n",
      "Training Step: 650  | total loss: \u001b[1m\u001b[32m0.50149\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 650 | loss: 0.50149 - acc: 0.7929 -- iter: 594/594\n",
      "--\n",
      "Training Step: 651  | total loss: \u001b[1m\u001b[32m0.49156\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 651 | loss: 0.49156 - acc: 0.7981 -- iter: 594/594\n",
      "--\n",
      "Training Step: 652  | total loss: \u001b[1m\u001b[32m0.48225\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 652 | loss: 0.48225 - acc: 0.8028 -- iter: 594/594\n",
      "--\n",
      "Training Step: 653  | total loss: \u001b[1m\u001b[32m0.47473\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 653 | loss: 0.47473 - acc: 0.8071 -- iter: 594/594\n",
      "--\n",
      "Training Step: 654  | total loss: \u001b[1m\u001b[32m0.52328\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 654 | loss: 0.52328 - acc: 0.7787 -- iter: 594/594\n",
      "--\n",
      "Training Step: 655  | total loss: \u001b[1m\u001b[32m0.51118\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 655 | loss: 0.51118 - acc: 0.7850 -- iter: 594/594\n",
      "--\n",
      "Training Step: 656  | total loss: \u001b[1m\u001b[32m0.50170\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 656 | loss: 0.50170 - acc: 0.7907 -- iter: 594/594\n",
      "--\n",
      "Training Step: 657  | total loss: \u001b[1m\u001b[32m0.49271\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 657 | loss: 0.49271 - acc: 0.7968 -- iter: 594/594\n",
      "--\n",
      "Training Step: 658  | total loss: \u001b[1m\u001b[32m0.48549\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 658 | loss: 0.48549 - acc: 0.8013 -- iter: 594/594\n",
      "--\n",
      "Training Step: 659  | total loss: \u001b[1m\u001b[32m0.47839\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 659 | loss: 0.47839 - acc: 0.8060 -- iter: 594/594\n",
      "--\n",
      "Training Step: 660  | total loss: \u001b[1m\u001b[32m0.51511\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 660 | loss: 0.51511 - acc: 0.7801 -- iter: 594/594\n",
      "--\n",
      "Training Step: 661  | total loss: \u001b[1m\u001b[32m0.50532\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 661 | loss: 0.50532 - acc: 0.7871 -- iter: 594/594\n",
      "--\n",
      "Training Step: 662  | total loss: \u001b[1m\u001b[32m0.49670\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 662 | loss: 0.49670 - acc: 0.7938 -- iter: 594/594\n",
      "--\n",
      "Training Step: 663  | total loss: \u001b[1m\u001b[32m0.48906\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 663 | loss: 0.48906 - acc: 0.7992 -- iter: 594/594\n",
      "--\n",
      "Training Step: 664  | total loss: \u001b[1m\u001b[32m0.48180\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 664 | loss: 0.48180 - acc: 0.8043 -- iter: 594/594\n",
      "--\n",
      "Training Step: 665  | total loss: \u001b[1m\u001b[32m0.47571\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 665 | loss: 0.47571 - acc: 0.8084 -- iter: 594/594\n",
      "--\n",
      "Training Step: 666  | total loss: \u001b[1m\u001b[32m0.47075\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 666 | loss: 0.47075 - acc: 0.8126 -- iter: 594/594\n",
      "--\n",
      "Training Step: 667  | total loss: \u001b[1m\u001b[32m0.46603\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 667 | loss: 0.46603 - acc: 0.8155 -- iter: 594/594\n",
      "--\n",
      "Training Step: 668  | total loss: \u001b[1m\u001b[32m0.46126\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 668 | loss: 0.46126 - acc: 0.8191 -- iter: 594/594\n",
      "--\n",
      "Training Step: 669  | total loss: \u001b[1m\u001b[32m0.45685\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 669 | loss: 0.45685 - acc: 0.8219 -- iter: 594/594\n",
      "--\n",
      "Training Step: 670  | total loss: \u001b[1m\u001b[32m0.49978\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 670 | loss: 0.49978 - acc: 0.7933 -- iter: 594/594\n",
      "--\n",
      "Training Step: 671  | total loss: \u001b[1m\u001b[32m0.49110\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 671 | loss: 0.49110 - acc: 0.7986 -- iter: 594/594\n",
      "--\n",
      "Training Step: 672  | total loss: \u001b[1m\u001b[32m0.53230\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 672 | loss: 0.53230 - acc: 0.7719 -- iter: 594/594\n",
      "--\n",
      "Training Step: 673  | total loss: \u001b[1m\u001b[32m0.52024\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 673 | loss: 0.52024 - acc: 0.7796 -- iter: 594/594\n",
      "--\n",
      "Training Step: 674  | total loss: \u001b[1m\u001b[32m0.50960\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 674 | loss: 0.50960 - acc: 0.7862 -- iter: 594/594\n",
      "--\n",
      "Training Step: 675  | total loss: \u001b[1m\u001b[32m0.50032\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 675 | loss: 0.50032 - acc: 0.7915 -- iter: 594/594\n",
      "--\n",
      "Training Step: 676  | total loss: \u001b[1m\u001b[32m0.49249\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 676 | loss: 0.49249 - acc: 0.7971 -- iter: 594/594\n",
      "--\n",
      "Training Step: 677  | total loss: \u001b[1m\u001b[32m0.48449\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 677 | loss: 0.48449 - acc: 0.8019 -- iter: 594/594\n",
      "--\n",
      "Training Step: 678  | total loss: \u001b[1m\u001b[32m0.52081\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 678 | loss: 0.52081 - acc: 0.7781 -- iter: 594/594\n",
      "--\n",
      "Training Step: 679  | total loss: \u001b[1m\u001b[32m0.51067\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 679 | loss: 0.51067 - acc: 0.7848 -- iter: 594/594\n",
      "--\n",
      "Training Step: 680  | total loss: \u001b[1m\u001b[32m0.50120\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 680 | loss: 0.50120 - acc: 0.7910 -- iter: 594/594\n",
      "--\n",
      "Training Step: 681  | total loss: \u001b[1m\u001b[32m0.49283\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 681 | loss: 0.49283 - acc: 0.7966 -- iter: 594/594\n",
      "--\n",
      "Training Step: 682  | total loss: \u001b[1m\u001b[32m0.48490\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 682 | loss: 0.48490 - acc: 0.8011 -- iter: 594/594\n",
      "--\n",
      "Training Step: 683  | total loss: \u001b[1m\u001b[32m0.47778\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 683 | loss: 0.47778 - acc: 0.8055 -- iter: 594/594\n",
      "--\n",
      "Training Step: 684  | total loss: \u001b[1m\u001b[32m0.47159\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 684 | loss: 0.47159 - acc: 0.8098 -- iter: 594/594\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 685  | total loss: \u001b[1m\u001b[32m0.46544\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 685 | loss: 0.46544 - acc: 0.8135 -- iter: 594/594\n",
      "--\n",
      "Training Step: 686  | total loss: \u001b[1m\u001b[32m0.50518\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 686 | loss: 0.50518 - acc: 0.7867 -- iter: 594/594\n",
      "--\n",
      "Training Step: 687  | total loss: \u001b[1m\u001b[32m0.49664\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 687 | loss: 0.49664 - acc: 0.7924 -- iter: 594/594\n",
      "--\n",
      "Training Step: 688  | total loss: \u001b[1m\u001b[32m0.48854\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 688 | loss: 0.48854 - acc: 0.7978 -- iter: 594/594\n",
      "--\n",
      "Training Step: 689  | total loss: \u001b[1m\u001b[32m0.48063\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 689 | loss: 0.48063 - acc: 0.8034 -- iter: 594/594\n",
      "--\n",
      "Training Step: 690  | total loss: \u001b[1m\u001b[32m0.47386\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 690 | loss: 0.47386 - acc: 0.8076 -- iter: 594/594\n",
      "--\n",
      "Training Step: 691  | total loss: \u001b[1m\u001b[32m0.46761\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 691 | loss: 0.46761 - acc: 0.8111 -- iter: 594/594\n",
      "--\n",
      "Training Step: 692  | total loss: \u001b[1m\u001b[32m0.46197\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 692 | loss: 0.46197 - acc: 0.8144 -- iter: 594/594\n",
      "--\n",
      "Training Step: 693  | total loss: \u001b[1m\u001b[32m0.45637\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 693 | loss: 0.45637 - acc: 0.8178 -- iter: 594/594\n",
      "--\n",
      "Training Step: 694  | total loss: \u001b[1m\u001b[32m0.45143\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 694 | loss: 0.45143 - acc: 0.8209 -- iter: 594/594\n",
      "--\n",
      "Training Step: 695  | total loss: \u001b[1m\u001b[32m0.44655\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 695 | loss: 0.44655 - acc: 0.8233 -- iter: 594/594\n",
      "--\n",
      "Training Step: 696  | total loss: \u001b[1m\u001b[32m0.49517\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 696 | loss: 0.49517 - acc: 0.7952 -- iter: 594/594\n",
      "--\n",
      "Training Step: 697  | total loss: \u001b[1m\u001b[32m0.48617\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 697 | loss: 0.48617 - acc: 0.8003 -- iter: 594/594\n",
      "--\n",
      "Training Step: 698  | total loss: \u001b[1m\u001b[32m0.47793\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 698 | loss: 0.47793 - acc: 0.8051 -- iter: 594/594\n",
      "--\n",
      "Training Step: 699  | total loss: \u001b[1m\u001b[32m0.47051\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 699 | loss: 0.47051 - acc: 0.8095 -- iter: 594/594\n",
      "--\n",
      "Training Step: 700  | total loss: \u001b[1m\u001b[32m0.52222\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 700 | loss: 0.52222 - acc: 0.7811 -- iter: 594/594\n",
      "--\n",
      "Training Step: 701  | total loss: \u001b[1m\u001b[32m0.51006\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 701 | loss: 0.51006 - acc: 0.7876 -- iter: 594/594\n",
      "--\n",
      "Training Step: 702  | total loss: \u001b[1m\u001b[32m0.55524\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 702 | loss: 0.55524 - acc: 0.7604 -- iter: 594/594\n",
      "--\n",
      "Training Step: 703  | total loss: \u001b[1m\u001b[32m0.54103\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 703 | loss: 0.54103 - acc: 0.7690 -- iter: 594/594\n",
      "--\n",
      "Training Step: 704  | total loss: \u001b[1m\u001b[32m0.57853\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 704 | loss: 0.57853 - acc: 0.7440 -- iter: 594/594\n",
      "--\n",
      "Training Step: 705  | total loss: \u001b[1m\u001b[32m0.56340\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 705 | loss: 0.56340 - acc: 0.7539 -- iter: 594/594\n",
      "--\n",
      "Training Step: 706  | total loss: \u001b[1m\u001b[32m0.54974\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 706 | loss: 0.54974 - acc: 0.7632 -- iter: 594/594\n",
      "--\n",
      "Training Step: 707  | total loss: \u001b[1m\u001b[32m0.53832\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 707 | loss: 0.53832 - acc: 0.7709 -- iter: 594/594\n",
      "--\n",
      "Training Step: 708  | total loss: \u001b[1m\u001b[32m0.56457\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 708 | loss: 0.56457 - acc: 0.7477 -- iter: 594/594\n",
      "--\n",
      "Training Step: 709  | total loss: \u001b[1m\u001b[32m0.55258\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 709 | loss: 0.55258 - acc: 0.7567 -- iter: 594/594\n",
      "--\n",
      "Training Step: 710  | total loss: \u001b[1m\u001b[32m0.57595\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 710 | loss: 0.57595 - acc: 0.7353 -- iter: 594/594\n",
      "--\n",
      "Training Step: 711  | total loss: \u001b[1m\u001b[32m0.56390\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 711 | loss: 0.56390 - acc: 0.7461 -- iter: 594/594\n",
      "--\n",
      "Training Step: 712  | total loss: \u001b[1m\u001b[32m0.55386\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 712 | loss: 0.55386 - acc: 0.7550 -- iter: 594/594\n",
      "--\n",
      "Training Step: 713  | total loss: \u001b[1m\u001b[32m0.54504\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 713 | loss: 0.54504 - acc: 0.7640 -- iter: 594/594\n",
      "--\n",
      "Training Step: 714  | total loss: \u001b[1m\u001b[32m0.56535\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 714 | loss: 0.56535 - acc: 0.7430 -- iter: 594/594\n",
      "--\n",
      "Training Step: 715  | total loss: \u001b[1m\u001b[32m0.55584\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 715 | loss: 0.55584 - acc: 0.7524 -- iter: 594/594\n",
      "--\n",
      "Training Step: 716  | total loss: \u001b[1m\u001b[32m0.57529\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 716 | loss: 0.57529 - acc: 0.7312 -- iter: 594/594\n",
      "--\n",
      "Training Step: 717  | total loss: \u001b[1m\u001b[32m0.56495\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 717 | loss: 0.56495 - acc: 0.7415 -- iter: 594/594\n",
      "--\n",
      "Training Step: 718  | total loss: \u001b[1m\u001b[32m0.55604\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 718 | loss: 0.55604 - acc: 0.7512 -- iter: 594/594\n",
      "--\n",
      "Training Step: 719  | total loss: \u001b[1m\u001b[32m0.54789\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 719 | loss: 0.54789 - acc: 0.7593 -- iter: 594/594\n",
      "--\n",
      "Training Step: 720  | total loss: \u001b[1m\u001b[32m0.56977\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 720 | loss: 0.56977 - acc: 0.7362 -- iter: 594/594\n",
      "--\n",
      "Training Step: 721  | total loss: \u001b[1m\u001b[32m0.56041\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 721 | loss: 0.56041 - acc: 0.7458 -- iter: 594/594\n",
      "--\n",
      "Training Step: 722  | total loss: \u001b[1m\u001b[32m0.55166\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 722 | loss: 0.55166 - acc: 0.7533 -- iter: 594/594\n",
      "--\n",
      "Training Step: 723  | total loss: \u001b[1m\u001b[32m0.54329\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 723 | loss: 0.54329 - acc: 0.7617 -- iter: 594/594\n",
      "--\n",
      "Training Step: 724  | total loss: \u001b[1m\u001b[32m0.53599\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 724 | loss: 0.53599 - acc: 0.7685 -- iter: 594/594\n",
      "--\n",
      "Training Step: 725  | total loss: \u001b[1m\u001b[32m0.52875\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 725 | loss: 0.52875 - acc: 0.7757 -- iter: 594/594\n",
      "--\n",
      "Training Step: 726  | total loss: \u001b[1m\u001b[32m0.52162\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 726 | loss: 0.52162 - acc: 0.7828 -- iter: 594/594\n",
      "--\n",
      "Training Step: 727  | total loss: \u001b[1m\u001b[32m0.51458\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 727 | loss: 0.51458 - acc: 0.7883 -- iter: 594/594\n",
      "--\n",
      "Training Step: 728  | total loss: \u001b[1m\u001b[32m0.50749\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 728 | loss: 0.50749 - acc: 0.7932 -- iter: 594/594\n",
      "--\n",
      "Training Step: 729  | total loss: \u001b[1m\u001b[32m0.50067\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 729 | loss: 0.50067 - acc: 0.7980 -- iter: 594/594\n",
      "--\n",
      "Training Step: 730  | total loss: \u001b[1m\u001b[32m0.49378\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 730 | loss: 0.49378 - acc: 0.8027 -- iter: 594/594\n",
      "--\n",
      "Training Step: 731  | total loss: \u001b[1m\u001b[32m0.48710\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 731 | loss: 0.48710 - acc: 0.8070 -- iter: 594/594\n",
      "--\n",
      "Training Step: 732  | total loss: \u001b[1m\u001b[32m0.48034\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 732 | loss: 0.48034 - acc: 0.8110 -- iter: 594/594\n",
      "--\n",
      "Training Step: 733  | total loss: \u001b[1m\u001b[32m0.47369\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 733 | loss: 0.47369 - acc: 0.8140 -- iter: 594/594\n",
      "--\n",
      "Training Step: 734  | total loss: \u001b[1m\u001b[32m0.46778\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 734 | loss: 0.46778 - acc: 0.8171 -- iter: 594/594\n",
      "--\n",
      "Training Step: 735  | total loss: \u001b[1m\u001b[32m0.46178\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 735 | loss: 0.46178 - acc: 0.8198 -- iter: 594/594\n",
      "--\n",
      "Training Step: 736  | total loss: \u001b[1m\u001b[32m0.51340\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 736 | loss: 0.51340 - acc: 0.7902 -- iter: 594/594\n",
      "--\n",
      "Training Step: 737  | total loss: \u001b[1m\u001b[32m0.50282\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 737 | loss: 0.50282 - acc: 0.7960 -- iter: 594/594\n",
      "--\n",
      "Training Step: 738  | total loss: \u001b[1m\u001b[32m0.55039\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 738 | loss: 0.55039 - acc: 0.7711 -- iter: 594/594\n",
      "--\n",
      "Training Step: 739  | total loss: \u001b[1m\u001b[32m0.53554\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 739 | loss: 0.53554 - acc: 0.7788 -- iter: 594/594\n",
      "--\n",
      "Training Step: 740  | total loss: \u001b[1m\u001b[32m0.57517\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 740 | loss: 0.57517 - acc: 0.7536 -- iter: 594/594\n",
      "--\n",
      "Training Step: 741  | total loss: \u001b[1m\u001b[32m0.55849\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 741 | loss: 0.55849 - acc: 0.7630 -- iter: 594/594\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 742  | total loss: \u001b[1m\u001b[32m0.54263\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 742 | loss: 0.54263 - acc: 0.7708 -- iter: 594/594\n",
      "--\n",
      "Training Step: 743  | total loss: \u001b[1m\u001b[32m0.53016\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 743 | loss: 0.53016 - acc: 0.7779 -- iter: 594/594\n",
      "--\n",
      "Training Step: 744  | total loss: \u001b[1m\u001b[32m0.56665\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 744 | loss: 0.56665 - acc: 0.7535 -- iter: 594/594\n",
      "--\n",
      "Training Step: 745  | total loss: \u001b[1m\u001b[32m0.55221\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 745 | loss: 0.55221 - acc: 0.7622 -- iter: 594/594\n",
      "--\n",
      "Training Step: 746  | total loss: \u001b[1m\u001b[32m0.58148\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 746 | loss: 0.58148 - acc: 0.7402 -- iter: 594/594\n",
      "--\n",
      "Training Step: 747  | total loss: \u001b[1m\u001b[32m0.56628\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 747 | loss: 0.56628 - acc: 0.7507 -- iter: 594/594\n",
      "--\n",
      "Training Step: 748  | total loss: \u001b[1m\u001b[32m0.55287\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 748 | loss: 0.55287 - acc: 0.7599 -- iter: 594/594\n",
      "--\n",
      "Training Step: 749  | total loss: \u001b[1m\u001b[32m0.54078\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 749 | loss: 0.54078 - acc: 0.7686 -- iter: 594/594\n",
      "--\n",
      "Training Step: 750  | total loss: \u001b[1m\u001b[32m0.56572\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 750 | loss: 0.56572 - acc: 0.7473 -- iter: 594/594\n",
      "--\n",
      "Training Step: 751  | total loss: \u001b[1m\u001b[32m0.55343\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 751 | loss: 0.55343 - acc: 0.7564 -- iter: 594/594\n",
      "--\n",
      "Training Step: 752  | total loss: \u001b[1m\u001b[32m0.57221\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 752 | loss: 0.57221 - acc: 0.7382 -- iter: 594/594\n",
      "--\n",
      "Training Step: 753  | total loss: \u001b[1m\u001b[32m0.56039\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 753 | loss: 0.56039 - acc: 0.7480 -- iter: 594/594\n",
      "--\n",
      "Training Step: 754  | total loss: \u001b[1m\u001b[32m0.54976\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 754 | loss: 0.54976 - acc: 0.7577 -- iter: 594/594\n",
      "--\n",
      "Training Step: 755  | total loss: \u001b[1m\u001b[32m0.54042\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 755 | loss: 0.54042 - acc: 0.7670 -- iter: 594/594\n",
      "--\n",
      "Training Step: 756  | total loss: \u001b[1m\u001b[32m0.53212\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 756 | loss: 0.53212 - acc: 0.7745 -- iter: 594/594\n",
      "--\n",
      "Training Step: 757  | total loss: \u001b[1m\u001b[32m0.52429\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 757 | loss: 0.52429 - acc: 0.7803 -- iter: 594/594\n",
      "--\n",
      "Training Step: 758  | total loss: \u001b[1m\u001b[32m0.54935\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 758 | loss: 0.54935 - acc: 0.7562 -- iter: 594/594\n",
      "--\n",
      "Training Step: 759  | total loss: \u001b[1m\u001b[32m0.53931\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 759 | loss: 0.53931 - acc: 0.7637 -- iter: 594/594\n",
      "--\n",
      "Training Step: 760  | total loss: \u001b[1m\u001b[32m0.56290\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 760 | loss: 0.56290 - acc: 0.7426 -- iter: 594/594\n",
      "--\n",
      "Training Step: 761  | total loss: \u001b[1m\u001b[32m0.55222\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 761 | loss: 0.55222 - acc: 0.7523 -- iter: 594/594\n",
      "--\n",
      "Training Step: 762  | total loss: \u001b[1m\u001b[32m0.54260\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 762 | loss: 0.54260 - acc: 0.7613 -- iter: 594/594\n",
      "--\n",
      "Training Step: 763  | total loss: \u001b[1m\u001b[32m0.53334\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 763 | loss: 0.53334 - acc: 0.7702 -- iter: 594/594\n",
      "--\n",
      "Training Step: 764  | total loss: \u001b[1m\u001b[32m0.52504\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 764 | loss: 0.52504 - acc: 0.7775 -- iter: 594/594\n",
      "--\n",
      "Training Step: 765  | total loss: \u001b[1m\u001b[32m0.51705\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 765 | loss: 0.51705 - acc: 0.7843 -- iter: 594/594\n",
      "--\n",
      "Training Step: 766  | total loss: \u001b[1m\u001b[32m0.50954\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 766 | loss: 0.50954 - acc: 0.7895 -- iter: 594/594\n",
      "--\n",
      "Training Step: 767  | total loss: \u001b[1m\u001b[32m0.50246\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 767 | loss: 0.50246 - acc: 0.7946 -- iter: 594/594\n",
      "--\n",
      "Training Step: 768  | total loss: \u001b[1m\u001b[32m0.53257\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 768 | loss: 0.53257 - acc: 0.7698 -- iter: 594/594\n",
      "--\n",
      "Training Step: 769  | total loss: \u001b[1m\u001b[32m0.52217\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 769 | loss: 0.52217 - acc: 0.7775 -- iter: 594/594\n",
      "--\n",
      "Training Step: 770  | total loss: \u001b[1m\u001b[32m0.55174\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 770 | loss: 0.55174 - acc: 0.7557 -- iter: 594/594\n",
      "--\n",
      "Training Step: 771  | total loss: \u001b[1m\u001b[32m0.53935\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 771 | loss: 0.53935 - acc: 0.7646 -- iter: 594/594\n",
      "--\n",
      "Training Step: 772  | total loss: \u001b[1m\u001b[32m0.52784\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 772 | loss: 0.52784 - acc: 0.7730 -- iter: 594/594\n",
      "--\n",
      "Training Step: 773  | total loss: \u001b[1m\u001b[32m0.51795\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 773 | loss: 0.51795 - acc: 0.7804 -- iter: 594/594\n",
      "--\n",
      "Training Step: 774  | total loss: \u001b[1m\u001b[32m0.50826\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 774 | loss: 0.50826 - acc: 0.7868 -- iter: 594/594\n",
      "--\n",
      "Training Step: 775  | total loss: \u001b[1m\u001b[32m0.49911\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 775 | loss: 0.49911 - acc: 0.7927 -- iter: 594/594\n",
      "--\n",
      "Training Step: 776  | total loss: \u001b[1m\u001b[32m0.49123\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 776 | loss: 0.49123 - acc: 0.7984 -- iter: 594/594\n",
      "--\n",
      "Training Step: 777  | total loss: \u001b[1m\u001b[32m0.48384\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 777 | loss: 0.48384 - acc: 0.8033 -- iter: 594/594\n",
      "--\n",
      "Training Step: 778  | total loss: \u001b[1m\u001b[32m0.53063\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 778 | loss: 0.53063 - acc: 0.7721 -- iter: 594/594\n",
      "--\n",
      "Training Step: 779  | total loss: \u001b[1m\u001b[32m0.51869\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 779 | loss: 0.51869 - acc: 0.7792 -- iter: 594/594\n",
      "--\n",
      "Training Step: 780  | total loss: \u001b[1m\u001b[32m0.55940\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 780 | loss: 0.55940 - acc: 0.7530 -- iter: 594/594\n",
      "--\n",
      "Training Step: 781  | total loss: \u001b[1m\u001b[32m0.54510\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 781 | loss: 0.54510 - acc: 0.7620 -- iter: 594/594\n",
      "--\n",
      "Training Step: 782  | total loss: \u001b[1m\u001b[32m0.53200\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 782 | loss: 0.53200 - acc: 0.7705 -- iter: 594/594\n",
      "--\n",
      "Training Step: 783  | total loss: \u001b[1m\u001b[32m0.52102\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 783 | loss: 0.52102 - acc: 0.7775 -- iter: 594/594\n",
      "--\n",
      "Training Step: 784  | total loss: \u001b[1m\u001b[32m0.51069\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 784 | loss: 0.51069 - acc: 0.7846 -- iter: 594/594\n",
      "--\n",
      "Training Step: 785  | total loss: \u001b[1m\u001b[32m0.50160\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 785 | loss: 0.50160 - acc: 0.7908 -- iter: 594/594\n",
      "--\n",
      "Training Step: 786  | total loss: \u001b[1m\u001b[32m0.54003\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 786 | loss: 0.54003 - acc: 0.7637 -- iter: 594/594\n",
      "--\n",
      "Training Step: 787  | total loss: \u001b[1m\u001b[32m0.52826\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 787 | loss: 0.52826 - acc: 0.7722 -- iter: 594/594\n",
      "--\n",
      "Training Step: 788  | total loss: \u001b[1m\u001b[32m0.51749\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 788 | loss: 0.51749 - acc: 0.7797 -- iter: 594/594\n",
      "--\n",
      "Training Step: 789  | total loss: \u001b[1m\u001b[32m0.50748\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 789 | loss: 0.50748 - acc: 0.7864 -- iter: 594/594\n",
      "--\n",
      "Training Step: 790  | total loss: \u001b[1m\u001b[32m0.54450\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 790 | loss: 0.54450 - acc: 0.7584 -- iter: 594/594\n",
      "--\n",
      "Training Step: 791  | total loss: \u001b[1m\u001b[32m0.53202\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 791 | loss: 0.53202 - acc: 0.7671 -- iter: 594/594\n",
      "--\n",
      "Training Step: 792  | total loss: \u001b[1m\u001b[32m0.52109\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 792 | loss: 0.52109 - acc: 0.7746 -- iter: 594/594\n",
      "--\n",
      "Training Step: 793  | total loss: \u001b[1m\u001b[32m0.51175\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 793 | loss: 0.51175 - acc: 0.7816 -- iter: 594/594\n",
      "--\n",
      "Training Step: 794  | total loss: \u001b[1m\u001b[32m0.54586\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 794 | loss: 0.54586 - acc: 0.7555 -- iter: 594/594\n",
      "--\n",
      "Training Step: 795  | total loss: \u001b[1m\u001b[32m0.53384\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 795 | loss: 0.53384 - acc: 0.7638 -- iter: 594/594\n",
      "--\n",
      "Training Step: 796  | total loss: \u001b[1m\u001b[32m0.52330\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 796 | loss: 0.52330 - acc: 0.7721 -- iter: 594/594\n",
      "--\n",
      "Training Step: 797  | total loss: \u001b[1m\u001b[32m0.51416\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 797 | loss: 0.51416 - acc: 0.7797 -- iter: 594/594\n",
      "--\n",
      "Training Step: 798  | total loss: \u001b[1m\u001b[32m0.50495\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 798 | loss: 0.50495 - acc: 0.7857 -- iter: 594/594\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 799  | total loss: \u001b[1m\u001b[32m0.49741\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 799 | loss: 0.49741 - acc: 0.7913 -- iter: 594/594\n",
      "--\n",
      "Training Step: 800  | total loss: \u001b[1m\u001b[32m0.53241\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 800 | loss: 0.53241 - acc: 0.7652 -- iter: 594/594\n",
      "--\n",
      "Training Step: 801  | total loss: \u001b[1m\u001b[32m0.52189\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 801 | loss: 0.52189 - acc: 0.7724 -- iter: 594/594\n",
      "--\n",
      "Training Step: 802  | total loss: \u001b[1m\u001b[32m0.51266\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 802 | loss: 0.51266 - acc: 0.7793 -- iter: 594/594\n",
      "--\n",
      "Training Step: 803  | total loss: \u001b[1m\u001b[32m0.50360\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 803 | loss: 0.50360 - acc: 0.7867 -- iter: 594/594\n",
      "--\n",
      "Training Step: 804  | total loss: \u001b[1m\u001b[32m0.49585\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 804 | loss: 0.49585 - acc: 0.7926 -- iter: 594/594\n",
      "--\n",
      "Training Step: 805  | total loss: \u001b[1m\u001b[32m0.48844\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 805 | loss: 0.48844 - acc: 0.7975 -- iter: 594/594\n",
      "--\n",
      "Training Step: 806  | total loss: \u001b[1m\u001b[32m0.48132\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 806 | loss: 0.48132 - acc: 0.8019 -- iter: 594/594\n",
      "--\n",
      "Training Step: 807  | total loss: \u001b[1m\u001b[32m0.47484\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 807 | loss: 0.47484 - acc: 0.8062 -- iter: 594/594\n",
      "--\n",
      "Training Step: 808  | total loss: \u001b[1m\u001b[32m0.51893\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 808 | loss: 0.51893 - acc: 0.7751 -- iter: 594/594\n",
      "--\n",
      "Training Step: 809  | total loss: \u001b[1m\u001b[32m0.50851\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 809 | loss: 0.50851 - acc: 0.7814 -- iter: 594/594\n",
      "--\n",
      "Training Step: 810  | total loss: \u001b[1m\u001b[32m0.54550\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 810 | loss: 0.54550 - acc: 0.7592 -- iter: 594/594\n",
      "--\n",
      "Training Step: 811  | total loss: \u001b[1m\u001b[32m0.53225\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 811 | loss: 0.53225 - acc: 0.7683 -- iter: 594/594\n",
      "--\n",
      "Training Step: 812  | total loss: \u001b[1m\u001b[32m0.52113\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 812 | loss: 0.52113 - acc: 0.7761 -- iter: 594/594\n",
      "--\n",
      "Training Step: 813  | total loss: \u001b[1m\u001b[32m0.51113\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 813 | loss: 0.51113 - acc: 0.7835 -- iter: 594/594\n",
      "--\n",
      "Training Step: 814  | total loss: \u001b[1m\u001b[32m0.54406\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 814 | loss: 0.54406 - acc: 0.7597 -- iter: 594/594\n",
      "--\n",
      "Training Step: 815  | total loss: \u001b[1m\u001b[32m0.53106\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 815 | loss: 0.53106 - acc: 0.7684 -- iter: 594/594\n",
      "--\n",
      "Training Step: 816  | total loss: \u001b[1m\u001b[32m0.55992\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 816 | loss: 0.55992 - acc: 0.7468 -- iter: 594/594\n",
      "--\n",
      "Training Step: 817  | total loss: \u001b[1m\u001b[32m0.54704\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 817 | loss: 0.54704 - acc: 0.7566 -- iter: 594/594\n",
      "--\n",
      "Training Step: 818  | total loss: \u001b[1m\u001b[32m0.53529\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 818 | loss: 0.53529 - acc: 0.7650 -- iter: 594/594\n",
      "--\n",
      "Training Step: 819  | total loss: \u001b[1m\u001b[32m0.52489\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 819 | loss: 0.52489 - acc: 0.7728 -- iter: 594/594\n",
      "--\n",
      "Training Step: 820  | total loss: \u001b[1m\u001b[32m0.55604\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 820 | loss: 0.55604 - acc: 0.7482 -- iter: 594/594\n",
      "--\n",
      "Training Step: 821  | total loss: \u001b[1m\u001b[32m0.54428\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 821 | loss: 0.54428 - acc: 0.7579 -- iter: 594/594\n",
      "--\n",
      "Training Step: 822  | total loss: \u001b[1m\u001b[32m0.56888\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 822 | loss: 0.56888 - acc: 0.7377 -- iter: 594/594\n",
      "--\n",
      "Training Step: 823  | total loss: \u001b[1m\u001b[32m0.55601\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 823 | loss: 0.55601 - acc: 0.7481 -- iter: 594/594\n",
      "--\n",
      "Training Step: 824  | total loss: \u001b[1m\u001b[32m0.57766\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 824 | loss: 0.57766 - acc: 0.7309 -- iter: 594/594\n",
      "--\n",
      "Training Step: 825  | total loss: \u001b[1m\u001b[32m0.56495\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 825 | loss: 0.56495 - acc: 0.7433 -- iter: 594/594\n",
      "--\n",
      "Training Step: 826  | total loss: \u001b[1m\u001b[32m0.58978\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 826 | loss: 0.58978 - acc: 0.7198 -- iter: 594/594\n",
      "--\n",
      "Training Step: 827  | total loss: \u001b[1m\u001b[32m0.57610\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 827 | loss: 0.57610 - acc: 0.7320 -- iter: 594/594\n",
      "--\n",
      "Training Step: 828  | total loss: \u001b[1m\u001b[32m0.59502\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 828 | loss: 0.59502 - acc: 0.7122 -- iter: 594/594\n",
      "--\n",
      "Training Step: 829  | total loss: \u001b[1m\u001b[32m0.58174\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 829 | loss: 0.58174 - acc: 0.7256 -- iter: 594/594\n",
      "--\n",
      "Training Step: 830  | total loss: \u001b[1m\u001b[32m0.57015\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 830 | loss: 0.57015 - acc: 0.7362 -- iter: 594/594\n",
      "--\n",
      "Training Step: 831  | total loss: \u001b[1m\u001b[32m0.56010\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 831 | loss: 0.56010 - acc: 0.7458 -- iter: 594/594\n",
      "--\n",
      "Training Step: 832  | total loss: \u001b[1m\u001b[32m0.58129\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 832 | loss: 0.58129 - acc: 0.7246 -- iter: 594/594\n",
      "--\n",
      "Training Step: 833  | total loss: \u001b[1m\u001b[32m0.57035\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 833 | loss: 0.57035 - acc: 0.7356 -- iter: 594/594\n",
      "--\n",
      "Training Step: 834  | total loss: \u001b[1m\u001b[32m0.56070\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 834 | loss: 0.56070 - acc: 0.7452 -- iter: 594/594\n",
      "--\n",
      "Training Step: 835  | total loss: \u001b[1m\u001b[32m0.55209\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 835 | loss: 0.55209 - acc: 0.7535 -- iter: 594/594\n",
      "--\n",
      "Training Step: 836  | total loss: \u001b[1m\u001b[32m0.57337\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 836 | loss: 0.57337 - acc: 0.7317 -- iter: 594/594\n",
      "--\n",
      "Training Step: 837  | total loss: \u001b[1m\u001b[32m0.56321\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 837 | loss: 0.56321 - acc: 0.7419 -- iter: 594/594\n",
      "--\n",
      "Training Step: 838  | total loss: \u001b[1m\u001b[32m0.58238\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 838 | loss: 0.58238 - acc: 0.7221 -- iter: 594/594\n",
      "--\n",
      "Training Step: 839  | total loss: \u001b[1m\u001b[32m0.57130\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 839 | loss: 0.57130 - acc: 0.7332 -- iter: 594/594\n",
      "--\n",
      "Training Step: 840  | total loss: \u001b[1m\u001b[32m0.56106\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 840 | loss: 0.56106 - acc: 0.7432 -- iter: 594/594\n",
      "--\n",
      "Training Step: 841  | total loss: \u001b[1m\u001b[32m0.55170\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 841 | loss: 0.55170 - acc: 0.7522 -- iter: 594/594\n",
      "--\n",
      "Training Step: 842  | total loss: \u001b[1m\u001b[32m0.54269\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 842 | loss: 0.54269 - acc: 0.7610 -- iter: 594/594\n",
      "--\n",
      "Training Step: 843  | total loss: \u001b[1m\u001b[32m0.53415\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 843 | loss: 0.53415 - acc: 0.7682 -- iter: 594/594\n",
      "--\n",
      "Training Step: 844  | total loss: \u001b[1m\u001b[32m0.55702\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 844 | loss: 0.55702 - acc: 0.7454 -- iter: 594/594\n",
      "--\n",
      "Training Step: 845  | total loss: \u001b[1m\u001b[32m0.54617\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 845 | loss: 0.54617 - acc: 0.7544 -- iter: 594/594\n",
      "--\n",
      "Training Step: 846  | total loss: \u001b[1m\u001b[32m0.53630\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 846 | loss: 0.53630 - acc: 0.7626 -- iter: 594/594\n",
      "--\n",
      "Training Step: 847  | total loss: \u001b[1m\u001b[32m0.52717\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 847 | loss: 0.52717 - acc: 0.7700 -- iter: 594/594\n",
      "--\n",
      "Training Step: 848  | total loss: \u001b[1m\u001b[32m0.51823\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 848 | loss: 0.51823 - acc: 0.7772 -- iter: 594/594\n",
      "--\n",
      "Training Step: 849  | total loss: \u001b[1m\u001b[32m0.50964\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 849 | loss: 0.50964 - acc: 0.7842 -- iter: 594/594\n",
      "--\n",
      "Training Step: 850  | total loss: \u001b[1m\u001b[32m0.50195\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 850 | loss: 0.50195 - acc: 0.7908 -- iter: 594/594\n",
      "--\n",
      "Training Step: 851  | total loss: \u001b[1m\u001b[32m0.49420\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 851 | loss: 0.49420 - acc: 0.7964 -- iter: 594/594\n",
      "--\n",
      "Training Step: 852  | total loss: \u001b[1m\u001b[32m0.48683\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 852 | loss: 0.48683 - acc: 0.8007 -- iter: 594/594\n",
      "--\n",
      "Training Step: 853  | total loss: \u001b[1m\u001b[32m0.47936\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 853 | loss: 0.47936 - acc: 0.8053 -- iter: 594/594\n",
      "--\n",
      "Training Step: 854  | total loss: \u001b[1m\u001b[32m0.47211\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 854 | loss: 0.47211 - acc: 0.8093 -- iter: 594/594\n",
      "--\n",
      "Training Step: 855  | total loss: \u001b[1m\u001b[32m0.46534\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 855 | loss: 0.46534 - acc: 0.8131 -- iter: 594/594\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 856  | total loss: \u001b[1m\u001b[32m0.45869\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 856 | loss: 0.45869 - acc: 0.8166 -- iter: 594/594\n",
      "--\n",
      "Training Step: 857  | total loss: \u001b[1m\u001b[32m0.45338\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 857 | loss: 0.45338 - acc: 0.8203 -- iter: 594/594\n",
      "--\n",
      "Training Step: 858  | total loss: \u001b[1m\u001b[32m0.44795\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 858 | loss: 0.44795 - acc: 0.8233 -- iter: 594/594\n",
      "--\n",
      "Training Step: 859  | total loss: \u001b[1m\u001b[32m0.44272\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 859 | loss: 0.44272 - acc: 0.8263 -- iter: 594/594\n",
      "--\n",
      "Training Step: 860  | total loss: \u001b[1m\u001b[32m0.50795\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 860 | loss: 0.50795 - acc: 0.7969 -- iter: 594/594\n",
      "--\n",
      "Training Step: 861  | total loss: \u001b[1m\u001b[32m0.49762\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 861 | loss: 0.49762 - acc: 0.8022 -- iter: 594/594\n",
      "--\n",
      "Training Step: 862  | total loss: \u001b[1m\u001b[32m0.55834\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 862 | loss: 0.55834 - acc: 0.7718 -- iter: 594/594\n",
      "--\n",
      "Training Step: 863  | total loss: \u001b[1m\u001b[32m0.54247\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 863 | loss: 0.54247 - acc: 0.7795 -- iter: 594/594\n",
      "--\n",
      "Training Step: 864  | total loss: \u001b[1m\u001b[32m0.52825\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 864 | loss: 0.52825 - acc: 0.7862 -- iter: 594/594\n",
      "--\n",
      "Training Step: 865  | total loss: \u001b[1m\u001b[32m0.51546\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 865 | loss: 0.51546 - acc: 0.7926 -- iter: 594/594\n",
      "--\n",
      "Training Step: 866  | total loss: \u001b[1m\u001b[32m0.50430\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 866 | loss: 0.50430 - acc: 0.7980 -- iter: 594/594\n",
      "--\n",
      "Training Step: 867  | total loss: \u001b[1m\u001b[32m0.49447\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 867 | loss: 0.49447 - acc: 0.8031 -- iter: 594/594\n",
      "--\n",
      "Training Step: 868  | total loss: \u001b[1m\u001b[32m0.48565\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 868 | loss: 0.48565 - acc: 0.8075 -- iter: 594/594\n",
      "--\n",
      "Training Step: 869  | total loss: \u001b[1m\u001b[32m0.47796\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 869 | loss: 0.47796 - acc: 0.8110 -- iter: 594/594\n",
      "--\n",
      "Training Step: 870  | total loss: \u001b[1m\u001b[32m0.51934\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 870 | loss: 0.51934 - acc: 0.7833 -- iter: 594/594\n",
      "--\n",
      "Training Step: 871  | total loss: \u001b[1m\u001b[32m0.50895\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 871 | loss: 0.50895 - acc: 0.7897 -- iter: 594/594\n",
      "--\n",
      "Training Step: 872  | total loss: \u001b[1m\u001b[32m0.49941\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 872 | loss: 0.49941 - acc: 0.7957 -- iter: 594/594\n",
      "--\n",
      "Training Step: 873  | total loss: \u001b[1m\u001b[32m0.49025\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 873 | loss: 0.49025 - acc: 0.8007 -- iter: 594/594\n",
      "--\n",
      "Training Step: 874  | total loss: \u001b[1m\u001b[32m0.48270\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 874 | loss: 0.48270 - acc: 0.8051 -- iter: 594/594\n",
      "--\n",
      "Training Step: 875  | total loss: \u001b[1m\u001b[32m0.47613\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 875 | loss: 0.47613 - acc: 0.8091 -- iter: 594/594\n",
      "--\n",
      "Training Step: 876  | total loss: \u001b[1m\u001b[32m0.51466\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 876 | loss: 0.51466 - acc: 0.7846 -- iter: 594/594\n",
      "--\n",
      "Training Step: 877  | total loss: \u001b[1m\u001b[32m0.50463\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 877 | loss: 0.50463 - acc: 0.7906 -- iter: 594/594\n",
      "--\n",
      "Training Step: 878  | total loss: \u001b[1m\u001b[32m0.53965\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 878 | loss: 0.53965 - acc: 0.7658 -- iter: 594/594\n",
      "--\n",
      "Training Step: 879  | total loss: \u001b[1m\u001b[32m0.52803\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 879 | loss: 0.52803 - acc: 0.7734 -- iter: 594/594\n",
      "--\n",
      "Training Step: 880  | total loss: \u001b[1m\u001b[32m0.51786\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 880 | loss: 0.51786 - acc: 0.7807 -- iter: 594/594\n",
      "--\n",
      "Training Step: 881  | total loss: \u001b[1m\u001b[32m0.50857\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 881 | loss: 0.50857 - acc: 0.7877 -- iter: 594/594\n",
      "--\n",
      "Training Step: 882  | total loss: \u001b[1m\u001b[32m0.53735\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 882 | loss: 0.53735 - acc: 0.7650 -- iter: 594/594\n",
      "--\n",
      "Training Step: 883  | total loss: \u001b[1m\u001b[32m0.52668\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 883 | loss: 0.52668 - acc: 0.7726 -- iter: 594/594\n",
      "--\n",
      "Training Step: 884  | total loss: \u001b[1m\u001b[32m0.51756\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 884 | loss: 0.51756 - acc: 0.7801 -- iter: 594/594\n",
      "--\n",
      "Training Step: 885  | total loss: \u001b[1m\u001b[32m0.50950\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 885 | loss: 0.50950 - acc: 0.7862 -- iter: 594/594\n",
      "--\n",
      "Training Step: 886  | total loss: \u001b[1m\u001b[32m0.54012\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 886 | loss: 0.54012 - acc: 0.7623 -- iter: 594/594\n",
      "--\n",
      "Training Step: 887  | total loss: \u001b[1m\u001b[32m0.53009\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 887 | loss: 0.53009 - acc: 0.7708 -- iter: 594/594\n",
      "--\n",
      "Training Step: 888  | total loss: \u001b[1m\u001b[32m0.55906\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 888 | loss: 0.55906 - acc: 0.7477 -- iter: 594/594\n",
      "--\n",
      "Training Step: 889  | total loss: \u001b[1m\u001b[32m0.54723\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 889 | loss: 0.54723 - acc: 0.7573 -- iter: 594/594\n",
      "--\n",
      "Training Step: 890  | total loss: \u001b[1m\u001b[32m0.57239\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 890 | loss: 0.57239 - acc: 0.7353 -- iter: 594/594\n",
      "--\n",
      "Training Step: 891  | total loss: \u001b[1m\u001b[32m0.56048\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 891 | loss: 0.56048 - acc: 0.7454 -- iter: 594/594\n",
      "--\n",
      "Training Step: 892  | total loss: \u001b[1m\u001b[32m0.54949\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 892 | loss: 0.54949 - acc: 0.7549 -- iter: 594/594\n",
      "--\n",
      "Training Step: 893  | total loss: \u001b[1m\u001b[32m0.53956\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 893 | loss: 0.53956 - acc: 0.7631 -- iter: 594/594\n",
      "--\n",
      "Training Step: 894  | total loss: \u001b[1m\u001b[32m0.56426\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 894 | loss: 0.56426 - acc: 0.7400 -- iter: 594/594\n",
      "--\n",
      "Training Step: 895  | total loss: \u001b[1m\u001b[32m0.55331\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 895 | loss: 0.55331 - acc: 0.7500 -- iter: 594/594\n",
      "--\n",
      "Training Step: 896  | total loss: \u001b[1m\u001b[32m0.57558\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 896 | loss: 0.57558 - acc: 0.7302 -- iter: 594/594\n",
      "--\n",
      "Training Step: 897  | total loss: \u001b[1m\u001b[32m0.56402\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 897 | loss: 0.56402 - acc: 0.7402 -- iter: 594/594\n",
      "--\n",
      "Training Step: 898  | total loss: \u001b[1m\u001b[32m0.58287\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 898 | loss: 0.58287 - acc: 0.7219 -- iter: 594/594\n",
      "--\n",
      "Training Step: 899  | total loss: \u001b[1m\u001b[32m0.57074\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 899 | loss: 0.57074 - acc: 0.7332 -- iter: 594/594\n",
      "--\n",
      "Training Step: 900  | total loss: \u001b[1m\u001b[32m0.56005\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 900 | loss: 0.56005 - acc: 0.7439 -- iter: 594/594\n",
      "--\n",
      "Training Step: 901  | total loss: \u001b[1m\u001b[32m0.55039\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 901 | loss: 0.55039 - acc: 0.7530 -- iter: 594/594\n",
      "--\n",
      "Training Step: 902  | total loss: \u001b[1m\u001b[32m0.57045\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 902 | loss: 0.57045 - acc: 0.7326 -- iter: 594/594\n",
      "--\n",
      "Training Step: 903  | total loss: \u001b[1m\u001b[32m0.55979\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 903 | loss: 0.55979 - acc: 0.7425 -- iter: 594/594\n",
      "--\n",
      "Training Step: 904  | total loss: \u001b[1m\u001b[32m0.54990\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 904 | loss: 0.54990 - acc: 0.7526 -- iter: 594/594\n",
      "--\n",
      "Training Step: 905  | total loss: \u001b[1m\u001b[32m0.54119\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 905 | loss: 0.54119 - acc: 0.7613 -- iter: 594/594\n",
      "--\n",
      "Training Step: 906  | total loss: \u001b[1m\u001b[32m0.53275\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 906 | loss: 0.53275 - acc: 0.7690 -- iter: 594/594\n",
      "--\n",
      "Training Step: 907  | total loss: \u001b[1m\u001b[32m0.52492\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 907 | loss: 0.52492 - acc: 0.7756 -- iter: 594/594\n",
      "--\n",
      "Training Step: 908  | total loss: \u001b[1m\u001b[32m0.54622\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 908 | loss: 0.54622 - acc: 0.7553 -- iter: 594/594\n",
      "--\n",
      "Training Step: 909  | total loss: \u001b[1m\u001b[32m0.53634\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 909 | loss: 0.53634 - acc: 0.7640 -- iter: 594/594\n",
      "--\n",
      "Training Step: 910  | total loss: \u001b[1m\u001b[32m0.52703\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 910 | loss: 0.52703 - acc: 0.7717 -- iter: 594/594\n",
      "--\n",
      "Training Step: 911  | total loss: \u001b[1m\u001b[32m0.51837\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 911 | loss: 0.51837 - acc: 0.7787 -- iter: 594/594\n",
      "--\n",
      "Training Step: 912  | total loss: \u001b[1m\u001b[32m0.51032\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 912 | loss: 0.51032 - acc: 0.7854 -- iter: 594/594\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 913  | total loss: \u001b[1m\u001b[32m0.50227\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 913 | loss: 0.50227 - acc: 0.7917 -- iter: 594/594\n",
      "--\n",
      "Training Step: 914  | total loss: \u001b[1m\u001b[32m0.49485\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 914 | loss: 0.49485 - acc: 0.7969 -- iter: 594/594\n",
      "--\n",
      "Training Step: 915  | total loss: \u001b[1m\u001b[32m0.48782\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 915 | loss: 0.48782 - acc: 0.8017 -- iter: 594/594\n",
      "--\n",
      "Training Step: 916  | total loss: \u001b[1m\u001b[32m0.48086\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 916 | loss: 0.48086 - acc: 0.8060 -- iter: 594/594\n",
      "--\n",
      "Training Step: 917  | total loss: \u001b[1m\u001b[32m0.47369\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 917 | loss: 0.47369 - acc: 0.8101 -- iter: 594/594\n",
      "--\n",
      "Training Step: 918  | total loss: \u001b[1m\u001b[32m0.46684\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 918 | loss: 0.46684 - acc: 0.8138 -- iter: 594/594\n",
      "--\n",
      "Training Step: 919  | total loss: \u001b[1m\u001b[32m0.46077\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 919 | loss: 0.46077 - acc: 0.8174 -- iter: 594/594\n",
      "--\n",
      "Training Step: 920  | total loss: \u001b[1m\u001b[32m0.45547\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 920 | loss: 0.45547 - acc: 0.8200 -- iter: 594/594\n",
      "--\n",
      "Training Step: 921  | total loss: \u001b[1m\u001b[32m0.44985\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 921 | loss: 0.44985 - acc: 0.8225 -- iter: 594/594\n",
      "--\n",
      "Training Step: 922  | total loss: \u001b[1m\u001b[32m0.51169\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 922 | loss: 0.51169 - acc: 0.7913 -- iter: 594/594\n",
      "--\n",
      "Training Step: 923  | total loss: \u001b[1m\u001b[32m0.50014\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 923 | loss: 0.50014 - acc: 0.7963 -- iter: 594/594\n",
      "--\n",
      "Training Step: 924  | total loss: \u001b[1m\u001b[32m0.48978\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 924 | loss: 0.48978 - acc: 0.8010 -- iter: 594/594\n",
      "--\n",
      "Training Step: 925  | total loss: \u001b[1m\u001b[32m0.48125\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 925 | loss: 0.48125 - acc: 0.8058 -- iter: 594/594\n",
      "--\n",
      "Training Step: 926  | total loss: \u001b[1m\u001b[32m0.47344\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 926 | loss: 0.47344 - acc: 0.8099 -- iter: 594/594\n",
      "--\n",
      "Training Step: 927  | total loss: \u001b[1m\u001b[32m0.46673\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 927 | loss: 0.46673 - acc: 0.8136 -- iter: 594/594\n",
      "--\n",
      "Training Step: 928  | total loss: \u001b[1m\u001b[32m0.52527\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 928 | loss: 0.52527 - acc: 0.7826 -- iter: 594/594\n",
      "--\n",
      "Training Step: 929  | total loss: \u001b[1m\u001b[32m0.51298\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 929 | loss: 0.51298 - acc: 0.7885 -- iter: 594/594\n",
      "--\n",
      "Training Step: 930  | total loss: \u001b[1m\u001b[32m0.56242\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 930 | loss: 0.56242 - acc: 0.7606 -- iter: 594/594\n",
      "--\n",
      "Training Step: 931  | total loss: \u001b[1m\u001b[32m0.54668\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 931 | loss: 0.54668 - acc: 0.7693 -- iter: 594/594\n",
      "--\n",
      "Training Step: 932  | total loss: \u001b[1m\u001b[32m0.53257\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 932 | loss: 0.53257 - acc: 0.7765 -- iter: 594/594\n",
      "--\n",
      "Training Step: 933  | total loss: \u001b[1m\u001b[32m0.52069\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 933 | loss: 0.52069 - acc: 0.7840 -- iter: 594/594\n",
      "--\n",
      "Training Step: 934  | total loss: \u001b[1m\u001b[32m0.51000\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 934 | loss: 0.51000 - acc: 0.7901 -- iter: 594/594\n",
      "--\n",
      "Training Step: 935  | total loss: \u001b[1m\u001b[32m0.50095\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 935 | loss: 0.50095 - acc: 0.7960 -- iter: 594/594\n",
      "--\n",
      "Training Step: 936  | total loss: \u001b[1m\u001b[32m0.53890\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 936 | loss: 0.53890 - acc: 0.7676 -- iter: 594/594\n",
      "--\n",
      "Training Step: 937  | total loss: \u001b[1m\u001b[32m0.52749\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 937 | loss: 0.52749 - acc: 0.7758 -- iter: 594/594\n",
      "--\n",
      "Training Step: 938  | total loss: \u001b[1m\u001b[32m0.55913\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 938 | loss: 0.55913 - acc: 0.7511 -- iter: 594/594\n",
      "--\n",
      "Training Step: 939  | total loss: \u001b[1m\u001b[32m0.54720\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 939 | loss: 0.54720 - acc: 0.7603 -- iter: 594/594\n",
      "--\n",
      "Training Step: 940  | total loss: \u001b[1m\u001b[32m0.53662\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 940 | loss: 0.53662 - acc: 0.7686 -- iter: 594/594\n",
      "--\n",
      "Training Step: 941  | total loss: \u001b[1m\u001b[32m0.52711\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 941 | loss: 0.52711 - acc: 0.7768 -- iter: 594/594\n",
      "--\n",
      "Training Step: 942  | total loss: \u001b[1m\u001b[32m0.51882\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 942 | loss: 0.51882 - acc: 0.7838 -- iter: 594/594\n",
      "--\n",
      "Training Step: 943  | total loss: \u001b[1m\u001b[32m0.51153\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 943 | loss: 0.51153 - acc: 0.7893 -- iter: 594/594\n",
      "--\n",
      "Training Step: 944  | total loss: \u001b[1m\u001b[32m0.54156\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 944 | loss: 0.54156 - acc: 0.7629 -- iter: 594/594\n",
      "--\n",
      "Training Step: 945  | total loss: \u001b[1m\u001b[32m0.53186\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 945 | loss: 0.53186 - acc: 0.7709 -- iter: 594/594\n",
      "--\n",
      "Training Step: 946  | total loss: \u001b[1m\u001b[32m0.55478\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 946 | loss: 0.55478 - acc: 0.7495 -- iter: 594/594\n",
      "--\n",
      "Training Step: 947  | total loss: \u001b[1m\u001b[32m0.54397\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 947 | loss: 0.54397 - acc: 0.7578 -- iter: 594/594\n",
      "--\n",
      "Training Step: 948  | total loss: \u001b[1m\u001b[32m0.56805\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 948 | loss: 0.56805 - acc: 0.7380 -- iter: 594/594\n",
      "--\n",
      "Training Step: 949  | total loss: \u001b[1m\u001b[32m0.55669\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 949 | loss: 0.55669 - acc: 0.7482 -- iter: 594/594\n",
      "--\n",
      "Training Step: 950  | total loss: \u001b[1m\u001b[32m0.54657\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 950 | loss: 0.54657 - acc: 0.7574 -- iter: 594/594\n",
      "--\n",
      "Training Step: 951  | total loss: \u001b[1m\u001b[32m0.53744\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 951 | loss: 0.53744 - acc: 0.7650 -- iter: 594/594\n",
      "--\n",
      "Training Step: 952  | total loss: \u001b[1m\u001b[32m0.52906\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 952 | loss: 0.52906 - acc: 0.7729 -- iter: 594/594\n",
      "--\n",
      "Training Step: 953  | total loss: \u001b[1m\u001b[32m0.52127\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 953 | loss: 0.52127 - acc: 0.7801 -- iter: 594/594\n",
      "--\n",
      "Training Step: 954  | total loss: \u001b[1m\u001b[32m0.54928\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 954 | loss: 0.54928 - acc: 0.7538 -- iter: 594/594\n",
      "--\n",
      "Training Step: 955  | total loss: \u001b[1m\u001b[32m0.53961\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 955 | loss: 0.53961 - acc: 0.7622 -- iter: 594/594\n",
      "--\n",
      "Training Step: 956  | total loss: \u001b[1m\u001b[32m0.53007\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 956 | loss: 0.53007 - acc: 0.7705 -- iter: 594/594\n",
      "--\n",
      "Training Step: 957  | total loss: \u001b[1m\u001b[32m0.52179\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 957 | loss: 0.52179 - acc: 0.7768 -- iter: 594/594\n",
      "--\n",
      "Training Step: 958  | total loss: \u001b[1m\u001b[32m0.51369\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 958 | loss: 0.51369 - acc: 0.7830 -- iter: 594/594\n",
      "--\n",
      "Training Step: 959  | total loss: \u001b[1m\u001b[32m0.50570\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 959 | loss: 0.50570 - acc: 0.7895 -- iter: 594/594\n",
      "--\n",
      "Training Step: 960  | total loss: \u001b[1m\u001b[32m0.53866\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 960 | loss: 0.53866 - acc: 0.7639 -- iter: 594/594\n",
      "--\n",
      "Training Step: 961  | total loss: \u001b[1m\u001b[32m0.52829\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 961 | loss: 0.52829 - acc: 0.7719 -- iter: 594/594\n",
      "--\n",
      "Training Step: 962  | total loss: \u001b[1m\u001b[32m0.51863\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 962 | loss: 0.51863 - acc: 0.7790 -- iter: 594/594\n",
      "--\n",
      "Training Step: 963  | total loss: \u001b[1m\u001b[32m0.50937\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 963 | loss: 0.50937 - acc: 0.7853 -- iter: 594/594\n",
      "--\n",
      "Training Step: 964  | total loss: \u001b[1m\u001b[32m0.50108\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 964 | loss: 0.50108 - acc: 0.7915 -- iter: 594/594\n",
      "--\n",
      "Training Step: 965  | total loss: \u001b[1m\u001b[32m0.49332\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 965 | loss: 0.49332 - acc: 0.7968 -- iter: 594/594\n",
      "--\n",
      "Training Step: 966  | total loss: \u001b[1m\u001b[32m0.48532\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 966 | loss: 0.48532 - acc: 0.8022 -- iter: 594/594\n",
      "--\n",
      "Training Step: 967  | total loss: \u001b[1m\u001b[32m0.47839\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 967 | loss: 0.47839 - acc: 0.8061 -- iter: 594/594\n",
      "--\n",
      "Training Step: 968  | total loss: \u001b[1m\u001b[32m0.47143\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 968 | loss: 0.47143 - acc: 0.8104 -- iter: 594/594\n",
      "--\n",
      "Training Step: 969  | total loss: \u001b[1m\u001b[32m0.46483\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 969 | loss: 0.46483 - acc: 0.8142 -- iter: 594/594\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 970  | total loss: \u001b[1m\u001b[32m0.45939\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 970 | loss: 0.45939 - acc: 0.8178 -- iter: 594/594\n",
      "--\n",
      "Training Step: 971  | total loss: \u001b[1m\u001b[32m0.45374\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 971 | loss: 0.45374 - acc: 0.8205 -- iter: 594/594\n",
      "--\n",
      "Training Step: 972  | total loss: \u001b[1m\u001b[32m0.44900\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 972 | loss: 0.44900 - acc: 0.8230 -- iter: 594/594\n",
      "--\n",
      "Training Step: 973  | total loss: \u001b[1m\u001b[32m0.44336\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 973 | loss: 0.44336 - acc: 0.8257 -- iter: 594/594\n",
      "--\n",
      "Training Step: 974  | total loss: \u001b[1m\u001b[32m0.43852\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 974 | loss: 0.43852 - acc: 0.8275 -- iter: 594/594\n",
      "--\n",
      "Training Step: 975  | total loss: \u001b[1m\u001b[32m0.43501\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 975 | loss: 0.43501 - acc: 0.8289 -- iter: 594/594\n",
      "--\n",
      "Training Step: 976  | total loss: \u001b[1m\u001b[32m0.43120\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 976 | loss: 0.43120 - acc: 0.8307 -- iter: 594/594\n",
      "--\n",
      "Training Step: 977  | total loss: \u001b[1m\u001b[32m0.42846\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 977 | loss: 0.42846 - acc: 0.8328 -- iter: 594/594\n",
      "--\n",
      "Training Step: 978  | total loss: \u001b[1m\u001b[32m0.42532\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 978 | loss: 0.42532 - acc: 0.8344 -- iter: 594/594\n",
      "--\n",
      "Training Step: 979  | total loss: \u001b[1m\u001b[32m0.42271\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 979 | loss: 0.42271 - acc: 0.8356 -- iter: 594/594\n",
      "--\n",
      "Training Step: 980  | total loss: \u001b[1m\u001b[32m0.49060\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 980 | loss: 0.49060 - acc: 0.8078 -- iter: 594/594\n",
      "--\n",
      "Training Step: 981  | total loss: \u001b[1m\u001b[32m0.48210\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 981 | loss: 0.48210 - acc: 0.8120 -- iter: 594/594\n",
      "--\n",
      "Training Step: 982  | total loss: \u001b[1m\u001b[32m0.54617\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 982 | loss: 0.54617 - acc: 0.7833 -- iter: 594/594\n",
      "--\n",
      "Training Step: 983  | total loss: \u001b[1m\u001b[32m0.53152\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 983 | loss: 0.53152 - acc: 0.7900 -- iter: 594/594\n",
      "--\n",
      "Training Step: 984  | total loss: \u001b[1m\u001b[32m0.57889\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 984 | loss: 0.57889 - acc: 0.7656 -- iter: 594/594\n",
      "--\n",
      "Training Step: 985  | total loss: \u001b[1m\u001b[32m0.56140\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 985 | loss: 0.56140 - acc: 0.7735 -- iter: 594/594\n",
      "--\n",
      "Training Step: 986  | total loss: \u001b[1m\u001b[32m0.54627\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 986 | loss: 0.54627 - acc: 0.7807 -- iter: 594/594\n",
      "--\n",
      "Training Step: 987  | total loss: \u001b[1m\u001b[32m0.53325\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 987 | loss: 0.53325 - acc: 0.7871 -- iter: 594/594\n",
      "--\n",
      "Training Step: 988  | total loss: \u001b[1m\u001b[32m0.52257\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 988 | loss: 0.52257 - acc: 0.7929 -- iter: 594/594\n",
      "--\n",
      "Training Step: 989  | total loss: \u001b[1m\u001b[32m0.51264\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 989 | loss: 0.51264 - acc: 0.7983 -- iter: 594/594\n",
      "--\n",
      "Training Step: 990  | total loss: \u001b[1m\u001b[32m0.54613\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 990 | loss: 0.54613 - acc: 0.7686 -- iter: 594/594\n",
      "--\n",
      "Training Step: 991  | total loss: \u001b[1m\u001b[32m0.53539\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 991 | loss: 0.53539 - acc: 0.7761 -- iter: 594/594\n",
      "--\n",
      "Training Step: 992  | total loss: \u001b[1m\u001b[32m0.52596\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 992 | loss: 0.52596 - acc: 0.7820 -- iter: 594/594\n",
      "--\n",
      "Training Step: 993  | total loss: \u001b[1m\u001b[32m0.51796\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 993 | loss: 0.51796 - acc: 0.7882 -- iter: 594/594\n",
      "--\n",
      "Training Step: 994  | total loss: \u001b[1m\u001b[32m0.54777\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 994 | loss: 0.54777 - acc: 0.7624 -- iter: 594/594\n",
      "--\n",
      "Training Step: 995  | total loss: \u001b[1m\u001b[32m0.53800\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 995 | loss: 0.53800 - acc: 0.7698 -- iter: 594/594\n",
      "--\n",
      "Training Step: 996  | total loss: \u001b[1m\u001b[32m0.56366\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 996 | loss: 0.56366 - acc: 0.7459 -- iter: 594/594\n",
      "--\n",
      "Training Step: 997  | total loss: \u001b[1m\u001b[32m0.55321\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 997 | loss: 0.55321 - acc: 0.7549 -- iter: 594/594\n",
      "--\n",
      "Training Step: 998  | total loss: \u001b[1m\u001b[32m0.57314\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 998 | loss: 0.57314 - acc: 0.7347 -- iter: 594/594\n",
      "--\n",
      "Training Step: 999  | total loss: \u001b[1m\u001b[32m0.56253\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 999 | loss: 0.56253 - acc: 0.7447 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1000  | total loss: \u001b[1m\u001b[32m0.58339\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1000 | loss: 0.58339 - acc: 0.7236 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1001  | total loss: \u001b[1m\u001b[32m0.57260\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1001 | loss: 0.57260 - acc: 0.7351 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1002  | total loss: \u001b[1m\u001b[32m0.56317\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1002 | loss: 0.56317 - acc: 0.7437 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1003  | total loss: \u001b[1m\u001b[32m0.55500\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1003 | loss: 0.55500 - acc: 0.7510 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1004  | total loss: \u001b[1m\u001b[32m0.54734\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1004 | loss: 0.54734 - acc: 0.7589 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1005  | total loss: \u001b[1m\u001b[32m0.54037\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1005 | loss: 0.54037 - acc: 0.7660 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1006  | total loss: \u001b[1m\u001b[32m0.56368\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1006 | loss: 0.56368 - acc: 0.7424 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1007  | total loss: \u001b[1m\u001b[32m0.55458\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1007 | loss: 0.55458 - acc: 0.7520 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1008  | total loss: \u001b[1m\u001b[32m0.57368\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1008 | loss: 0.57368 - acc: 0.7320 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1009  | total loss: \u001b[1m\u001b[32m0.56346\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1009 | loss: 0.56346 - acc: 0.7418 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1010  | total loss: \u001b[1m\u001b[32m0.55370\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1010 | loss: 0.55370 - acc: 0.7517 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1011  | total loss: \u001b[1m\u001b[32m0.54547\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1011 | loss: 0.54547 - acc: 0.7590 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1012  | total loss: \u001b[1m\u001b[32m0.56489\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1012 | loss: 0.56489 - acc: 0.7397 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1013  | total loss: \u001b[1m\u001b[32m0.55455\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1013 | loss: 0.55455 - acc: 0.7495 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1014  | total loss: \u001b[1m\u001b[32m0.54532\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1014 | loss: 0.54532 - acc: 0.7577 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1015  | total loss: \u001b[1m\u001b[32m0.53600\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1015 | loss: 0.53600 - acc: 0.7658 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1016  | total loss: \u001b[1m\u001b[32m0.55926\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1016 | loss: 0.55926 - acc: 0.7441 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1017  | total loss: \u001b[1m\u001b[32m0.54840\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1017 | loss: 0.54840 - acc: 0.7537 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1018  | total loss: \u001b[1m\u001b[32m0.53805\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1018 | loss: 0.53805 - acc: 0.7633 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1019  | total loss: \u001b[1m\u001b[32m0.52911\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1019 | loss: 0.52911 - acc: 0.7719 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1020  | total loss: \u001b[1m\u001b[32m0.55898\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1020 | loss: 0.55898 - acc: 0.7450 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1021  | total loss: \u001b[1m\u001b[32m0.54662\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1021 | loss: 0.54662 - acc: 0.7550 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1022  | total loss: \u001b[1m\u001b[32m0.53523\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1022 | loss: 0.53523 - acc: 0.7642 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1023  | total loss: \u001b[1m\u001b[32m0.52474\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1023 | loss: 0.52474 - acc: 0.7723 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1024  | total loss: \u001b[1m\u001b[32m0.55774\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1024 | loss: 0.55774 - acc: 0.7461 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1025  | total loss: \u001b[1m\u001b[32m0.54523\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1025 | loss: 0.54523 - acc: 0.7553 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1026  | total loss: \u001b[1m\u001b[32m0.57507\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1026 | loss: 0.57507 - acc: 0.7338 -- iter: 594/594\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1027  | total loss: \u001b[1m\u001b[32m0.56016\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1027 | loss: 0.56016 - acc: 0.7446 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1028  | total loss: \u001b[1m\u001b[32m0.58441\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1028 | loss: 0.58441 - acc: 0.7262 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1029  | total loss: \u001b[1m\u001b[32m0.56945\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1029 | loss: 0.56945 - acc: 0.7379 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1030  | total loss: \u001b[1m\u001b[32m0.55588\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1030 | loss: 0.55588 - acc: 0.7483 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1031  | total loss: \u001b[1m\u001b[32m0.54428\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1031 | loss: 0.54428 - acc: 0.7583 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1032  | total loss: \u001b[1m\u001b[32m0.53324\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1032 | loss: 0.53324 - acc: 0.7663 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1033  | total loss: \u001b[1m\u001b[32m0.52330\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1033 | loss: 0.52330 - acc: 0.7746 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1034  | total loss: \u001b[1m\u001b[32m0.55517\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1034 | loss: 0.55517 - acc: 0.7500 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1035  | total loss: \u001b[1m\u001b[32m0.54259\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1035 | loss: 0.54259 - acc: 0.7593 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1036  | total loss: \u001b[1m\u001b[32m0.53134\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1036 | loss: 0.53134 - acc: 0.7675 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1037  | total loss: \u001b[1m\u001b[32m0.52095\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1037 | loss: 0.52095 - acc: 0.7746 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1038  | total loss: \u001b[1m\u001b[32m0.51162\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1038 | loss: 0.51162 - acc: 0.7817 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1039  | total loss: \u001b[1m\u001b[32m0.50239\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1039 | loss: 0.50239 - acc: 0.7882 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1040  | total loss: \u001b[1m\u001b[32m0.49510\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1040 | loss: 0.49510 - acc: 0.7942 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1041  | total loss: \u001b[1m\u001b[32m0.48763\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1041 | loss: 0.48763 - acc: 0.7998 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1042  | total loss: \u001b[1m\u001b[32m0.48042\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1042 | loss: 0.48042 - acc: 0.8049 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1043  | total loss: \u001b[1m\u001b[32m0.47373\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1043 | loss: 0.47373 - acc: 0.8087 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1044  | total loss: \u001b[1m\u001b[32m0.51624\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1044 | loss: 0.51624 - acc: 0.7819 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1045  | total loss: \u001b[1m\u001b[32m0.50554\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1045 | loss: 0.50554 - acc: 0.7880 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1046  | total loss: \u001b[1m\u001b[32m0.49592\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1046 | loss: 0.49592 - acc: 0.7939 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1047  | total loss: \u001b[1m\u001b[32m0.48705\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1047 | loss: 0.48705 - acc: 0.7990 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1048  | total loss: \u001b[1m\u001b[32m0.47957\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1048 | loss: 0.47957 - acc: 0.8040 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1049  | total loss: \u001b[1m\u001b[32m0.47210\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1049 | loss: 0.47210 - acc: 0.8083 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1050  | total loss: \u001b[1m\u001b[32m0.46550\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1050 | loss: 0.46550 - acc: 0.8119 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1051  | total loss: \u001b[1m\u001b[32m0.45958\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1051 | loss: 0.45958 - acc: 0.8154 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1052  | total loss: \u001b[1m\u001b[32m0.51284\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1052 | loss: 0.51284 - acc: 0.7854 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1053  | total loss: \u001b[1m\u001b[32m0.50188\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1053 | loss: 0.50188 - acc: 0.7919 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1054  | total loss: \u001b[1m\u001b[32m0.55369\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1054 | loss: 0.55369 - acc: 0.7639 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1055  | total loss: \u001b[1m\u001b[32m0.53871\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1055 | loss: 0.53871 - acc: 0.7722 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1056  | total loss: \u001b[1m\u001b[32m0.57594\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1056 | loss: 0.57594 - acc: 0.7481 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1057  | total loss: \u001b[1m\u001b[32m0.55953\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1057 | loss: 0.55953 - acc: 0.7580 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1058  | total loss: \u001b[1m\u001b[32m0.54553\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1058 | loss: 0.54553 - acc: 0.7662 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1059  | total loss: \u001b[1m\u001b[32m0.53347\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1059 | loss: 0.53347 - acc: 0.7741 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1060  | total loss: \u001b[1m\u001b[32m0.56080\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1060 | loss: 0.56080 - acc: 0.7516 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1061  | total loss: \u001b[1m\u001b[32m0.54825\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1061 | loss: 0.54825 - acc: 0.7611 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1062  | total loss: \u001b[1m\u001b[32m0.53738\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1062 | loss: 0.53738 - acc: 0.7692 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1063  | total loss: \u001b[1m\u001b[32m0.52718\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1063 | loss: 0.52718 - acc: 0.7773 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1064  | total loss: \u001b[1m\u001b[32m0.55473\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1064 | loss: 0.55473 - acc: 0.7532 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1065  | total loss: \u001b[1m\u001b[32m0.54375\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1065 | loss: 0.54375 - acc: 0.7626 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1066  | total loss: \u001b[1m\u001b[32m0.53469\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1066 | loss: 0.53469 - acc: 0.7700 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1067  | total loss: \u001b[1m\u001b[32m0.52559\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1067 | loss: 0.52559 - acc: 0.7770 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1068  | total loss: \u001b[1m\u001b[32m0.55040\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1068 | loss: 0.55040 - acc: 0.7539 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1069  | total loss: \u001b[1m\u001b[32m0.54017\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1069 | loss: 0.54017 - acc: 0.7632 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1070  | total loss: \u001b[1m\u001b[32m0.53107\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1070 | loss: 0.53107 - acc: 0.7708 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1071  | total loss: \u001b[1m\u001b[32m0.52294\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1071 | loss: 0.52294 - acc: 0.7773 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1072  | total loss: \u001b[1m\u001b[32m0.54766\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1072 | loss: 0.54766 - acc: 0.7541 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1073  | total loss: \u001b[1m\u001b[32m0.53717\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1073 | loss: 0.53717 - acc: 0.7620 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1074  | total loss: \u001b[1m\u001b[32m0.56203\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1074 | loss: 0.56203 - acc: 0.7414 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1075  | total loss: \u001b[1m\u001b[32m0.55075\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1075 | loss: 0.55075 - acc: 0.7517 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1076  | total loss: \u001b[1m\u001b[32m0.57398\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1076 | loss: 0.57398 - acc: 0.7293 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1077  | total loss: \u001b[1m\u001b[32m0.56131\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1077 | loss: 0.56131 - acc: 0.7400 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1078  | total loss: \u001b[1m\u001b[32m0.58102\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1078 | loss: 0.58102 - acc: 0.7211 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1079  | total loss: \u001b[1m\u001b[32m0.56850\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1079 | loss: 0.56850 - acc: 0.7326 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1080  | total loss: \u001b[1m\u001b[32m0.55719\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1080 | loss: 0.55719 - acc: 0.7432 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1081  | total loss: \u001b[1m\u001b[32m0.54704\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1081 | loss: 0.54704 - acc: 0.7525 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1082  | total loss: \u001b[1m\u001b[32m0.57150\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1082 | loss: 0.57150 - acc: 0.7308 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1083  | total loss: \u001b[1m\u001b[32m0.55998\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1083 | loss: 0.55998 - acc: 0.7412 -- iter: 594/594\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1084  | total loss: \u001b[1m\u001b[32m0.54964\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1084 | loss: 0.54964 - acc: 0.7516 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1085  | total loss: \u001b[1m\u001b[32m0.54001\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1085 | loss: 0.54001 - acc: 0.7606 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1086  | total loss: \u001b[1m\u001b[32m0.56600\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1086 | loss: 0.56600 - acc: 0.7371 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1087  | total loss: \u001b[1m\u001b[32m0.55468\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1087 | loss: 0.55468 - acc: 0.7477 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1088  | total loss: \u001b[1m\u001b[32m0.57547\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1088 | loss: 0.57547 - acc: 0.7263 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1089  | total loss: \u001b[1m\u001b[32m0.56328\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1089 | loss: 0.56328 - acc: 0.7375 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1090  | total loss: \u001b[1m\u001b[32m0.58346\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1090 | loss: 0.58346 - acc: 0.7168 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1091  | total loss: \u001b[1m\u001b[32m0.57089\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1091 | loss: 0.57089 - acc: 0.7293 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1092  | total loss: \u001b[1m\u001b[32m0.59229\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1092 | loss: 0.59229 - acc: 0.7091 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1093  | total loss: \u001b[1m\u001b[32m0.57847\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1093 | loss: 0.57847 - acc: 0.7222 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1094  | total loss: \u001b[1m\u001b[32m0.56655\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1094 | loss: 0.56655 - acc: 0.7340 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1095  | total loss: \u001b[1m\u001b[32m0.55577\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1095 | loss: 0.55577 - acc: 0.7447 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1096  | total loss: \u001b[1m\u001b[32m0.54574\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1096 | loss: 0.54574 - acc: 0.7544 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1097  | total loss: \u001b[1m\u001b[32m0.53676\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1097 | loss: 0.53676 - acc: 0.7632 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1098  | total loss: \u001b[1m\u001b[32m0.52790\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1098 | loss: 0.52790 - acc: 0.7719 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1099  | total loss: \u001b[1m\u001b[32m0.52004\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1099 | loss: 0.52004 - acc: 0.7784 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1100  | total loss: \u001b[1m\u001b[32m0.51219\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1100 | loss: 0.51219 - acc: 0.7850 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1101  | total loss: \u001b[1m\u001b[32m0.50438\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1101 | loss: 0.50438 - acc: 0.7902 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1102  | total loss: \u001b[1m\u001b[32m0.53810\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1102 | loss: 0.53810 - acc: 0.7632 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1103  | total loss: \u001b[1m\u001b[32m0.52724\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1103 | loss: 0.52724 - acc: 0.7719 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1104  | total loss: \u001b[1m\u001b[32m0.51694\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1104 | loss: 0.51694 - acc: 0.7790 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1105  | total loss: \u001b[1m\u001b[32m0.50799\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1105 | loss: 0.50799 - acc: 0.7857 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1106  | total loss: \u001b[1m\u001b[32m0.49919\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1106 | loss: 0.49919 - acc: 0.7909 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1107  | total loss: \u001b[1m\u001b[32m0.49120\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1107 | loss: 0.49120 - acc: 0.7963 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1108  | total loss: \u001b[1m\u001b[32m0.48289\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1108 | loss: 0.48289 - acc: 0.8007 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1109  | total loss: \u001b[1m\u001b[32m0.47567\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1109 | loss: 0.47567 - acc: 0.8048 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1110  | total loss: \u001b[1m\u001b[32m0.51876\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1110 | loss: 0.51876 - acc: 0.7779 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1111  | total loss: \u001b[1m\u001b[32m0.50726\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1111 | loss: 0.50726 - acc: 0.7848 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1112  | total loss: \u001b[1m\u001b[32m0.49744\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1112 | loss: 0.49744 - acc: 0.7910 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1113  | total loss: \u001b[1m\u001b[32m0.48799\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1113 | loss: 0.48799 - acc: 0.7967 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1114  | total loss: \u001b[1m\u001b[32m0.47975\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1114 | loss: 0.47975 - acc: 0.8019 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1115  | total loss: \u001b[1m\u001b[32m0.47249\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1115 | loss: 0.47249 - acc: 0.8062 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1116  | total loss: \u001b[1m\u001b[32m0.52361\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1116 | loss: 0.52361 - acc: 0.7785 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1117  | total loss: \u001b[1m\u001b[32m0.51188\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1117 | loss: 0.51188 - acc: 0.7853 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1118  | total loss: \u001b[1m\u001b[32m0.54501\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1118 | loss: 0.54501 - acc: 0.7665 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1119  | total loss: \u001b[1m\u001b[32m0.53146\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1119 | loss: 0.53146 - acc: 0.7744 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1120  | total loss: \u001b[1m\u001b[32m0.57070\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1120 | loss: 0.57070 - acc: 0.7491 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1121  | total loss: \u001b[1m\u001b[32m0.55490\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1121 | loss: 0.55490 - acc: 0.7589 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1122  | total loss: \u001b[1m\u001b[32m0.54121\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1122 | loss: 0.54121 - acc: 0.7677 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1123  | total loss: \u001b[1m\u001b[32m0.52954\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1123 | loss: 0.52954 - acc: 0.7756 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1124  | total loss: \u001b[1m\u001b[32m0.56093\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1124 | loss: 0.56093 - acc: 0.7502 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1125  | total loss: \u001b[1m\u001b[32m0.54763\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1125 | loss: 0.54763 - acc: 0.7590 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1126  | total loss: \u001b[1m\u001b[32m0.53614\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1126 | loss: 0.53614 - acc: 0.7675 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1127  | total loss: \u001b[1m\u001b[32m0.52655\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1127 | loss: 0.52655 - acc: 0.7749 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1128  | total loss: \u001b[1m\u001b[32m0.55613\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1128 | loss: 0.55613 - acc: 0.7501 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1129  | total loss: \u001b[1m\u001b[32m0.54477\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1129 | loss: 0.54477 - acc: 0.7591 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1130  | total loss: \u001b[1m\u001b[32m0.57365\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1130 | loss: 0.57365 - acc: 0.7337 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1131  | total loss: \u001b[1m\u001b[32m0.56138\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1131 | loss: 0.56138 - acc: 0.7445 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1132  | total loss: \u001b[1m\u001b[32m0.55065\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1132 | loss: 0.55065 - acc: 0.7544 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1133  | total loss: \u001b[1m\u001b[32m0.54115\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1133 | loss: 0.54115 - acc: 0.7625 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1134  | total loss: \u001b[1m\u001b[32m0.53262\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1134 | loss: 0.53262 - acc: 0.7702 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1135  | total loss: \u001b[1m\u001b[32m0.52496\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1135 | loss: 0.52496 - acc: 0.7767 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1136  | total loss: \u001b[1m\u001b[32m0.51791\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1136 | loss: 0.51791 - acc: 0.7830 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1137  | total loss: \u001b[1m\u001b[32m0.51127\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1137 | loss: 0.51127 - acc: 0.7894 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1138  | total loss: \u001b[1m\u001b[32m0.50497\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1138 | loss: 0.50497 - acc: 0.7941 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1139  | total loss: \u001b[1m\u001b[32m0.49848\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1139 | loss: 0.49848 - acc: 0.7987 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1140  | total loss: \u001b[1m\u001b[32m0.49250\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1140 | loss: 0.49250 - acc: 0.8024 -- iter: 594/594\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1141  | total loss: \u001b[1m\u001b[32m0.48620\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1141 | loss: 0.48620 - acc: 0.8061 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1142  | total loss: \u001b[1m\u001b[32m0.51507\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1142 | loss: 0.51507 - acc: 0.7828 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1143  | total loss: \u001b[1m\u001b[32m0.50601\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1143 | loss: 0.50601 - acc: 0.7887 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1144  | total loss: \u001b[1m\u001b[32m0.49778\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1144 | loss: 0.49778 - acc: 0.7940 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1145  | total loss: \u001b[1m\u001b[32m0.48955\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1145 | loss: 0.48955 - acc: 0.7998 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1146  | total loss: \u001b[1m\u001b[32m0.48180\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1146 | loss: 0.48180 - acc: 0.8048 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1147  | total loss: \u001b[1m\u001b[32m0.47472\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1147 | loss: 0.47472 - acc: 0.8087 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1148  | total loss: \u001b[1m\u001b[32m0.46849\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1148 | loss: 0.46849 - acc: 0.8123 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1149  | total loss: \u001b[1m\u001b[32m0.46203\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1149 | loss: 0.46203 - acc: 0.8158 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1150  | total loss: \u001b[1m\u001b[32m0.45680\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1150 | loss: 0.45680 - acc: 0.8182 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1151  | total loss: \u001b[1m\u001b[32m0.45121\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1151 | loss: 0.45121 - acc: 0.8205 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1152  | total loss: \u001b[1m\u001b[32m0.44535\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1152 | loss: 0.44535 - acc: 0.8235 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1153  | total loss: \u001b[1m\u001b[32m0.44067\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1153 | loss: 0.44067 - acc: 0.8255 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1154  | total loss: \u001b[1m\u001b[32m0.43617\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1154 | loss: 0.43617 - acc: 0.8278 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1155  | total loss: \u001b[1m\u001b[32m0.43237\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1155 | loss: 0.43237 - acc: 0.8297 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1156  | total loss: \u001b[1m\u001b[32m0.50257\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1156 | loss: 0.50257 - acc: 0.7969 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1157  | total loss: \u001b[1m\u001b[32m0.49222\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1157 | loss: 0.49222 - acc: 0.8026 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1158  | total loss: \u001b[1m\u001b[32m0.48288\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1158 | loss: 0.48288 - acc: 0.8073 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1159  | total loss: \u001b[1m\u001b[32m0.47480\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1159 | loss: 0.47480 - acc: 0.8113 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1160  | total loss: \u001b[1m\u001b[32m0.46701\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1160 | loss: 0.46701 - acc: 0.8157 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1161  | total loss: \u001b[1m\u001b[32m0.46090\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1161 | loss: 0.46090 - acc: 0.8188 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1162  | total loss: \u001b[1m\u001b[32m0.52015\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1162 | loss: 0.52015 - acc: 0.7891 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1163  | total loss: \u001b[1m\u001b[32m0.50794\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1163 | loss: 0.50794 - acc: 0.7947 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1164  | total loss: \u001b[1m\u001b[32m0.55423\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1164 | loss: 0.55423 - acc: 0.7681 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1165  | total loss: \u001b[1m\u001b[32m0.53947\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1165 | loss: 0.53947 - acc: 0.7755 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1166  | total loss: \u001b[1m\u001b[32m0.57359\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1166 | loss: 0.57359 - acc: 0.7504 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1167  | total loss: \u001b[1m\u001b[32m0.55791\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1167 | loss: 0.55791 - acc: 0.7606 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1168  | total loss: \u001b[1m\u001b[32m0.58666\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1168 | loss: 0.58666 - acc: 0.7364 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1169  | total loss: \u001b[1m\u001b[32m0.57196\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1169 | loss: 0.57196 - acc: 0.7474 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1170  | total loss: \u001b[1m\u001b[32m0.55949\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1170 | loss: 0.55949 - acc: 0.7572 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1171  | total loss: \u001b[1m\u001b[32m0.54856\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1171 | loss: 0.54856 - acc: 0.7653 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1172  | total loss: \u001b[1m\u001b[32m0.56750\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1172 | loss: 0.56750 - acc: 0.7448 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1173  | total loss: \u001b[1m\u001b[32m0.55709\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1173 | loss: 0.55709 - acc: 0.7544 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1174  | total loss: \u001b[1m\u001b[32m0.57317\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1174 | loss: 0.57317 - acc: 0.7363 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1175  | total loss: \u001b[1m\u001b[32m0.56308\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1175 | loss: 0.56308 - acc: 0.7457 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1176  | total loss: \u001b[1m\u001b[32m0.55435\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1176 | loss: 0.55435 - acc: 0.7543 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1177  | total loss: \u001b[1m\u001b[32m0.54688\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1177 | loss: 0.54688 - acc: 0.7614 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1178  | total loss: \u001b[1m\u001b[32m0.57028\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1178 | loss: 0.57028 - acc: 0.7349 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1179  | total loss: \u001b[1m\u001b[32m0.56095\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1179 | loss: 0.56095 - acc: 0.7447 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1180  | total loss: \u001b[1m\u001b[32m0.57711\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1180 | loss: 0.57711 - acc: 0.7231 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1181  | total loss: \u001b[1m\u001b[32m0.56869\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1181 | loss: 0.56869 - acc: 0.7331 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1182  | total loss: \u001b[1m\u001b[32m0.56055\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1182 | loss: 0.56055 - acc: 0.7428 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1183  | total loss: \u001b[1m\u001b[32m0.55307\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1183 | loss: 0.55307 - acc: 0.7517 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1184  | total loss: \u001b[1m\u001b[32m0.54622\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1184 | loss: 0.54622 - acc: 0.7582 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1185  | total loss: \u001b[1m\u001b[32m0.53923\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1185 | loss: 0.53923 - acc: 0.7660 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1186  | total loss: \u001b[1m\u001b[32m0.55684\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1186 | loss: 0.55684 - acc: 0.7470 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1187  | total loss: \u001b[1m\u001b[32m0.54845\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1187 | loss: 0.54845 - acc: 0.7555 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1188  | total loss: \u001b[1m\u001b[32m0.56933\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1188 | loss: 0.56933 - acc: 0.7346 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1189  | total loss: \u001b[1m\u001b[32m0.55974\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1189 | loss: 0.55974 - acc: 0.7437 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1190  | total loss: \u001b[1m\u001b[32m0.55044\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1190 | loss: 0.55044 - acc: 0.7521 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1191  | total loss: \u001b[1m\u001b[32m0.54194\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1191 | loss: 0.54194 - acc: 0.7606 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1192  | total loss: \u001b[1m\u001b[32m0.56232\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1192 | loss: 0.56232 - acc: 0.7409 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1193  | total loss: \u001b[1m\u001b[32m0.55165\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1193 | loss: 0.55165 - acc: 0.7503 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1194  | total loss: \u001b[1m\u001b[32m0.57544\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1194 | loss: 0.57544 - acc: 0.7278 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1195  | total loss: \u001b[1m\u001b[32m0.56323\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1195 | loss: 0.56323 - acc: 0.7387 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1196  | total loss: \u001b[1m\u001b[32m0.58553\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1196 | loss: 0.58553 - acc: 0.7182 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1197  | total loss: \u001b[1m\u001b[32m0.57197\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1197 | loss: 0.57197 - acc: 0.7306 -- iter: 594/594\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1198  | total loss: \u001b[1m\u001b[32m0.59396\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1198 | loss: 0.59396 - acc: 0.7085 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1199  | total loss: \u001b[1m\u001b[32m0.57956\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1199 | loss: 0.57956 - acc: 0.7222 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1200  | total loss: \u001b[1m\u001b[32m0.56639\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1200 | loss: 0.56639 - acc: 0.7345 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1201  | total loss: \u001b[1m\u001b[32m0.55494\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1201 | loss: 0.55494 - acc: 0.7450 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1202  | total loss: \u001b[1m\u001b[32m0.54418\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1202 | loss: 0.54418 - acc: 0.7540 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1203  | total loss: \u001b[1m\u001b[32m0.53455\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1203 | loss: 0.53455 - acc: 0.7626 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1204  | total loss: \u001b[1m\u001b[32m0.52541\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1204 | loss: 0.52541 - acc: 0.7705 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1205  | total loss: \u001b[1m\u001b[32m0.51696\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1205 | loss: 0.51696 - acc: 0.7777 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1206  | total loss: \u001b[1m\u001b[32m0.50879\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1206 | loss: 0.50879 - acc: 0.7842 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1207  | total loss: \u001b[1m\u001b[32m0.50092\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1207 | loss: 0.50092 - acc: 0.7907 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1208  | total loss: \u001b[1m\u001b[32m0.49310\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1208 | loss: 0.49310 - acc: 0.7961 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1209  | total loss: \u001b[1m\u001b[32m0.48613\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1209 | loss: 0.48613 - acc: 0.8017 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1210  | total loss: \u001b[1m\u001b[32m0.47825\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1210 | loss: 0.47825 - acc: 0.8062 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1211  | total loss: \u001b[1m\u001b[32m0.47172\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1211 | loss: 0.47172 - acc: 0.8101 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1212  | total loss: \u001b[1m\u001b[32m0.46528\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1212 | loss: 0.46528 - acc: 0.8136 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1213  | total loss: \u001b[1m\u001b[32m0.45882\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1213 | loss: 0.45882 - acc: 0.8172 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1214  | total loss: \u001b[1m\u001b[32m0.51236\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1214 | loss: 0.51236 - acc: 0.7869 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1215  | total loss: \u001b[1m\u001b[32m0.50156\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1215 | loss: 0.50156 - acc: 0.7930 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1216  | total loss: \u001b[1m\u001b[32m0.49148\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1216 | loss: 0.49148 - acc: 0.7981 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1217  | total loss: \u001b[1m\u001b[32m0.48215\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1217 | loss: 0.48215 - acc: 0.8033 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1218  | total loss: \u001b[1m\u001b[32m0.47413\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1218 | loss: 0.47413 - acc: 0.8073 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1219  | total loss: \u001b[1m\u001b[32m0.46626\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1219 | loss: 0.46626 - acc: 0.8112 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1220  | total loss: \u001b[1m\u001b[32m0.52270\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1220 | loss: 0.52270 - acc: 0.7828 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1221  | total loss: \u001b[1m\u001b[32m0.51048\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1221 | loss: 0.51048 - acc: 0.7892 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1222  | total loss: \u001b[1m\u001b[32m0.56035\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1222 | loss: 0.56035 - acc: 0.7637 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1223  | total loss: \u001b[1m\u001b[32m0.54426\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1223 | loss: 0.54426 - acc: 0.7725 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1224  | total loss: \u001b[1m\u001b[32m0.58036\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1224 | loss: 0.58036 - acc: 0.7484 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1225  | total loss: \u001b[1m\u001b[32m0.56387\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1225 | loss: 0.56387 - acc: 0.7583 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1226  | total loss: \u001b[1m\u001b[32m0.59316\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1226 | loss: 0.59316 - acc: 0.7361 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1227  | total loss: \u001b[1m\u001b[32m0.57688\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1227 | loss: 0.57688 - acc: 0.7474 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1228  | total loss: \u001b[1m\u001b[32m0.56261\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1228 | loss: 0.56261 - acc: 0.7572 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1229  | total loss: \u001b[1m\u001b[32m0.55039\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1229 | loss: 0.55039 - acc: 0.7659 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1230  | total loss: \u001b[1m\u001b[32m0.57303\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1230 | loss: 0.57303 - acc: 0.7432 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1231  | total loss: \u001b[1m\u001b[32m0.56077\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1231 | loss: 0.56077 - acc: 0.7534 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1232  | total loss: \u001b[1m\u001b[32m0.58800\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1232 | loss: 0.58800 - acc: 0.7262 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1233  | total loss: \u001b[1m\u001b[32m0.57547\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1233 | loss: 0.57547 - acc: 0.7384 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1234  | total loss: \u001b[1m\u001b[32m0.56444\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1234 | loss: 0.56444 - acc: 0.7495 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1235  | total loss: \u001b[1m\u001b[32m0.55536\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1235 | loss: 0.55536 - acc: 0.7583 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1236  | total loss: \u001b[1m\u001b[32m0.54730\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1236 | loss: 0.54730 - acc: 0.7662 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1237  | total loss: \u001b[1m\u001b[32m0.54017\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1237 | loss: 0.54017 - acc: 0.7741 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1238  | total loss: \u001b[1m\u001b[32m0.56372\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1238 | loss: 0.56372 - acc: 0.7477 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1239  | total loss: \u001b[1m\u001b[32m0.55479\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1239 | loss: 0.55479 - acc: 0.7554 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1240  | total loss: \u001b[1m\u001b[32m0.54623\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1240 | loss: 0.54623 - acc: 0.7632 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1241  | total loss: \u001b[1m\u001b[32m0.53868\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1241 | loss: 0.53868 - acc: 0.7704 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1242  | total loss: \u001b[1m\u001b[32m0.53193\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1242 | loss: 0.53193 - acc: 0.7767 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1243  | total loss: \u001b[1m\u001b[32m0.52506\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1243 | loss: 0.52506 - acc: 0.7825 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1244  | total loss: \u001b[1m\u001b[32m0.51852\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1244 | loss: 0.51852 - acc: 0.7884 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1245  | total loss: \u001b[1m\u001b[32m0.51156\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1245 | loss: 0.51156 - acc: 0.7933 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1246  | total loss: \u001b[1m\u001b[32m0.53505\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1246 | loss: 0.53505 - acc: 0.7707 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1247  | total loss: \u001b[1m\u001b[32m0.52603\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1247 | loss: 0.52603 - acc: 0.7779 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1248  | total loss: \u001b[1m\u001b[32m0.51741\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1248 | loss: 0.51741 - acc: 0.7845 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1249  | total loss: \u001b[1m\u001b[32m0.50937\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1249 | loss: 0.50937 - acc: 0.7900 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1250  | total loss: \u001b[1m\u001b[32m0.50167\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1250 | loss: 0.50167 - acc: 0.7945 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1251  | total loss: \u001b[1m\u001b[32m0.49405\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1251 | loss: 0.49405 - acc: 0.8004 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1252  | total loss: \u001b[1m\u001b[32m0.52480\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1252 | loss: 0.52480 - acc: 0.7763 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1253  | total loss: \u001b[1m\u001b[32m0.51423\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1253 | loss: 0.51423 - acc: 0.7835 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1254  | total loss: \u001b[1m\u001b[32m0.55344\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1254 | loss: 0.55344 - acc: 0.7562 -- iter: 594/594\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1255  | total loss: \u001b[1m\u001b[32m0.54002\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1255 | loss: 0.54002 - acc: 0.7652 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1256  | total loss: \u001b[1m\u001b[32m0.52796\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1256 | loss: 0.52796 - acc: 0.7737 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1257  | total loss: \u001b[1m\u001b[32m0.51618\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1257 | loss: 0.51618 - acc: 0.7810 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1258  | total loss: \u001b[1m\u001b[32m0.50570\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1258 | loss: 0.50570 - acc: 0.7876 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1259  | total loss: \u001b[1m\u001b[32m0.49666\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1259 | loss: 0.49666 - acc: 0.7935 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1260  | total loss: \u001b[1m\u001b[32m0.48870\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1260 | loss: 0.48870 - acc: 0.7984 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1261  | total loss: \u001b[1m\u001b[32m0.48084\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1261 | loss: 0.48084 - acc: 0.8030 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1262  | total loss: \u001b[1m\u001b[32m0.47338\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1262 | loss: 0.47338 - acc: 0.8071 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1263  | total loss: \u001b[1m\u001b[32m0.46617\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1263 | loss: 0.46617 - acc: 0.8112 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1264  | total loss: \u001b[1m\u001b[32m0.51219\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1264 | loss: 0.51219 - acc: 0.7836 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1265  | total loss: \u001b[1m\u001b[32m0.50148\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1265 | loss: 0.50148 - acc: 0.7899 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1266  | total loss: \u001b[1m\u001b[32m0.55017\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1266 | loss: 0.55017 - acc: 0.7615 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1267  | total loss: \u001b[1m\u001b[32m0.53633\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1267 | loss: 0.53633 - acc: 0.7700 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1268  | total loss: \u001b[1m\u001b[32m0.57244\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1268 | loss: 0.57244 - acc: 0.7448 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1269  | total loss: \u001b[1m\u001b[32m0.55667\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1269 | loss: 0.55667 - acc: 0.7547 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1270  | total loss: \u001b[1m\u001b[32m0.54245\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1270 | loss: 0.54245 - acc: 0.7634 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1271  | total loss: \u001b[1m\u001b[32m0.52972\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1271 | loss: 0.52972 - acc: 0.7717 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1272  | total loss: \u001b[1m\u001b[32m0.56542\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1272 | loss: 0.56542 - acc: 0.7447 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1273  | total loss: \u001b[1m\u001b[32m0.55173\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1273 | loss: 0.55173 - acc: 0.7549 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1274  | total loss: \u001b[1m\u001b[32m0.57915\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1274 | loss: 0.57915 - acc: 0.7321 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1275  | total loss: \u001b[1m\u001b[32m0.56527\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1275 | loss: 0.56527 - acc: 0.7429 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1276  | total loss: \u001b[1m\u001b[32m0.55308\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1276 | loss: 0.55308 - acc: 0.7530 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1277  | total loss: \u001b[1m\u001b[32m0.54241\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1277 | loss: 0.54241 - acc: 0.7624 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1278  | total loss: \u001b[1m\u001b[32m0.53314\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1278 | loss: 0.53314 - acc: 0.7700 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1279  | total loss: \u001b[1m\u001b[32m0.52468\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1279 | loss: 0.52468 - acc: 0.7780 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1280  | total loss: \u001b[1m\u001b[32m0.51728\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1280 | loss: 0.51728 - acc: 0.7844 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1281  | total loss: \u001b[1m\u001b[32m0.51041\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1281 | loss: 0.51041 - acc: 0.7896 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1282  | total loss: \u001b[1m\u001b[32m0.53834\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1282 | loss: 0.53834 - acc: 0.7659 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1283  | total loss: \u001b[1m\u001b[32m0.52904\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1283 | loss: 0.52904 - acc: 0.7740 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1284  | total loss: \u001b[1m\u001b[32m0.52063\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1284 | loss: 0.52063 - acc: 0.7806 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1285  | total loss: \u001b[1m\u001b[32m0.51228\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1285 | loss: 0.51228 - acc: 0.7872 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1286  | total loss: \u001b[1m\u001b[32m0.50460\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1286 | loss: 0.50460 - acc: 0.7923 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1287  | total loss: \u001b[1m\u001b[32m0.49751\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1287 | loss: 0.49751 - acc: 0.7971 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1288  | total loss: \u001b[1m\u001b[32m0.53587\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1288 | loss: 0.53587 - acc: 0.7679 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1289  | total loss: \u001b[1m\u001b[32m0.52515\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1289 | loss: 0.52515 - acc: 0.7751 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1290  | total loss: \u001b[1m\u001b[32m0.51569\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1290 | loss: 0.51569 - acc: 0.7823 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1291  | total loss: \u001b[1m\u001b[32m0.50645\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1291 | loss: 0.50645 - acc: 0.7889 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1292  | total loss: \u001b[1m\u001b[32m0.54188\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1292 | loss: 0.54188 - acc: 0.7615 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1293  | total loss: \u001b[1m\u001b[32m0.53023\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1293 | loss: 0.53023 - acc: 0.7692 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1294  | total loss: \u001b[1m\u001b[32m0.56464\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1294 | loss: 0.56464 - acc: 0.7431 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1295  | total loss: \u001b[1m\u001b[32m0.55101\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1295 | loss: 0.55101 - acc: 0.7535 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1296  | total loss: \u001b[1m\u001b[32m0.58236\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1296 | loss: 0.58236 - acc: 0.7300 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1297  | total loss: \u001b[1m\u001b[32m0.56690\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1297 | loss: 0.56690 - acc: 0.7417 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1298  | total loss: \u001b[1m\u001b[32m0.55355\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1298 | loss: 0.55355 - acc: 0.7519 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1299  | total loss: \u001b[1m\u001b[32m0.54195\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1299 | loss: 0.54195 - acc: 0.7613 -- iter: 594/594\n",
      "--\n",
      "Training Step: 1300  | total loss: \u001b[1m\u001b[32m0.57026\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1300 | loss: 0.57026 - acc: 0.7369 -- iter: 594/594\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# Start training (apply gradient descent algorithm)\n",
    "model2.fit(X_train,labels2.values,batch_size=891, n_epoch=1300, show_metric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (1, 297), indices imply (1, 418)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/home/nerd/Data/Conda/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_arrays\u001b[0;34m(arrays, names, axes)\u001b[0m\n\u001b[1;32m   4309\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mform_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4310\u001b[0;31m         \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4311\u001b[0m         \u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nerd/Data/Conda/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, blocks, axes, do_integrity_check, fastpath)\u001b[0m\n\u001b[1;32m   2794\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_integrity_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2795\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nerd/Data/Conda/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m_verify_integrity\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3005\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_integrity\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmgr_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3006\u001b[0;31m                 \u001b[0mconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3007\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtot_items\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nerd/Data/Conda/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconstruction_error\u001b[0;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[1;32m   4279\u001b[0m     raise ValueError(\"Shape of passed values is {0}, indices imply {1}\".format(\n\u001b[0;32m-> 4280\u001b[0;31m         passed, implied))\n\u001b[0m\u001b[1;32m   4281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (1, 297), indices imply (1, 418)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-e5144226f80a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msubmission\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Survived'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtestdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPassengerId\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/nerd/Data/Conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    273\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[1;32m    274\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nerd/Data/Conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_init_dict\u001b[0;34m(self, data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_arrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nerd/Data/Conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_arrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m   5602\u001b[0m     \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_ensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5604\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nerd/Data/Conda/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_arrays\u001b[0;34m(arrays, names, axes)\u001b[0m\n\u001b[1;32m   4312\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4313\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4314\u001b[0;31m         \u001b[0mconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nerd/Data/Conda/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconstruction_error\u001b[0;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[1;32m   4278\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Empty data passed with indices specified.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4279\u001b[0m     raise ValueError(\"Shape of passed values is {0}, indices imply {1}\".format(\n\u001b[0;32m-> 4280\u001b[0;31m         passed, implied))\n\u001b[0m\u001b[1;32m   4281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (1, 297), indices imply (1, 418)"
     ]
    }
   ],
   "source": [
    "submission  = pd.DataFrame({'Survived':pred}, index=testdf.PassengerId.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission['Survived']=submission.Survived.apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submissiontensor12.csv', index_label='PassengerId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tflearn.datasets import titanic as tit\n",
    "tit.download_dataset('titanic_dataset.csv')\n",
    "\n",
    "# Load CSV file, indicate that the first column represents labels\n",
    "from tflearn.data_utils import load_csv\n",
    "data, labels = load_csv('titanic_dataset.csv', target_column=0,\n",
    "                        categorical_labels=True, n_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels2 = pd.get_dummies(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
